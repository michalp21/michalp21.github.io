---
layout: post
title: "Digital Photography Notes - Optics"
categories: note
tags: photography
---

> This note is based on lectures 3 and 4 of Levoy's Digital Photography. It covers basic geometric optics starting with refraction, and ends with aberrations and other lens effects. These lectures are more math-heavy to begin with, and on top of that I tried to round out the proofs. Luckily it's nothing advanced.

<!--more-->

- TOC
{:toc}

# Physical vs Geometrical Optics

![optics2kinds.png]{: width="500"}

Optics, the study of light, is typically divided into two branches:
- **geometrical optics** characterizes light primarily as rays
- **physical optics**, or wave optics, characterizes light as a wave, with light rays drawn perpendicular to the wave

That said, waves do give some nice intuition even in the first regime. The main difference is that geometrical optics *doesn't* account for the effects of diffraction and interference. It's a useful approximation when the light's wavelength is much smaller than the objects it interacts with.

# Refraction

Snell's Law explains what happens to light when it passes between two transparent substances.

![snellslaw.png]{: width="600"}

The image above shows two mediums meeting at an **interface**. In the left subimage, the waves of light approach the interface at an angle of $$\theta_i$$ above the horizontal, with speed $$v_i$$, and recedes at angle $$\theta_t$$ below the horizontal, with speed $$v_t$$. In other words, the speed and direction of the light has changed.

Here's an intuitive explanation. Knowing that the velocity of light decreases in the second medium, the waves travel less distance over time, so they're closer together. Now, notice how the incoming waves line up with the exiting waves at the interface: no lines are lost and no lines are created from thin air. For this property to hold, the waves must exit at a smaller angle.

{% comment %}
The property is basically the requirement that the phase of a wave be continuous across the interface. For this to remain true (not just at a snapshot in time), the frequency must be constant and the angle must change. The wavelength $$\lambda$$ is related to velocity $$v$$ and frequency $$f$$ as:

$$
\lambda = \frac{v}{f}
$$


Since $$f$$ is constant and $$v$$ decreases, $$\lambda$$ decreases.
{% endcomment %}

We can instead look at the light rays, shown in the right subimage. Due to perpendicularity, we get equivalent angles $$\theta_i,\theta_t$$ with respect to the vertical, or **normal**. We get a few equalities:

$$
\frac{x_i}{x_t} = \frac{\sin\theta_i}{\sin\theta_t} = \frac{v_i}{v_t}=\frac{n_t}{n_i}
$$


The first equality is from trigonometry, the second is from proportionality of $$x$$ and $$v$$, and the third is from defining the **index of refraction**[^cropfactor] or refractive index as:

$$
n_r=\frac{\text{speed of light in vacuum}}{\text{speed of light in medium r}}
$$


From the equations above we get the usual formulation of **Snell's Law**:

$$
n_1\sin\theta_1 = n_2\sin\theta_2
$$


And wrapping up the example above, we notice a few things:
- light travels *more slowly* and bends *closer to the normal* through substances with *higher refractive indices*
- light does not refract when hitting the interface perpendicularly

Some common indices of refraction:
- air: ~1.0
- water: 1.33
- glass: 1.5 - 1.8

![mirage1.jpg]{: width="500"}

Refractive indices change with temperature. For example, hot air has a lower refractive index than cool air. This is the source of some **mirages**, like the commonly seen "wet road" phenomenon.

![mirage2.png]{: width="500"}

In a bit more detail: in places with a gradient of hot air up high to cool air on the ground, some faraway light rays moving partially downward will bend away from the normal, flattening out and even curving upward! When they reach your eyes, it will seem like they originated from a point below the ground.

# Lens Shapes

![refractionlens.png]{: width="400"}

When passing through a lens, light refracts twice:
- it bends closer to the normal when entering the lens
- it bends away from the normal when exiting the lens

Light hitting the lens perpendicularly does not bend. The only place the light hits perpendicularly when entering *and* exiting the lens is on the **principal or optical axis** ($$\overline{C_2C_1}$$ above).

![twocircles.png]{: width="300"}

The two sides of a lens can be characterized by the radii of their respective circles (spheres in 3D). If they intersect, they form a **convex lens**, otherwise, they form a **concave lens**. They are also called **positive** and **negative** lenses, respectively.

![convexconcave.png]{: width="200"}

Lenses come in many shapes. Following convention, the radius is negative on the right half of a circle, and positive for the left half.

![convexconcave1.png]{: width="600"}

Assuming light rays move left to right, if the rays meet at a point to the right of the lens, they produce a **real image**. If they diverge after the lens, the rays can be traced backwards, converging to a **virtual image** on the left side.

Likewise, if the rays originate from a point to the left of the lens, we have a **real object**. If they come from a divergent source, they can be traced forward, meeting on the right side, forming a **virtual object** (not pictured).

Parallel rays are on the border of convergent and divergent. We treat them as real, rather than virtual, points at infinity.

# Gaussian Lens Formula

## Simple Derivation

From now on let's assume our object is in focus, so we don't have to keep saying object/image *focus* distance. There's a pretty simple relationship between the object distance, the image distance, and the focal length that will help clarify all the virtual object/image business. In this derivation we make some pretty broad assumptions, in particular, that all parallel points pass through the focal point $$f$$. We also use a convex lens for illustration, but the derivation works fine for concave lenses too.

![gaussray1.png]{: width="400"}

Using similar triangles, we get:

$$
\begin{align*}
\frac{\vert y_i \vert}{y_o} &= \frac{s_i}{s_o}\\
\frac{\vert y_i \vert}{y_o} &= \frac{s_i-f}{f}\\
\end{align*}
$$

Equating and rearranging, we get the **Gaussian lens formula**:

$$
\frac{1}{s_o} + \frac{1}{s_i} = \frac{1}{f}
$$

## Paraxial Approximation

Note: I made some changes to the material on the slides.[^change] Levoy justifiably marks this section as optional.

Here's a longer derivation of the Gaussian lens formula from more fundamental premises. Again we use a convex lens but it works for concave too.

![paraxial.png]{: width="400"}

Above we show a light ray shining from an object point $$O$$, hitting a spherical surface with radius $$r_1$$ (not labeled above), and landing on the image point $$P$$. We use the **paraxial approximation**, which assumes $$e\approx 0$$, or that the incident ray on the lens is nearly horizontal. As a result, we get several simplifications via trigonometry:

$$
\begin{align*}
\sin u &= h/l \approx u \text{ (for } u \text{ in radians)}\\ 
\cos u &= z/l \approx 1\\
\tan u &\approx \sin u \approx u
\end{align*}
$$

This is called **first-order optics**, because we can get the same approximations from the first-degree Taylor polynomial of $$\sin u$$ and $$\cos u$$ centered at zero. Just to illustrate, the Taylor series expansions around zero are:

$$
\begin{align*}
\sin\phi = \phi - \frac{\phi^3}{3!} + \frac{\phi^5}{5!} - \frac{\phi^7}{7!} + \ldots\\
\cos\phi = 1 - \frac{\phi^2}{2!} + \frac{\phi^4}{4!} - \frac{\phi^6}{6!} + \ldots
\end{align*}
$$

And we just use the first term to approximate each function.

![paraxial1.png]{: width="400"}

Recall Snell's Law states $$n\sin \theta = n'\sin \theta'$$. The paraxial approximation is:

$$
n\theta \approx n'\theta'
$$

![paraxial2.png]{: width="400"}

We want an equation relating $$z_1'$$ with $$z_1$$. What follows is a bunch of elementary math. By geometry:

$$
\begin{align*}
\theta&=u+a\\
\theta'&=a-u'\\
\end{align*}
$$

We use that to expand our Snell's Law approximation:

$$
n(u+a) \approx n'(a-u')
$$


Then from trigonometry we can say:

$$
\begin{align*}
a \approx \sin(a) &= \frac{h}{r_1}\\[1em]
u \approx \tan(u) &= \frac{h}{z_1}\\[1em]
u' \approx \tan(u') &= \frac{h}{z_1'}\\
\end{align*}
$$

We use those to manipulate our Snell's Law approximation some more:

$$
\begin{align*}
n\left(\frac{h}{z_1}+\frac{h}{r_1}\right) &\approx n'\left(\frac{h}{r_1}-\frac{h}{z_1'}\right)\\
\Rightarrow \frac{n}{z_1}+\frac{n'}{z_1'} &\approx \frac{n'-n}{r_1}\\
\end{align*}
$$

Notice how $$h$$ has canceled out. No matter where a ray originating from $$O$$ lands on the lens (no matter what height $$h$$), it will always hit $$P$$. We have shown that there isn't spherical aberration under the assumptions we made.

{% comment %}
![paraxial3.png]{: width="300"}

As you might recall, the focal length is the distance of the point of convergence $$P$$ for rays emanating from a point $$O$$ at infinity. Setting $$z_1=\infty$$, then, we can define the focal length $$f$$ like so:

$$
\begin{align*}
0+n/r_1 &\approx n'/r_1-n/z_1'\\
\Rightarrow z_1' &\approx r_1n'/(n'-n)\\
f&\triangleq r_1n'/(n'-n)
\end{align*}
$$
{% endcomment %}

![paraxiala.png]{: width="400"}

We calculated light going from air to glass, but we also need to do glass back to air. Let the right side of the lens be a spherical surface of radius $$r_2$$ (not labeled). After refracting once on the left side, the ray will hit refract on the right side, and finally land on $$Q$$.

We basically redo the calculations with new object and image points. The new image point is $$Q$$, but what is the new object point? Well, the incident ray for the right side is just the transmitted ray from the left side, which intersects the optical axis at $$P$$. So the image from the left interface becomes a *virtual* object at the right interface. For this reason, we let the distance from the right side of the lens to $$P$$ be negative. The resulting equation is:

$$
-\frac{n'}{z_2}+\frac{n}{z_2'} \approx \frac{n-n'}{r_2}
$$


Summing the equations for the left and right sides, we get

$$
\begin{align*}
\frac{n}{z_1}+\frac{n}{z_2'} &\approx (n'-n)\left(\frac{1}{r_1}-\frac{1}{r_2} \right) + \frac{n'}{z_2}-\frac{n'}{z_1'}\\
\Rightarrow \frac{n}{z_1}+\frac{n}{z_2'} &\approx (n'-n)\left(\frac{1}{r_1}-\frac{1}{r_2} \right) + \frac{n'd}{(z_1'-d)z_1'}
\end{align*}
$$

Where the second line uses $$\vert -z_2\vert = \vert z_1'\vert - d$$ to simplify the right terms.

Let's use some nicer variables to represent the distances of the object and image to the left and right side of the lens, respectively: $$s_o = z_1$$ and $$s_i = z_2'$$. As $$d\rightarrow 0$$, called the **thin lens approximation**, the right term approaches zero, and we obtain the **lens maker's formula**:

$$
\begin{equation}
\frac{1}{s_o} + \frac{1}{s_i} = (n' - 1)\left(\frac{1}{r_1} - \frac{1}{r_2}\right)\label{eq:lmf}
\end{equation}
$$

Note: we set $$n=1$$, the refractive index of air. 

If we move the object $$s_0$$ to infinity, the image distance $$s_i$$ approaches the focal length $$f$$, so we can say:

$$
\frac{1}{f} = (n' - 1)\left(\frac{1}{r_1} - \frac{1}{r_2}\right)
$$

Equating with the previous equation, we obtain the Gaussian lens formula:

$$
\frac{1}{s_o} + \frac{1}{s_i} = \frac{1}{f}
$$

An interactive applet can be found [here](https://sites.google.com/site/marclevoylectures/applets/gaussian-lens-formula).

# Uses For The Gaussian Lens Formula

The Gaussian lens formula serves the basis for the rest of this note, and much more. Here's a few important uses.

## Changing Object Distance (Revisited)

What can the Gaussian lens formula do for us?

![changefocus.png]{: width="300"}

It basically dictates what happens in the image above. Again, it models the interactions of object distance, image distance, and focal length, *when the object is in focus*.

If the sensor is at the focal point, then $$s_i=f$$, and the object is focused at infinity (top drawing in the picture above).

We can also have $$s_o=s_i=2f$$ (third drawing):

$$
\frac{1}{2f} + \frac{1}{2f} = \frac{1}{f}
$$

giving us 1:1 imaging, which means the scene and image scales are the same. For example, a 36 mm wide object will fill a 36 mm wide sensor.[^size]

Similar to the first drawing, if $$s_o=f$$ then $$s_i=\infty$$ (not pictured), meaning we physically can't focus on objects closer to the lens than $$f$$.

## Virtual Images and Objects

What if the object distance is less than the focal length? Say $$s_o=\frac{f}{2}$$. Then:

$$
\frac{2}{f} + \frac{1}{s_i} = \frac{1}{f}
$$

so $$s_i=-f$$.

In general:
- if $$0<s_o<f$$, then $$s_i<0$$
- if $$0<s_i<f$$, then $$s_o<0$$

By convention, real objects lie to the left of the lens while real images lie to the right, and both have positive distances. Negative distances, then, correspond with *virtual* objects and images.

## Concave Lenses

So far we assumed we were working with a convex lens. By convention, convex lenses have positive focal length, while concave lenses have negative focal length. Concave lenses actually behave much nicer, creating a virtual image at any object distance. For example, let $$s_o=2$$ and $$f=-1$$.

$$
\frac{1}{2} + \frac{1}{s_i} = -1
$$

Then $$s_i=-\frac{2}{3}$$. 

From the Gaussian lens formula, we can see that if $$s_o>0$$ (as it must be for a real object) and $$f<0$$, then it must be the case that $$s_i<0$$ and $$\vert s_i\vert<s_o$$.

## Power of a Lens

How can we compare the strengths of different lenses? Let's define the **power** $$P$$ of a lens as:

$$
P=\frac{1}{f}
$$

where the units are 1/meters, or **diopters**. Due to the convention from the previous section, convex lenses have positive diopters while concave lenses have negative diopters.

If we combine two lenses, we can just add their diopters. The two equations are equivalent:

$$
\begin{align*}
    \frac{1}{f_{tot}} &= \frac{1}{f_1} + \frac{1}{f_2}\\[1em]
    P_{tot} &= P_1 + P_2
\end{align*}
$$

## Magnification

![gaussray1.png]{: width="400"}

By similar triangles, magnification can be determined from the image and object distances:

$$
M \triangleq \frac{y_i}{y_o} = -\frac{s_i}{s_o} = -\frac{f}{s_o-f}
$$

The triangle symbolizes equality by definition. The negative sign is just due to differing conventions; the heights have opposite signs depending whether they're above or below the optical axis, unlike the object and image distances if neither is virtual. 

**Clarification**: Most people, including Levoy later in the slides, seem to ignore the sign on magnification, effectively taking $$\vert M \vert$$. From now on, I'm dropping the sign too.

### Hardware For Magnification

Gathered from [this site](https://www.cambridgeincolour.com/tutorials/macro-extension-tubes-closeup.htm).

**Extension tubes**: devices fitted between the camera body and the lens, which increase magnification by increasing effective image distance.
- Measured in mm.
- They have no optics, but allow electronics to pass through. They can be stacked.
- They raise the effective f-number, reducing the amount of light.
- They're better for wide-angle lenses.

**Close-up filters/lenses**: a special lens screwed onto the front of the actual lens, increasing magnification by *decreasing* effective focal length.
- Measured in diopters.
- Optical elements may reduce image quality.
- They're better for telephoto lenses.

**Teleconverters**: like extension tubes, but with optics. They increase magnification by *increasing* effective focal length.
- Measured by magnification multiplier (e.g. 1.4x)
- Optical elements may reduce image quality.
- They raise the effective f-number, reducing the amount of light.

**Macro Lenses**: the best and most expensive option for close-up photography.
- Measured in $$M$$.
- Corrects for aberrations.

**Clarification**: Alright, you have to be wondering: how can close-up filters and teleconverters achieve the same thing in different ways? I'm not 100% certain but here's my thinking. Teleconverters work like telephoto lens elements, where a combination of lens elements can achieve the same focal length and object distance at a reduced image distance; see the Telephoto section below. Since the teleconverter is screwed in between the existing lens and the camera, the image distance doesn't decrease, leading to an increase in the effective focal length.

Close-up filters allow you to (drum roll) get close-up to the object, i.e. reduce object distance, by *lowering* focal length; see the example at the end of this note called Close-Up Filters. Based on the last expression for magnification, we can see that focal length and object distance aren't related linearly, and I presume in close-up situations, the object distance has a stronger effect on the magnification than the focal length.

In other words:
- close-up filter: $$f\downarrow\quad s_o\Downarrow\quad s_i – \quad M\uparrow$$
- teleconverter: $$f\uparrow\quad s_o – \quad s_i – \quad M\uparrow$$

# 3D Perspective Transform

![frustrum.png]{: width="400"}

A lens transforms a 3D object into a 3D image, and the sensor extracts a 2D slice of that image. We can think of this as an extension of the transform going directly from 3D to 2D from [Image Formation]({{ site.baseurl }}{% post_url 2021-10-20-image-formation %}).

**Clarification**: How is that different from a pinhole camera? Recall that a pinhole camera performs a projective transformation. We previously showed one object and its image on one sensor.

![frustrum2d.png]{: width="400"}

Here's two objects and two sensors. A pinhole camera has infinite depth of field, so any placement of the sensor will capture both objects in focus (and in fact, the entirety of the visible scene).

![frustrum2d_lens.png]{: width="400"}

Replacing the pinhole with a lens, the sensor at any position will still capture both objects. The difference is that only one plane in the object space will be in focus for any given plane in image space (see the blurred arrows). For any square in object space, we get a trapezoid of the *corresponding focused objects* in image space, a bijective transformation. Moving from the 2D visualization to 3D, the square and trapezoid become a cube and pyramidal frustrum, as shown in Levoy's drawing. The latter converges to the focal point, *not* the optical center of the lens, and no such transformation exists for the pinhole camera.

![perspectivetransform.png]{: width="500"}

The other point Levoy makes is that a linear change in object distance is accompanied by a *nonlinear* change in image distance, evident from Gauss' lens formula, and illustrated by the grid lines in the image above. Note that the warped grid in image space *doesn't* represent a 3D view of a plane sticking out of the page, but a trapezoid on the same plane as everything else in the diagram. The image comes from an [interactive applet](https://sites.google.com/site/marclevoylectures/applets/operation-of-a-thin-lens).

# Depth of Field
 
## Derivation
 
![dofformula1.png]{: width="500"}
 
In [Image Formation]({{ site.baseurl }}{% post_url 2021-10-20-image-formation %}) we gave a simplified presentation of the circle of confusion using symmetric cones. Now we use a more accurate diagram. The vertical line to the right of the lens signifies the **circle of confusion**, the in-focus image plane is the vertical line to the right of the lens, the area of acceptable focus is shaded red, and the width of this area is called the **depth of focus**. We define the **conjugate of the circle of confusion** and the **depth of field** similarly, to the left of the lens. It's clear that the depth of field is asymmetrical around the in-focus object plane. In addition, the circle of confusion is usually smaller than its conjugate, which we can see from its formula; since usually $$0<M<1$$, then usually $$\frac{C}{M}>C$$.

![dofformula2.png]{: width="500"}

Let $$U=s_o$$, just for convention's sake. We assume the object distance (focal distance) is much larger than the focal length, or $$U\gg f$$, or equivalently $$f\approx s_i$$.

$$
\begin{align*}
    M = \frac{s_i}{U} &\approx \frac{f}{U}\\
    \frac{C}{M} &\approx \frac{CU}{f}
\end{align*}
$$

![dofformula3.png]{: width="500"}

Recall the aperture diameter is $$\frac{f}{N}$$. Let $$D_1$$ be the **front depth of field** and $$D_2$$ be the **back depth of field**. By similar triangles we get:

$$
\frac{D_1}{CU/f} = \frac{U-D_1}{f/N}
$$

which gives an expression for the front depth of field:

$$
D_1 = \frac{NCU^2}{f^2+NCU}
$$

Similarly, the back depth of field is:

$$
D_2 = \frac{NCU^2}{f^2-NCU}
$$

We sum those to get the total depth of field:

$$
\begin{equation}
D = D_1 + D_2 = \frac{2NCU^2f^2}{f^4-N^2C^2U^2} \label{eq:dof1}
\end{equation}
$$

The $$N^2C^2U^2$$ term can be ignored when the conjugate circle of confusion is small relative to the aperture,[^conj] giving the final formula for depth of field:

$$
\begin{equation}
D = \frac{2NCU^2}{f^2} \label{eq:dof2}
\end{equation}
$$

While it may not be the most accurate formula, it does illustrate the relationships between its component variables for reasonable object distances. An interactive depth of field applet can be found [here](https://graphics.stanford.edu/courses/cs178-14/applets/dof.html).

## Hyperfocal distance

The back depth of field becomes infinite if:

$$
U = \frac{f^2}{NC} \triangleq H
$$

In that case, the front depth of field becomes:

$$
D_1 = \frac{NCU^2}{f^2-NCU} = \frac{H}{2}
$$

That is, if the object distance is set to the **hyperfocal distance** $$H$$ (equivalently, if you focus at $$H$$), everything from distance $$\frac{H}{2}$$ to infinity will be in focus. It's a good number to know on its own, especially for certain use cases like landscape photography.

We can also express the depth of field in terms of $$H$$. Without disregarding the term $$U^2N^2C^2$$ in the denominator of \eqref{eq:dof1}, we get:

$$
D = \frac{2HU^2}{H^2-U^2}
$$

Now, another perspective on what it means to ignore the term $$U^2N^2C^2$$, is that the hyperfocal distance is much larger than the object distance, or $$U^2 \ll H^2$$. If so, then the formula reduces to \eqref{eq:dof2}, or equivalently:

$$
D = \frac{2U^2}{H}
$$

So the simplified depth of field formula, \eqref{eq:dof2}, relies on the object distance $$U$$ falling within a "nice" range, roughly specified by:

$$
f\ll U \ll H
$$

## Macro Depth of Field

Our derivation for the depth of field doesn't quite work for macro lenses, because of the assumption that $$f=s_i$$. Discarding it, the accurate formula for small object distances can be stated, in terms of $$M$$:

$$
D_{\text{macro}}=\frac{2NC(M+1)}{M^2}
$$

## Other Derivations

The derivations above use the $$f=s_i$$ approximation pretty early, which makes the calculations easier but gives less general formulas. In particular, the general versions would better convey where the macro version comes from. Of course there's even more complicated versions dealing with things like thick lenses. [Here](https://www.largeformatphotography.info/articles/DoFinDepth.pdf) and [here](https://www.imajtrek.com/new_page_10.htm) are some more complete derivations.

|               | N,C,U,f                                  | H                              | M                                                  |
| ------------- | ---------------------------------------- | ------------------------------ | -------------------------------------------------- |
| any $$U$$       | $$\frac{2NCUf^2(U-f)}{f^4-N^2C^2(U-f)^2}$$ | $$\frac{2HU(U-f)}{H^2-(U-f)^2}$$ | $$\frac{2NC(M+1)}{M^2-\left(\frac{NC}{f}\right)^2}$$ |
| $$f\ll U$$      | $$\frac{2NCU^2f^2}{f^4-N^2C^2U^2}$$        | $$\frac{2HU^2}{H^2-U^2}$$        |                                                    |
| $$U\ll H$$      | $$\frac{2NC(U-f)}{f^2}$$                   | $$\frac{2U(U-H)}{H}$$            | $$\frac{2NC(M+1)}{M^2}$$                             |
| $$f\ll U\ll H$$ | $$\frac{2NCU^2}{f^2}$$                     | $$\frac{2U^2}{H}$$               |                                                    |

The table shows several variations of the formula, depending on the assumptions made on the object distance $$U$$, and the particular terms used.[^yourself]

# Depth of Field In Practice

For intuition's sake, depth of field is simplified by necessity. Most of the conclusions drawn below use the simpler versions of the equations, and therefore apply to midrange distances. Some [people](https://theonlinephotographer.typepad.com/the_online_photographer/2009/06/depth-of-field-hellthe-sequel.html) will bemoan these approximations, but as long as we're aware of their limitations, it shouldn't be a problem.

## Basic Relationships

We use \eqref{eq:dof2} for the observations below.

![doffnumber.png]{: width="300"}

Depth of field is linear with f-number.

![dofsubjectdistance.png]{: width="600"}

 Depth of field is quadratic with object distance.

![doffocallength.png]{: width="600"}

Depth of field is inverse-quadratic with focal length.

## Dolly Zoom

Recall in a dolly zoom, the magnification of the subject stays constant by increasing both object distance and focal length in a principled manner.

By \eqref{eq:lmf}, when magnification is held constant, the depth of field depends on neither focal length nor object distance. Due to perspective distortion, however, the blurriness of the background will change, since the FOV has changed and the blurred elements take up a different amount of screen space.

## Sensor Size

As the sensor size goes down:
- the focal length tends to go down to maintain FOV
- the pixel size tends to go down to maintain resolution, which makes the circle of confusion smaller

Focal length has a greater effect (squared, compared to linear, from \eqref{eq:dof2}) than circle of confusion, so generally speaking, as sensor size $$\times 2$$, depth of field is $$\times \frac{1}{2}$$.

## Circle of Confusion

The circle of confusion depends on many things, including: the sensing medium, reproduction medium, viewing distance, human vision, etc. Some typical cases:
- 35 mm film to print: .02 mm (on negative)
- high-end SLR: 6 μm (1 pixel)
- can go smaller when downsizing for the web or for a poor lens

The lectures show example depth of field calculations for different photographs taken by Levoy, including viewing considerations. They're too large to reproduce, and it would be pointless to show downsized ones, so I urge you to check out the lecture slides.

## Bokeh

![bokeh.jpg]{: width="400"}

**Bokeh** is the appearance of small out-of-focus features in a photograph with shallow depth of field. The shape of the features are determined by the boundary of the aperture. In the picture above, the slightly closed blades on the aperture produce octagonal lights. Not every image has noticeable bokeh; it is most pronounced with point light sources.

~~It's pronounced *BOW-cuh*!~~

Everyone seems to have a strong belief about the pronunciation, and everyone's belief seems to be different. I'm personally switching to *boh-keh*.

## Seeing Through Occlusions

![dofocclusion.png]{: width="400"}

The depth of field blur is not the same as a convolution. Using a shallow enough depth of field, we can eliminate occlusions, which would not be possible by blurring a static image featuring the occlusion. Apparently a chain link fence sat between the camera and the owl in the picture above, but it is not visible. Some of the rays from the owl got blocked, but some made it around the fence wire. A given pixel, then, may have a mixture of colors from the owl and the fence, and the result on the image is not an outright occlusion, but decreased contrast.

# Lens Aberrations

Let's recap the assumptions we made so far:
- Geometric instead of physical optics
- Spherical lenses
- Paraxial approximation of ray angles
- Thin lenses instead of thick, or even compound lenses

There's seven main types of lens aberrations. We can divide lens aberrations into:
- **chromatic aberrations (CA)**: two aberrations rooted in the varying wavelengths of light
- **monochromatic aberrations**: five Seidel aberrations for monochromatic light

Another way to 

## Chromatic Aberrations

![prism.png]{: width="500"}

The index of refraction varies by wavelength! That's how an optical prism works after all. The **dispersion** is the variation in refractive index by wavelength. It's modeled by some function, for instance the **Sellmeier equation** (right subimage above), outputting a single refractive index for a given wavelength.
- Higher dispersion means greater variation
- Amount of variation depends on the material
- Index is typically higher for blue than red, meaning blue light bends more

There's two kinds of CA:
- **longitudinal**
    - *cause*: lens has different effective focal lengths for different wavelengths of light
    - *effect*: different colors on the image will be out of focus
- **lateral**
    - *cause*: different colors have different magnifications
    - *effect*: colors on the image will be magnified (with respect to the optical axis) different amounts, producing "color fringing" towards the edges

![chromaticaberration.png]{: width="500"}

It's possible to correct longitudinal CA with an additional lens element. The right subimage shows an **achromatic doublet**, composed of a normal convex element and a complementary concave element with different dispersions (crown and flint refer to the different glass types). By adjusting the dispersions, we can correct at two wavelengths. In the right subimage, the doublet is designed so red and blue light are aligned. Though green wavelengths are imperfectly corrected, the overall dispersion is greatly reduced. Longitudinal CA can be also be corrected by lowering the aperture, so the light hits the lens closer to the normal (I think), limiting dispersion. 

Lateral CA can be somewhat corrected either with a lens construction symmetrical about the aperture, or with software.

{% comment %}
![lateralab.jpg]{: width="300"}

Note that for simple lenses, or lenses with elements symmetric about the aperture, the chief rays don't refract very much, limiting chromatic aberration.
{% endcomment %}

![chromaticab.gif]{: width="400"}

The two kinds of chromatic aberration are shown above side-by-side. Notice how longitudinal CA takes the form of out-of-focus elements spread uniformly across the whole image. In lateral CA we can clearly see the radial magnification of different colors, with a stronger effect at the edges.

## Spherical Aberrations

Remember the goal of a lens is to focus light rays at a point.

![lenstypes.png]{: width="250"}

The lens shape which focuses incoming rays to a single point is actually a hyperboloid. In contrast, a spherical lens exhibits what's called **spherical aberration**, which can be corrected by stopping down the aperture.

![softfocus.jpg]{: width="400"}
*Left: soft focus on; right: soft focus off*

Spherical lenses are more common simply because they're easier to make. It's still possible to buy a "soft focus" lens, which intentionally does not correct spherical aberration.

![expkernel.png]{: width="150"}

Fun fact: the soft focus effect is equivalent to a post-processing convolution with the exponential kernel.

## Coma

![coma.png]{: width="200"}

**Coma** is when the magnification varies based on distance from the optical axis, similar to lateral CA. Unlike lateral CA, it can be reduced by stopping down the aperture.

## Astigmatism

![astigatism.png]{: width="500"}

**Astigmatism** is when rays in the transverse plane (x-z plane) focus at a different distance from the rays in the sagittal plane (y-z plane). There's two kinds:
- **opthalmic**: due to radially asymmetric (oblong) lens
- **optical**: even for radially symmetric lens; the third-order aberration

It can be reduced by stopping down the aperture.

![astigmatismopthalmic.png]{: width="300"}

The effect is shown above. Note that vertical or horizontal focus should be different in strength, or else it's just a normal, out-of-focus lens.

## Field Curvature

**Field curvature** refers to the fact that spherical objects focus a curved surface in object space onto a curved surface in image space. If the object surface is flat, the corresponding image surface must be even more curved. Only some ring on a flat imaging plane will be in focus. It can be fixed by closing down the aperture.

![fieldcurvature.png]{: width="300"}

The top subimage shows the required object and image surfaces for a focused image using a spherical lens. The bottom subimages show different ranges of radii in focus.

## Distortion

![distortion.jpg]{: width="500"}

There's three kinds of distortion:
- **pincusion**: magnification increases further from the optical axis
- **barrel**: magnification decreases further from the optical axis
- **mustache**: mix of the two above

It is easily correctable with software.

**Clarification**: From the image above, it might look like the magnification doesn't change along the horizontal and vertical axes, but it does, and would be visible with a larger grid.

## NOT Perspective

Notice how perspective isn't in the list. It's *not* a lens distortion or aberration! Perspective can only produce "distortion" in a subjective sense, taking into account an object's typical viewing distance.

# Other Lens Artifacts

There are several other effects besides aberrations, that may or may not be desired.

## Veiling Glare

![veilingglare1.jpg]{: width="400"}

**Veiling glare** is a glare lowering contrast throughout the image.

![veilingglare.png]{: width="300"}

At every interface, both refraction and reflection occur. The image above shows how a few reflections can cause a ray from one source to produce two outgoing rays. Usually more light refracts than it reflects, but the light coming from the secondary rays is enough to lower the contrast of the image. It can be reduced by anti-reflection coatings to block unwanted reflections within the lens, and lens hoods to block excessive incoming light.

## Lens Flare

![lensflare.jpg]{: width="400"}

**Lens flares** (or **ghosting**) are structured artifacts which move predictably around the image based on the direction of the camera (it's easier to see what it is than to try to explain it). The image above includes a special case where the light source looks star-shaped, called a **starburst**. Lens flares can be reduced with a lens hood, or by moving the light source out of the frame.

## Vignetting

![vignetting.png]{: width="300"}

**Vignetting** is the darkening of an image around the edges. From another perspective, it is the soft boundary of the image circle coming into view.

![vignetting1.png]{: width="300"}

The first kind is **optical vignetting**, due to the distance traveled from the lens to the sensor. In the diagram above, we see several things happening, all based on the angle $$\theta$$ the pixel makes lies above the horizontal:
- Irradiance is proportional to the projected area of aperture as seen by the sensor. In other words, at point $$H$$, the sensor sees the aperture as an ellipse with less area compared to the circle that point $$A$$ would see. This area decreases as $$\cos\theta$$.[^projellipse]
- Irradiance is proportional to the projected area of the pixel as seen by the aperture. Basically, the previous effect works backwards, with the aperture seeing an stretched out pixel at $$H$$ compared to $$A$$, with area also decreasing as $$\cos\theta$$.
- Irradiance is proportional to squared distance from aperture to pixel, and distance rises as $$1/\cos\theta$$.
- Combining all three, light drops as $$\cos^4\theta$$.

![vignetting2.png]{: width="600"}

There's also **optical vignetting**, where the light coming from wide angles is blocked by the barrel of the lens, especially noticeable for wide apertures, as seen in the bottom-left lens from the left subimage above. The main result is once again a decrease in light on the edges. Since the effective shape of the aperture changes, so does the bokeh (if present) towards the edges of an image, known as the **cat's eye effect**.

There's **mechanical vignetting**, caused by incorrect add-ons for the lens, or a lens whose image circle is too small for the sensor. In both cases, the image shows a relatively sharp cutoff to black in the shape of whatever obstacles lie in front of the lens.

Finally, there's **pixel vignetting** due to sensor construction. For light coming in at an angle, the walls of the pixel cast shadows, reducing light captured at the edges of the sensor.

Unlike the previous two artifacts, vignetting is digitally correctable, although noise might increase as pixels are brightened. Optical vignetting can be lowered by decreasing the aperture, and mechanical vignetting is eliminated by using compatible camera parts.

## Diffraction

Here we'll briefly dip our feet into the wave aspects of light. **Diffraction** is when light spreads out as it passes through an aperture. The result is a slight blurring of the image (yet again). When diffraction causes a noticeable blur, the lens is said to have become *diffraction limited*.

The amount of spreading increases as the aperture diameter *decreases* and the distance from the sensor *increases*, so the blur varies with $$N=\frac{f}{A}$$. It also depends on pixel size, just like depth of field, and the wavelength of the light.

Note: Diffraction is also responsible for the image circle, and hence vignetting.

# Lens Systems

## Telephoto

![telephoto.png]{: width="400"}

A **telephoto lens** has a long focal length at a compressed size. Normally, a convex lens would have to be positioned at the middle line in the diagram above, at distance $$f$$ from the sensor. To fit the same lens in a more compact space, the strength of the convex lens is boosted, and it's combined with a concave lens to refocus the rays at the original point, as in figure (a). Figure (b) shows a **reverse telephoto lens** using the opposite mechanism, in order to make room for a reflex mirror in an SLR, for instance.

![telephoto1.png]{: width="600"}

Here is one of Levoy's drawings, which might be clearer. The blue lens is the original, and the green is the telephoto combination. They have the same focal length.

![telephoto2.png]{: width="600"}

Here's a comparison.

## Zoom

Zoom lenses have many components, but allow for variable focal length.

![zoom.png]{: width="400"}

Only two elements are shown above, but the point is that the different elements have to move at varying rates to offer a linear zoom.

You can experiment with zoom on an interactive applet found [here](https://graphics.stanford.edu/courses/cs178-14/applets/zoom.html).

## Lens Design

![lenssoftware.png]{: width="400"}

Modern lens systems are designed with software, using optimization to craft good lens recipes. Usually they do not optimize the selection and arrangement of lens elements, instead minimizing aberrations by adjusting the surface shapes. The recipes for modern cameras are kept secret; supposedly even the patents for commercial lenses don't reveal the exact recipe.

# Examples

## Eyeglasses I

**Q:** The professor has glasses with the prescription:
- right eye: -.75 diopters
- left eye: -1.00 diopters

What's wrong with the professor's eyes?

**A:** The professor has myopia (nearsightedness).

![eyeproblems.png]{: width="600"}

His eye is stronger than normal, with a focal point in front of the retina (which we know since the diagram shows parallel incoming rays). A concave lens is needed to "weaken" it, so the light hits the retina properly. Note that nearby objects will converge further back based on the Gaussian lens formula, and with the lens they may now focus *behind* the retina. The professor will need to remove his glasses to see close things properly.

## Macro Lenses

**Q:** How can the Casio EX-F1 at 73mm and the Canon MP-E 65mm macro, which have similar $$f$$’s, have such different focusing distances?

**A:** Because macro lenses are built to allow long $$s_i$$, which requires lower $$s_o$$, given similar $$f$$.

## Close-Up Filters

**Q:** If we attach a close-up filter of power $$\frac{1}{500\text{mm}}$$ to a lens with focal length $$f=200\text{mm}$$ and minimum object distance $$s_o=1000\text{mm}$$,[^wrong] how close can we focus on the object?

**A:** By the Gaussian lens formula, in order to change the object distance $$s_o$$ we have to change either the image distance $$s_i$$ or the focal length $$f$$. What a close-up filter allows you to do is reduce the object distance for a *fixed image distance* by tweaking the overall focal length. Remember, for a given focal length, a camera's minimum focus distance is based on the furthest the sensor can get from the lens.

![closeup.png]{: width="500"}

By the Gaussian lens formula, the image distance for the lens without the filter, corresponding to the minimum object distance, must be:

$$
s_i = \frac{1}{\frac{1}{f}-\frac{1}{s_o}} = \frac{1}{\frac{1}{200\text{mm}} -\frac{1}{1000\text{mm}}} = 250\text{mm}
$$

Now using the close-up filter on the lens (set at the same focal length as before), the combined power is:

$$
\begin{align*}
    \frac{1}{200 \text{mm}} + \frac{1}{500\text{mm}} &= \frac{1}{143\text{mm}}\\[1em]
    5.0 + 2.0 &= 7.0 \text{ diopters}
\end{align*}
$$

So at the minimum in-lens image distance, the close-up filter allows the object distance to be:

$$
s_o = \frac{1}{\frac{1}{f}-\frac{1}{s_i}} = \frac{1}{\frac{1}{143\text{mm}} -\frac{1}{250\text{mm}}} = 334\text{mm}
$$

That's about a $$\times 3$$ improvement over 1000 mm!

We can measure the magnification with and without the lens.
- with: $$\frac{250}{1000} = 1:4$$
- without: $$\frac{250}{334} = 3:4$$

![closeup1.png]{: width="500"}

And there's the side-by-side comparison. It works!

**Clarification:** the close-up filter reduces the focal length to 145 mm, which is still in the range of the lens (45-200 mm), so why not just reduce the focal length without the filter? For this lens, a lower focal length gives a lower image distance (and it may be true for all zoom lenses), and consequently we just wouldn't be able to get as close to the object. Here's the full explanation from the man himself:

> What I may have failed to make clear enough during lecture is, while the modified focal length of 143mm with the close-up filter does lie within the range already offered by the 45-200mm zoom lens, with the filter you are achieving that focal length while the lens is configured for 200mm. In that configuration, the barrel is fully extended, producing an image distance of 250mm, and (applying the Gaussian lens formula) an object distance so of 334mm. If the lens without the closeup filter were configured for this same focal length of 143mm the barrel would not be fully extended, so the image distance would be less than 250mm (I don’t know how much less - it depends on the lens mechanics), and the object distance would be longer than 334mm. Or if the lens without the closeup filter were configured for a focal length of 200mm, even though the barrel is fully extended and the image distance is 250mm, the object distance would be 1000mm. Thus, the filter allows you to combine the maximum barrel extension (hence the longest possible image distance) with an otherwise impossibly short object distance, producing higher magnification.

## Eyeglasses II

**Q:** The professor's eyes are worse than he let on. His glasses actually have the prescription:
- right eye: -0.75 -1.00 axis 135
- left eye: -1.00 -0.75 axis 180

What's wrong with the professor's eyes?

**A:** The professor has myopia *and* astigmatism.

![astigmatism1.png]{: width="300"}

In glasses, astigmatism can be fixed with an additional cylindrical lens component (in addition to the concave lens needed for myopia); cylindrical refers to the curvature of the surface, not to the shape of the entire lens. Levoy drew the picture above to help explain, but unfortunately I don't understand it.

## Chromatic Aberrations For Humans

**Q:** Why don't humans experience chromatic aberration?

**A:** The word goes, our eye produces it, but our brain corrects it. In the lecture, someone asked a followup on the possibility that it has something to do with the reduced resolution of our retinas at the edges. Levoy thought it was an interesting point, and so do I.

# Unresolved Questions

These are questions I had. Skip if you're clueless ha

## Paraxial Approximation

I'm confused about whether the paraxial approximation means:
- the light ray makes a small angle with the horizontal
- the light ray hits the lens at a low height, i.e. giving a low $$e$$

If it's the first one, then I don't understand how the paraxial approximation is used  (via the Gaussian lens formula) for the derivation of macro depth of field, since the angle could be large. If it's the second one, then it allows the angle to be large, but that seems to destroy the trigonometric approximations.

## Frustrum Shape

I'm pretty certain that the in-focus planes corresponding to each object plane converge to the focal point and not the optical center. I'm just not sure if Levoy was also talking about a separate frustrum converging to the optical center somehow.

## Chromatic Aberrations

I'm confused about the mathematical basis for longitudinal and lateral CA. They are supposedly caused by variation in magnification and focal length, based on the wavelength of light. We can draw this connection by:
- relating both magnification and focal length to image and object distance by their respective definitions
- relating those to refractive index with the lens maker's formula
- relating the refractive index to wavelength

Considering object distance is fixed, what really matters is image distance. So how can the two types of CA occur separately? Why doesn't an achromatic doublet fix both types of CA, if it corrects the image distances for different wavelengths?

## Aberrations In General

I have more questions about all the aberrations.
- Why can't we fix lateral CA by stopping down the aperture, while we can do so for monochromatic oblique aberrations?
- How does stopping down the aperture help for any of the aberrations it can reduce? 
- What's the difference between spherical aberration and field curvature?
- What's the difference between coma and pincushion distortion?

[^cropfactor]: Possibly helpful note: The index of refraction is like the crop factor, in that we're comparing all measurements to a baseline measurement. For crop factor it was the 35 mm sensor size, and for index of refraction it is the speed of light.
[^change]: I modified the notation from Levoy so it would be easier to label refractions on both the left and right side, and mostly finished the derivation of the Lensmaker's formula, which he skips for brevity. I basically follow Hecht, except he makes the transmitted rays from the left-side interface converge to a real image at $$P$$ (figure 5.6), but when introducing the right-side interface, he makes the same rays diverge for no apparent reason, making a virtual image at $$P'$$ (figure 5.14).
[^size]: The formula technically doesn't say anything about relative sizes, but it's trivial to show the relationship with magnification, which we show later in this note.
[^wrong]: I think the lens has a minimum focus distance of $$1000\text{mm}$$, but in the slides Levoy says minimum object distance. There may be some kind of discrepancy in the numbers but the example is still illustrative so I kept it.
[^conj]: Levoy gives that explanation in the lecture, and here I explain his explanation. The denominator is $$f^4-N^2C^2U^2$$, and by algebra we can basically discard the right term if $$f^4 \gg N^2C^2U^2$$. Here's the simplification: 
    
    $$
    \begin{align*}f^4&\gg N^2C^2U^2\\ f^2&\gg NCU\\ \frac{f}{N}&\gg\frac{CU}{f}\end{align*}
    $$
    
    The left side is the aperture, and the right is the conjugate circle of confusion.
    
[^yourself]: If you just want to jump between formulas, all you really need is (1) $$M=\frac{f}{U-f}$$, introduced in the Magnification section, and (2) $$H=\frac{f^2}{NC}+f\approx \frac{f^2}{NC}$$; when we introduced they hyperfocal distance we used the approximation to start, but the true expression may be necessary.
[^projellipse]: I found this [SO thread](https://math.stackexchange.com/questions/2385120/area-of-a-circle-as-it-changes-to-an-ellipse-when-viewing-at-different-angles) on the topic.

[optics2kinds.png]: /assets/images/optics/optics2kinds.png
[snellslaw.png]: /assets/images/optics/snellslaw.png
[mirage1.jpg]: /assets/images/optics/mirage1.jpg
[mirage2.png]: /assets/images/optics/mirage2.png
[refractionlens.png]: /assets/images/optics/refractionlens.png
[twocircles.png]: /assets/images/optics/twocircles.png
[convexconcave.png]: /assets/images/optics/convexconcave.png
[convexconcave1.png]: /assets/images/optics/convexconcave1.png
[gaussray1.png]: /assets/images/optics/gaussray1.png
[paraxial.png]: /assets/images/optics/paraxial.png
[paraxial1.png]: /assets/images/optics/paraxial1.png
[paraxial2.png]: /assets/images/optics/paraxial2.png
[paraxial3.png]: /assets/images/optics/paraxial3.png
[paraxiala.png]: /assets/images/optics/paraxiala.png
[changefocus.png]: /assets/images/optics/changefocus.png
[gaussray1.png]: /assets/images/optics/gaussray1.png
[frustrum.png]: /assets/images/optics/frustrum.png
[frustrum2d.png]: /assets/images/optics/frustrum2d.png
[frustrum2d_lens.png]: /assets/images/optics/frustrum2d_lens.png
[perspectivetransform.png]: /assets/images/optics/perspectivetransform.png
[dofformula1.png]: /assets/images/optics/dofformula1.png
[dofformula2.png]: /assets/images/optics/dofformula2.png
[dofformula3.png]: /assets/images/optics/dofformula3.png
[doffnumber.png]: /assets/images/optics/doffnumber.png
[dofsubjectdistance.png]: /assets/images/optics/dofsubjectdistance.png
[doffocallength.png]: /assets/images/optics/doffocallength.png
[bokeh.jpg]: /assets/images/optics/bokeh.jpg
[dofocclusion.png]: /assets/images/optics/dofocclusion.png
[prism.png]: /assets/images/optics/prism.png
[chromaticaberration.png]: /assets/images/optics/chromaticaberration.png
[lateralab.jpg]: /assets/images/optics/lateralab.jpg
[chromaticab.gif]: /assets/images/optics/chromaticab.gif
[lenstypes.png]: /assets/images/optics/lenstypes.png
[softfocus.jpg]: /assets/images/optics/softfocus.jpg
[expkernel.png]: /assets/images/optics/expkernel.png
[coma.png]: /assets/images/optics/coma.png
[astigatism.png]: /assets/images/optics/astigatism.png
[astigmatismopthalmic.png]: /assets/images/optics/astigmatismopthalmic.png
[fieldcurvature.png]: /assets/images/optics/fieldcurvature.png
[distortion.jpg]: /assets/images/optics/distortion.jpg
[veilingglare1.jpg]: /assets/images/optics/veilingglare1.jpg
[veilingglare.png]: /assets/images/optics/veilingglare.png
[lensflare.jpg]: /assets/images/optics/lensflare.jpg
[vignetting.png]: /assets/images/optics/vignetting.png
[vignetting1.png]: /assets/images/optics/vignetting1.png
[vignetting2.png]: /assets/images/optics/vignetting2.png
[telephoto.png]: /assets/images/optics/telephoto.png
[telephoto1.png]: /assets/images/optics/telephoto1.png
[telephoto2.png]: /assets/images/optics/telephoto2.png
[zoom.png]: /assets/images/optics/zoom.png
[lenssoftware.png]: /assets/images/optics/lenssoftware.png
[eyeproblems.png]: /assets/images/optics/eyeproblems.png
[closeup.png]: /assets/images/optics/closeup.png
[closeup1.png]: /assets/images/optics/closeup1.png
[astigmatism1.png]: /assets/images/optics/astigmatism1.png