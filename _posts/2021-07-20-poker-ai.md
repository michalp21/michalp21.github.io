---
layout: post
title: "Project Reupload #2: Poker AI"
categories: project
tags: game poker reinforcement-learning
---

> I'm writing posts on my old projects. In this post, I'll introduce my final project for one of my classes in junior year, a simplified implementation of a state-of-the-art poker AI.

<!--more-->

## AI Poker
Zoom in to my COMS4995 lecture in Spring of 2018, where I was discussing with two random classmates, Dan and Ethan, what we should do for our class project. We all recently learned about [AlphaGo's][ag] victory against Lee Sedol in Go, and Ethan liked poker, so we thought, *maybe someone made an AlphaGo for poker*. Turns out in January of that year, a duo from Carnegie Mellon presented Libratus, an algorithm that beat four top human professionals at **heads-up no-limit (HUNL) Texas Hold'em poker**:
- Texas Hold'em: the most popular "core rules" of poker, including dealing and betting
- No-limit: bets are not fixed, and are bounded by a predetermined minimum and an all-in maximum
- Heads-up: two players

The immediate question: did they more or less apply the method from AlphaGo to HUNL poker? The short answer is no.

Even considering a two-player version, poker is different from Go in several aspects:
- Drawn cards introduce randomness.
- Each player's hand is hidden from the other.

While the first point complicates evaluation of the stronger player, the second point is more important. In Go, both players always know the exact state of the game, but in Poker, players have to deal with **imperfect information**. This renders useless the core methods of AlphaGo and its successors.

Like AlphaGo, no official open-source code was released, but unlike AlphaGo, nobody had taken a shot at an unofficial implementation. So Ethan, Dan, and I decided to create an open-source version of Libratus, with the added goal of aiding the research community. Enthusiasm and naivette: the perfect combo!

![noam]
*Objection: it needs 25 million core hours anyway so what's the big deal??*

## Process
Here's a silly timeline of my experiences that spring:
1. This notation is incomprehensible
2. There is no deep learning
3. This is actually a super complicated project but we're past project milestone...
4. We cracked a core algorithm!
5. Wait we have to write a poker simulator
6. Wait we have to interface with ACPC in order to benchmark
7. Wait the class is over??

I'll walk through some of it.

### Notation
At the time I had zero experience with poker or game theory, crucial components of Libratus (and only brief exposure to reinforcement learning, which hindered understanding of related game AI literature). It would take almost a whole week to get through a paper. I remember griping over the Notation and Background section of the paper [Safe And Nested Subsolving For Imperfect-Information Games][sns].

### Deep Learning
We were initially going to implement [DeepStack][ds], a poker AI from 2017 that utilized deep learning. When we switched to Libratus, I think I just assumed there would be deep learning, because it seemed like the only way to tackle super challenging games like Go and Poker. I was shocked to find that Libratus did *not* use deep learning and still performed better than DeepStack.

### Project Milestone
Towards the middle of our project we realized just how complicated Libratus was. I thought we would be dealing with something like [AlphaGo Zero][agz] (the first successor to AlphaGo) which elegantly combined MCTS with a two-headed policy and value network, and removed "human data, guidance or domain knowledge beyond game rules."

I discovered, however, that AlphaGo Zero was an anomaly among game AIs, and for Libratus, domain knowledge was just as important as game theory. Even worse, we were well into the project already, and we faced the all-too-common dilemma of building a complete, faithful implementation versus having something presentable at the end of the semester. For example we may not have fully implemented card and action abstractions, which are basically groupings of cards and actions to reduce the number of possibilities per turn.[^forgot]

It was at this point too that I became a bit jaded about benchmarking AIs with games. I might write another post about it, but in summary, the world-class poker bot transformed in my mind from a leap forward for AI into just another game someone was able to build an algorithm for.[^same] Not a unique view at all, and not a slight against Libratus either; it's still an unprecedented feat.

### Core Algorithm Cracked
We had one major win, successfully writing MCCFR-p, the algorithm which computes a so-called blueprint strategy. We started by reading up on the more basic CFR algorithm (this [paper][cfr] specifically), and writing a basic implementation for Khun poker, a toy version of poker. We were then able to translate this to the more sophisticated algorithm on a bigger poker game.

### Simulator and ACPC server
The last stretch saw algorithmic challenges almost wholly replaced by engineering ones. We had to finalize the poker simulator, so a) we could demo the game and b) to simulate valid moves while pre-training the strategies. We wrote rules for Leduc poker which reduced the game size to something trainable on our puny machines (but still larger than Kuhn poker), but introduced its own difficulties because it's a lesser known ruleset, and we had trouble validating it. Finally we needed to implement gameplay with other bots via an ACPC server. Luckily we found a Python wrapper and plugged it in.

## Takeaways
I at least was completely burnt out by the end, and just when we were getting a hang of things we put a bow on it and never touched it again. I highly doubt it was helpful to researchers, as we had hoped. The message "This is a work in progress" sits on the readme like a dead open sign on a dilapidated storefront. It's all kind of a regret of mine! Still I have to remember this was my first class with a final project, and I learned an incredible amount in a short period of time. It was also my first attempt at reading through the literature of a certain field and trying to implement something in it from scratch. The repo is [here][libratus] for the curious.

The research world doesn't slow down. Noam Brown has continued putting out excellent work, such as [Pluribus][pluribus] for multiplayer poker, and [ReBeL][rebel], which utilizes deep reinforcement learning. Someone seems to be working on an [open-source] implementation of Pluribus, and an official repo exists for ReBeL implemented on a different game called [Liar's Dice][dice]. I'm excited to see what the future holds for poker AIs!

[^forgot]: Unfortunately I forgot a lot of the details, and looking at it now, our code isn't a paragon of best practices.
[^same]: Same applies to all versions of AlphaGo by the way.

[ag]: https://www.nature.com/articles/nature16961
[ds]: https://science.sciencemag.org/content/356/6337/508
[agz]: https://www.nature.com/articles/nature24270
[noam]: /assets/images/noam.png
[sns]: https://arxiv.org/abs/1705.02955
[cfr]: http://modelai.gettysburg.edu/2013/cfr/cfr.pdf
[libratus]: https://github.com/michalp21/coms4995-finalproj
[pluribus]: https://www.cs.cmu.edu/~noamb/papers/19-Science-Superhuman.pdf
[rebel]: https://arxiv.org/abs/2007.13544
[open-source]: https://github.com/fedden/poker_ai
[dice]: https://github.com/facebookresearch/rebel