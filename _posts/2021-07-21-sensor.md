---
layout: post
title: "Project Reupload #3: VI-sensor"
categories: project
tags: vio slam computer-vision hardware sensor
---

> I'm writing posts on my old projects. Here I will introduce one of my most ambitious projects (at the time), a visual-inertial sensor for use with SLAM.

<!--more-->

## Tele-Cube

For this project, I tried to make a virtual cube move based on sensor movement. 

{% include callout.html content="That's trivial. There are cheap sensors with out-of-box demos for this." icon="eyeroll" %}

![sparkfunimu]
*Straight from the Sparkfun website*

I could have made that my project but I wanted to do more. A rotation demo shows three degrees of (rotational) freedom. That's like knowing how an airplane is oriented without knowing where it actually is in the sky. I wanted an extra three degrees of freedom along positional axes.

{% include callout.html content="Any IMU will have an accelerometer and a gyroscope, one for calculating position and one for rotation." icon="eyeroll" %}

The problem with IMUs is that the accelerometer directly measures acceleration. To get a position estimate, the IMU has to double integrate over the acceleration. The error from this operation accumulates pretty quickly and the result is a drifting position estimate, which becomes useless pretty quickly. The gyroscope has a similar problem.

The problem of using sensor data to determining change of position and orientation, or pose, over time is called **odometry**. IMUs are not capable of good odometry on their own,[^imubad] so one improvement is to combine multiple sensors with different properties together, like a visual-inertial (VI) sensor, and perform **visual-inertial odometry**. Yet another improvement is to use a more sophisticated algorithm,[^sophisticated] called **simultaneous localization and mapping (SLAM)**.

## The Hardware

A good VI sensor, like the discontinued [Skybotix VI-sensor][visensor] below, uses one or more high quality cameras and IMUs, with hardware synchronization and god-tier calibration.

![visensor]
*For just  â‚¬3900.00!*

Obviously my sensor was a bit lower spec. I decided to use an open-source SLAM algorithm called ROVIO, which requires hardware synchronization of the camera and IMU, and a global shutter camera. To satisfy those two points I got a Point Grey Firefly MV camera, the cheapest global shutter camera I could find with an external triggering capability. My IMU on the other hand was a cheapy from Sparkfun, which at least incorporated a magnetometer to improve measurements a bit. The IMU supports Arduino onboard, which I used to trigger the camera shutter at regular intervals. I 3D-printed a holder to keep both sensors locked in place. The sensors needed to be calibrated, both as individual sensors and together as a unit. The IMU came with a handy calibration tutorial, and [Kalibr](https://github.com/ethz-asl/kalibr) took care of the rest.

![visensor2]
*My VI sensor .\_.*

The sensor ended up being the focus of the project, but remember it was just part of a whole pipeline. It needed to feed its data to a computer, which needed to run ROVIO, feed the pose data to a graphics engine, and rotate a cube.

I wanted to keep a fairly mobile system so I could *in principle* mount everything on a drone, which ruled out a desktop computer. I tried the very low power Raspberry Pi, but it was too weak to run SLAM, ruling out most single board computers. I chose Unity to handle the virtual cube, so I chose Ubuntu 16 for its compatibility with Unity and ROS (for ROVIO). The smallest system I could find that could run a full Ubuntu OS and had enough processing power to run ROVIO was an Intel NUC. Finally I used a battery pack to power the NUC.

![glow]
*I thought this was a cool picture when I took it. Shows a NUC, my laptop, and an external drive.*

## Note on Money

One flaw in this tidy little blog post is the omission of all the failed approaches I tried first, many which involved acquiring some special hardware.

A small investment can go a long way. I would not have finished this project without some key purchases like the IMU or camera. As I realized, however, the purchases eventually hit diminishing returns. I ended up buying things and then returning them when I realized they didn't quite fit my needs, or holding onto them to this day. Some of those items were somewhat cheap, like the Raspberry Pi, and some were not (like an eGPU...yeah...I later tried to make up for it by cryptomining).

Some of the waste can be offset by spending even more time asking knowledgeable people, either online or in real life. Research is also hit by diminshing returns though, eventually resulting in a paralysis of ambivalence. You will eventually have to make a purchase and sometimes there's no better way to know if it works than to buy it and try it.

So while I did spend an uncomfortable amount of internship earnings on this project, I learned a few tips to squeeze the most efficiency out of my time and money. In order, I recommend:
1. Sticking to software instead of hardware if possible.
2. Reducing the scope of the project.
3. Taking advantage of a lab or makerspace.
4. Using Amazon Prime.[^sponsor]

Notice choosing software projects comes *first!* Nearly everything software-related outside of a production environment can be done for free. *Maybe* you pay a few bucks for hosting or cloud services. For hardware projects, the other tips should help. To explain Amazon Prime a bit more, I found it useful both for quick deliveries and fully refunded returns, allowing me to iterate quickly. Students get a discount, and a free trial is available to anyone.

## Results

After many painful months, I did manage to move the virtual cube!

![videmo]
*Please hold your applause*

A video feed from the camera is displayed in the upper left corner, and the cube is rotated and translated to match the pose of the sensor in real time.

Well, there was actually some noticeable lag in the movement, and I identified a few sources:
- At the time, I was using a package called ros-sharp to pass ROS video frames to Unity. I think either they were raw instead of compressed image frames, or they were being passed with JSON instead of BSON.
- The method to display the video feed in Unity may have been inefficient. The demo ran more smoothly without video.
- As my advisor kindly pointed out, SLAM is intensive enough that many mobile/embedded SLAM systems have a VPU dedicated to underlying computer vision procedures. My homemade version obviously lacked anything of the sort, but the NUC did have a decent general purpose CPU. I wasn't able to test if SLAM alone was a bottleneck, but I did notice a performance difference with and without the screen recording running.

I wish I had more time to fix the display issues, but at least ROVIO runs smoothly, meaning my sensor worked too. I even managed to get the whole thing working off the battery, carrying it around in a backpack setup. I will add a picture if I can find it or recreate it.

You may be wondering things like:
- Why not just display the video straight from ROS?
- Why not use a laptop instead of a NUC?
- Why use Unity instead of say Gazebo?
- Why use SLAM at all?

Some choices can be excused by incomplete knowledge, some were completely asinine, and all were cut off from a satisfactory level of deliberation in my commitment to the MVP.

## Takeaways

As usual, I ran into issues with project scope. Unlike my [Wisp]({{ site.baseurl }}{% post_url 2021-07-19-wisp %}) project, I *never* added features, but instead the true scale of the project was completely hidden from me since I lacked experience with the field (even worse than the [poker project]({{ site.baseurl }}{% post_url 2021-07-20-poker-ai %})). I thought okay the state-of-the-art is in SLAM so I'll do that! When I realized how complicated everything was, I ended up using an off-the-shelf SLAM in order to reach the MVP. There's nothing wrong with using a prebuilt package, but I could have chosen from a more lightweight family of algorithms like VIO, which would have lowered requirements on both the sensor components and computer. Additionally, I was unable to debug it when things went awry. I actually started with a completely different SLAM algorithm called VINS-MONO, but had to ditch it because I simply couldn't get it to run.

My number one priority after building the sensor was to dive more deeply into SLAM and related algorithms. Still, basic knowledge of the sensor and how it fits into the whole pose estimation pipeline is invaluable. I was exposed to tons of concepts about cameras, lenses, and basic photography that I find fascinating even now. Most of all I dove into the completely new fields of robotics and computer vision and came out with something to show for it. Without a doubt, one of my favorite projects.

I will release specific tutorials I put together during the project.

\[Update 2021-08-08\]: Tutorials [uploaded]({{ site.baseurl }}{% post_url 2021-08-07-sensor-tutorials %})!

[^imubad]: In particular, I mean cheap MEMS IMUs.
[^sophisticated]: VIO is more complicated than one-sensor odometry by virtue of the additional sensor modality. SLAM is more sophisticated than VIO for reasons other than the particular sensor(s) used, namely mapping and loop-closure.
[^sponsor]: Not sponsored I promise.

[sparkfunimu]: /assets/images/sparkfunimu.gif
[visensor]: /assets/images/visensor.png
[visensor2]: /assets/images/visensor2.png
[glow]: /assets/images/glow.jpg
[videmo]: /assets/images/videmo.png