---
layout: post
title: "What Isn't Model-Based RL?"
categories: article
tags: reinforcement-learning model
disclaimers: ianae
---

> Having only encountered model-free RL, I had a hard time pinning down what made a model-based RL algorithm for some reason, and here is my shot at the difference. This is not exactly a tutorial or anything, just an attempt to clear up some confusions I had with various terms. I try not to be too prescriptive.

<!--more-->

## Let's Start With What It Is

We start with an environment that can be represented as an MDP. Our goal is to find the optimal policy of an agent in this MDP.

The **dynamics** of an MDP are the state transitions and rewards. A **model** is just some representation of the dynamics. Knowing the dynamics is equivalent to having a model.

Sutton and Barto {% cite Sutton1998 %} confirms this:
> By a model of the environment we mean anything that an agent can use to predict how the environment will respond to its actions.

There are two types of a models:
- **Distribution model**: gives probabilities of all transition events as the full distribution $$p(r,s'\vert s,a)$$.
- **Sampling model**: provides transition samples, i.e. a reward $$r$$ and next state $$s'$$, given state $$s$$ and action $$a$$.

The model may or may not be learned by the agent. Unlike the dynamics which are inherent in the MDP, the model is an aspect of the agent or algorithm, and is totally optional.[^complete]

If we have (and use) a model, then it becomes a **planning** problem. If we don't have a model, then we have a **learning** problem.

We can see the same definition in Sutton and Barto:
> The heart of both learning and planning methods is the estimation of value functions by backing-up update operations. The difference is that whereas planning uses simulated experience generated by a model, learning methods use real experience generated by the environment {% cite Sutton1998 %}.

They emphasize the usage of simulated vs real experience. Models are clearly simulators, as the experiences they generate are not from the real world dynamics themselves, i.e. real life. 

{% include callout.html content="Then if model == planning, and no model == learning, what the heck is model-based reinforcement learning?" icon="browraise" %}

In model-based reinforcement learning (RL), we have to **learn** a model from **real experience**, and use it to **plan** the optimal policy from **simulated experience**. It quite simply smashes planning and learning together.

![mbrl]
*Source: David Silver {% cite silver2015 %}*

{% include callout.html content="Why not model-free reinforcement planning?" icon="browraise" %}

Probably just a quirk of history. Check out section 1.7 of {% cite Sutton1998 %}! Nowadays, "reinforcement" in RL probably serves to separate it from supervised learning as a learning paradigm, and the addition of "model-based" identifies a class of RL that incorporates planning.[^rp]

## Sanity Check
First some simpler conclusions:
1. Model-based is *not* the same as model-based RL. Out of the four classes of model-based methods in [this blog post](https://bair.berkeley.edu/blog/2019/12/12/mbpo/), only the last two seem to be model-based RL.[^familiar]
2. The kind of model we have (distribution or sampling) is separate from knowledge of the dynamics, and in fact assumes we *do* know the dynamics.
3. If we know the dynamics, then there is no room for learning. On the other hand, if we refuse to know anything about the dynamics, there is no room for a model. For this reason, we could say *model-based* RL is an attempt to *learn the dynamics*, in addition to learning the optimal policy.
4. RL in general is *not* distinguished by the goal of learning the dynamics.

## What It Isn't
### Other Senses of Model
Since I brought up supervised learning, I ought to clarify an unfortunate overlap in terminology. "Model" as described above is *not* the same as a supervised machine learning "model," which rather refers to the output of a machine learning algorithm, without (necessarily) any connection to dynamics. The term "function approximator" is used to avoid confusion when supervised learning models are used in RL algorithms, for example in Deep Q-Learning.
 
 Yet another possible usage is a description of the MDP as a "model" of the environment. The assumption of an environment represented as an MDP characterizes the class of algorithms we consider in the first place, and therefore anticipates the model-based/model-free distinction in RL.
 
### Value Functions
Both model-based and model-free RL may estimate value functions, which are not models. They influence how new experience is obtained, but value functions represent expected cumulative returns instead of the dynamics.

A model could still be necessary, due to our goal of finding the optimal policy. An RL algorithm could learn model-free all the way up to outputting a state-value function $$v$$. Strictly speaking, to finish the problem, a model (such as four-argument $$p$$) is still required to extract the policy (if the situation allows it).

$$\pi^*(s) = \text{argmax}_a \sum_{r,s'}p(r,s'\vert s,a)(r + \gamma v^*(s'))$$

The action-value function $$q$$ does not have this problem:

$$\pi^*(s) = \text{argmax}_a q^*(s,a)$$
 
### Other Senses of Simulator
The colloquial definition of a simulator from Merriam-Webster is:
> a device that enables the operator to reproduce or represent under test conditions phenomena likely to occur in actual performance

Confusion is likely to arise due to the two senses of the word "simulator": one as programs or devices, like physics simulators and flight simulators, and the other as models of an MDP's dynamics. I'll just go ahead and claim *all simulators in the first sense are also simulators in the second sense*. Is a simulation program not a handcrafted model of the real world? Then within a simulator, there is technically no learning involved, as all examples are simulated and the simulator itself is not learned. If we want to keep thinking of it as RL, we could instead learn a policy within the simulator *for* the simulator, and just cross our fingers that it transfers well to the real world.

Is that really a good reason to call something model-based? Is A3C done in a physics engine all of a sudden a planning algorithm? Well, A3C could work on real-world data *in principle* so maybe that's a good enough reason to continue to call it a model-free RL algorithm, absent any context. Then I think it would be permissible to say A3C can be used for planning or model-free RL depending how it's used.

Yet another source of confusion is the fact that some "simulated" environments are actually the target environment – no finger-crossing needed. When we're training an AI to learn Go, we aren't interested in learning to move the physical pieces on a physical board. All we care about is mastering the mathematical formulation of the game. All gameplay occuring in a Go simulator should therefore be considered real experience for our particular setup. Maybe we could instead use "virtual" to desribe these situations, as it's different enough from the word "simulated" while still capturing the non-reality of the domain of interest.

### Data Storage and Reuse
An isolated experience $$(s, a, r, s')$$, encountered, used once, and discarded by the agent, *does not count* as a model. That should be obvious: if any interaction with the environment whatsoever counted as a model, then all RL would be model-based. Temporarily storing several experiences or even whole episodes before calculating returns, like in Monte Carlo methods, is not much different.

Inching closer to model-based methods, we have experience replay (ER), which stores experiences in a buffer, and samples from them later, either individually or in mini-batches, randomly or with a heuristic priority. The experiences are potentially used *multiple times* before being deleted to make room for new ones. The reuse of experience constitutes a sort of data augmentation, which is really what a model does during simulation. Unfortunately, we can't query it at arbitrary states unless they are present in the buffer, which means we can't use the buffer to run trajectories forward or backward {% cite Pan9780999241127 %}.

Are those essential for a model? Vanilla Dyna-Q {% cite Sutton1998 %} is supposedly model-based, yet it only samples randomly from previous experiences.

![dynaq]
*Just looks like ER to me.*

Meanwhile, {% cite NEURIPS2019_1b742ae2 %} seems to think it's a salient distinction:
> On the other hand, a replay memory is less flexible than a model, since we cannot query it at arbitrary states that are not present in the replay memory.

They go on to use Dyna-Q as an example of model-based RL, *but* use a multi-layer perceptron (or more importantly, a parametric function approximator, in opposition to nonparametric ER) to model transitions, terminations, and rewards, allowing it to sample unseen states. Of course they do that, but then their definition of planning is so loose that it includes ER again {% cite NEURIPS2019_1b742ae2 %}:
> any algorithm that uses additional computation to improve its predictions or behaviour without consuming additional data.

There are bound to be more examples I haven't seen that straddle the boundary between temporary storage and model, so I won't really offer my opinion here.

{% include callout.html content="I don't care about your opinion. Is ER a model?" icon="browraise" %}

Based on my extensive survey of two papers, I'll conclude that the literature overall separates ER from models, though with much hedging. 

## Examples

Now I'm going to pretend like my definitions inform my classification of existing RL algorithms, and not the other way around. Summarizing:
- DP methods use models, and are not RL. The dynamics are explicitly used to calculate an optimal policy.[^dynamic]
- MC, TD, and policy gradient methods are model-free RL assuming Q functions are used instead of V functions. Dynamics not required.
- MCTS uses a model for the rollout phase, but it's not learned, so it's a planning algorithm (the use of a virtual target environment like a Go program doesn't change this).
- Deep Q-Learning is model-free RL, since we decided ER isn't a model.
- Dyna-Q is model-based RL. Because "model" is written in the pseudocode.

[^complete]: Barto+Sutton use the game of blackjack as an example to distinguish a distribution model (full dynamics) from some other "complete knowledge": "Although we have complete knowledge of the environment in this task, it would not be easy to apply DP methods to compute the value function. DP methods require the distribution of next events—in particular, they require the environments dynamics as given by the four-argument function $$p$$—and it is not easy to determine this for blackjack. For example, suppose the player’s sum is 14 and he chooses to stick. What is his probability of terminating with a reward of +1 as a function of the dealer’s showing card?" To be honest I'm still not sure what "complete knowledge of the environment" refers to. Maybe it refers to fully observable state, which is true of blackjack, and allows us to use an MDP. Maybe he's saying the rules can be used as a sampling model but not a distribution model. Or it could be a game theory term I'm unfamiliar with.
[^rp]: Then unless there are other major planning paradigms, the term "reinforcement planning" is not so useful anymore.
[^familiar]: I'm not familiar with value-equivalence prediction though so I could be wrong.
[^generative]: Supervised learning has a somewhat parallel distinction between generative and discriminative models that I kind of want to write a post about.
[^dynamic]: So "dynamic" in dynamic programming has nothing to do with wham bam presto forte after all. See [this answer][prestoforte] on StackOverflow.

[mbrl]: /assets/images/mbrl.png
[dynaq]: /assets/images/dynaq.png
[prestoforte]: https://cstheory.stackexchange.com/a/5643

https://www.ijcai.org/proceedings/2018/0666.pdf