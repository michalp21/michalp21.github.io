<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-11-02T10:14:13-05:00</updated><id>/feed.xml</id><title type="html">Xn</title><subtitle>Personal Site</subtitle><author><name>Michal Porubcin</name></author><entry><title type="html">Digital Photography Notes - Optics</title><link href="/note/2021/11/01/optics.html" rel="alternate" type="text/html" title="Digital Photography Notes - Optics" /><published>2021-11-01T00:00:00-05:00</published><updated>2021-11-01T00:00:00-05:00</updated><id>/note/2021/11/01/optics</id><content type="html" xml:base="/note/2021/11/01/optics.html">&lt;blockquote&gt;
  &lt;p&gt;This note is based on lectures 3 and 4 of Levoy’s Digital Photography. It covers basic geometric optics starting with refraction, and ends with aberrations and other lens effects. These lectures are more math-heavy to begin with, and on top of that I tried to round out the proofs. Luckily it’s nothing advanced.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#physical-vs-geometrical-optics&quot; id=&quot;markdown-toc-physical-vs-geometrical-optics&quot;&gt;Physical vs Geometrical Optics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#refraction&quot; id=&quot;markdown-toc-refraction&quot;&gt;Refraction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lens-shapes&quot; id=&quot;markdown-toc-lens-shapes&quot;&gt;Lens Shapes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#gaussian-lens-formula&quot; id=&quot;markdown-toc-gaussian-lens-formula&quot;&gt;Gaussian Lens Formula&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#simple-derivation&quot; id=&quot;markdown-toc-simple-derivation&quot;&gt;Simple Derivation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#paraxial-approximation&quot; id=&quot;markdown-toc-paraxial-approximation&quot;&gt;Paraxial Approximation&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#uses-for-the-gaussian-lens-formula&quot; id=&quot;markdown-toc-uses-for-the-gaussian-lens-formula&quot;&gt;Uses For The Gaussian Lens Formula&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#changing-object-distance-revisited&quot; id=&quot;markdown-toc-changing-object-distance-revisited&quot;&gt;Changing Object Distance (Revisited)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#virtual-images-and-objects&quot; id=&quot;markdown-toc-virtual-images-and-objects&quot;&gt;Virtual Images and Objects&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#concave-lenses&quot; id=&quot;markdown-toc-concave-lenses&quot;&gt;Concave Lenses&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#power-of-a-lens&quot; id=&quot;markdown-toc-power-of-a-lens&quot;&gt;Power of a Lens&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#magnification&quot; id=&quot;markdown-toc-magnification&quot;&gt;Magnification&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#hardware-for-magnification&quot; id=&quot;markdown-toc-hardware-for-magnification&quot;&gt;Hardware For Magnification&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#3d-perspective-transform&quot; id=&quot;markdown-toc-3d-perspective-transform&quot;&gt;3D Perspective Transform&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#depth-of-field&quot; id=&quot;markdown-toc-depth-of-field&quot;&gt;Depth of Field&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#derivation&quot; id=&quot;markdown-toc-derivation&quot;&gt;Derivation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#hyperfocal-distance&quot; id=&quot;markdown-toc-hyperfocal-distance&quot;&gt;Hyperfocal distance&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#macro-depth-of-field&quot; id=&quot;markdown-toc-macro-depth-of-field&quot;&gt;Macro Depth of Field&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#other-derivations&quot; id=&quot;markdown-toc-other-derivations&quot;&gt;Other Derivations&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#depth-of-field-in-practice&quot; id=&quot;markdown-toc-depth-of-field-in-practice&quot;&gt;Depth of Field In Practice&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#basic-relationships&quot; id=&quot;markdown-toc-basic-relationships&quot;&gt;Basic Relationships&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#dolly-zoom&quot; id=&quot;markdown-toc-dolly-zoom&quot;&gt;Dolly Zoom&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#sensor-size&quot; id=&quot;markdown-toc-sensor-size&quot;&gt;Sensor Size&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#circle-of-confusion&quot; id=&quot;markdown-toc-circle-of-confusion&quot;&gt;Circle of Confusion&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#bokeh&quot; id=&quot;markdown-toc-bokeh&quot;&gt;Bokeh&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#seeing-through-occlusions&quot; id=&quot;markdown-toc-seeing-through-occlusions&quot;&gt;Seeing Through Occlusions&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lens-aberrations&quot; id=&quot;markdown-toc-lens-aberrations&quot;&gt;Lens Aberrations&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#chromatic-aberrations&quot; id=&quot;markdown-toc-chromatic-aberrations&quot;&gt;Chromatic Aberrations&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#spherical-aberrations&quot; id=&quot;markdown-toc-spherical-aberrations&quot;&gt;Spherical Aberrations&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#coma&quot; id=&quot;markdown-toc-coma&quot;&gt;Coma&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#astigmatism&quot; id=&quot;markdown-toc-astigmatism&quot;&gt;Astigmatism&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#field-curvature&quot; id=&quot;markdown-toc-field-curvature&quot;&gt;Field Curvature&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#distortion&quot; id=&quot;markdown-toc-distortion&quot;&gt;Distortion&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#not-perspective&quot; id=&quot;markdown-toc-not-perspective&quot;&gt;NOT Perspective&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#other-lens-artifacts&quot; id=&quot;markdown-toc-other-lens-artifacts&quot;&gt;Other Lens Artifacts&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#veiling-glare&quot; id=&quot;markdown-toc-veiling-glare&quot;&gt;Veiling Glare&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#lens-flare&quot; id=&quot;markdown-toc-lens-flare&quot;&gt;Lens Flare&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#vignetting&quot; id=&quot;markdown-toc-vignetting&quot;&gt;Vignetting&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#diffraction&quot; id=&quot;markdown-toc-diffraction&quot;&gt;Diffraction&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lens-systems&quot; id=&quot;markdown-toc-lens-systems&quot;&gt;Lens Systems&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#telephoto&quot; id=&quot;markdown-toc-telephoto&quot;&gt;Telephoto&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#zoom&quot; id=&quot;markdown-toc-zoom&quot;&gt;Zoom&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#lens-design&quot; id=&quot;markdown-toc-lens-design&quot;&gt;Lens Design&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#examples&quot; id=&quot;markdown-toc-examples&quot;&gt;Examples&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#eyeglasses-i&quot; id=&quot;markdown-toc-eyeglasses-i&quot;&gt;Eyeglasses I&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#macro-lenses&quot; id=&quot;markdown-toc-macro-lenses&quot;&gt;Macro Lenses&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#close-up-filters&quot; id=&quot;markdown-toc-close-up-filters&quot;&gt;Close-Up Filters&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#eyeglasses-ii&quot; id=&quot;markdown-toc-eyeglasses-ii&quot;&gt;Eyeglasses II&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#chromatic-aberrations-for-humans&quot; id=&quot;markdown-toc-chromatic-aberrations-for-humans&quot;&gt;Chromatic Aberrations For Humans&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#unresolved-questions&quot; id=&quot;markdown-toc-unresolved-questions&quot;&gt;Unresolved Questions&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#paraxial-approximation-1&quot; id=&quot;markdown-toc-paraxial-approximation-1&quot;&gt;Paraxial Approximation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#frustrum-shape&quot; id=&quot;markdown-toc-frustrum-shape&quot;&gt;Frustrum Shape&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#chromatic-aberrations-1&quot; id=&quot;markdown-toc-chromatic-aberrations-1&quot;&gt;Chromatic Aberrations&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#aberrations-in-general&quot; id=&quot;markdown-toc-aberrations-in-general&quot;&gt;Aberrations In General&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;physical-vs-geometrical-optics&quot;&gt;Physical vs Geometrical Optics&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/optics2kinds.png&quot; alt=&quot;optics2kinds.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Optics, the study of light, is typically divided into two branches:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;geometrical optics&lt;/strong&gt; characterizes light primarily as rays&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;physical optics&lt;/strong&gt;, or wave optics, characterizes light as a wave, with light rays drawn perpendicular to the wave&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That said, waves do give some nice intuition even in the first regime. The main difference is that geometrical optics &lt;em&gt;doesn’t&lt;/em&gt; account for the effects of diffraction and interference. It’s a useful approximation when the light’s wavelength is much smaller than the objects it interacts with.&lt;/p&gt;

&lt;h1 id=&quot;refraction&quot;&gt;Refraction&lt;/h1&gt;

&lt;p&gt;Snell’s Law explains what happens to light when it passes between two transparent substances.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/snellslaw.png&quot; alt=&quot;snellslaw.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The image above shows two mediums meeting at an &lt;strong&gt;interface&lt;/strong&gt;. In the left subimage, the waves of light approach the interface at an angle of \(\theta_i\) above the horizontal, with speed \(v_i\), and recedes at angle \(\theta_t\) below the horizontal, with speed \(v_t\). In other words, the speed and direction of the light has changed.&lt;/p&gt;

&lt;p&gt;Here’s an intuitive explanation. Knowing that the velocity of light decreases in the second medium, the waves travel less distance over time, so they’re closer together. Now, notice how the incoming waves line up with the exiting waves at the interface: no lines are lost and no lines are created from thin air. For this property to hold, the waves must exit at a smaller angle.&lt;/p&gt;

&lt;p&gt;We can instead look at the light rays, shown in the right subimage. Due to perpendicularity, we get equivalent angles \(\theta_i,\theta_t\) with respect to the vertical, or &lt;strong&gt;normal&lt;/strong&gt;. We get a few equalities:&lt;/p&gt;

\[\frac{x_i}{x_t} = \frac{\sin\theta_i}{\sin\theta_t} = \frac{v_i}{v_t}=\frac{n_t}{n_i}\]

&lt;p&gt;The first equality is from trigonometry, the second is from proportionality of \(x\) and \(v\), and the third is from defining the &lt;strong&gt;index of refraction&lt;/strong&gt;&lt;sup id=&quot;fnref:cropfactor&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:cropfactor&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; or refractive index as:&lt;/p&gt;

\[n_r=\frac{\text{speed of light in vacuum}}{\text{speed of light in medium r}}\]

&lt;p&gt;From the equations above we get the usual formulation of &lt;strong&gt;Snell’s Law&lt;/strong&gt;:&lt;/p&gt;

\[n_1\sin\theta_1 = n_2\sin\theta_2\]

&lt;p&gt;And wrapping up the example above, we notice a few things:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;light travels &lt;em&gt;more slowly&lt;/em&gt; and bends &lt;em&gt;closer to the normal&lt;/em&gt; through substances with &lt;em&gt;higher refractive indices&lt;/em&gt;&lt;/li&gt;
  &lt;li&gt;light does not refract when hitting the interface perpendicularly&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some common indices of refraction:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;air: ~1.0&lt;/li&gt;
  &lt;li&gt;water: 1.33&lt;/li&gt;
  &lt;li&gt;glass: 1.5 - 1.8&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/mirage1.jpg&quot; alt=&quot;mirage1.jpg&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Refractive indices change with temperature. For example, hot air has a lower refractive index than cool air. This is the source of some &lt;strong&gt;mirages&lt;/strong&gt;, like the commonly seen “wet road” phenomenon.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/mirage2.png&quot; alt=&quot;mirage2.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In a bit more detail: in places with a gradient of hot air up high to cool air on the ground, some faraway light rays moving partially downward will bend away from the normal, flattening out and even curving upward! When they reach your eyes, it will seem like they originated from a point below the ground.&lt;/p&gt;

&lt;h1 id=&quot;lens-shapes&quot;&gt;Lens Shapes&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/refractionlens.png&quot; alt=&quot;refractionlens.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When passing through a lens, light refracts twice:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;it bends closer to the normal when entering the lens&lt;/li&gt;
  &lt;li&gt;it bends away from the normal when exiting the lens&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Light hitting the lens perpendicularly does not bend. The only place the light hits perpendicularly when entering &lt;em&gt;and&lt;/em&gt; exiting the lens is on the &lt;strong&gt;principal or optical axis&lt;/strong&gt; (\(\overline{C_2C_1}\) above).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/twocircles.png&quot; alt=&quot;twocircles.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The two sides of a lens can be characterized by the radii of their respective circles (spheres in 3D). If they intersect, they form a &lt;strong&gt;convex lens&lt;/strong&gt;, otherwise, they form a &lt;strong&gt;concave lens&lt;/strong&gt;. They are also called &lt;strong&gt;positive&lt;/strong&gt; and &lt;strong&gt;negative&lt;/strong&gt; lenses, respectively.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/convexconcave.png&quot; alt=&quot;convexconcave.png&quot; width=&quot;200&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Lenses come in many shapes. Following convention, the radius is negative on the right half of a circle, and positive for the left half.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/convexconcave1.png&quot; alt=&quot;convexconcave1.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Assuming light rays move left to right, if the rays meet at a point to the right of the lens, they produce a &lt;strong&gt;real image&lt;/strong&gt;. If they diverge after the lens, the rays can be traced backwards, converging to a &lt;strong&gt;virtual image&lt;/strong&gt; on the left side.&lt;/p&gt;

&lt;p&gt;Likewise, if the rays originate from a point to the left of the lens, we have a &lt;strong&gt;real object&lt;/strong&gt;. If they come from a divergent source, they can be traced forward, meeting on the right side, forming a &lt;strong&gt;virtual object&lt;/strong&gt; (not pictured).&lt;/p&gt;

&lt;p&gt;Parallel rays are on the border of convergent and divergent. We treat them as real, rather than virtual, points at infinity.&lt;/p&gt;

&lt;h1 id=&quot;gaussian-lens-formula&quot;&gt;Gaussian Lens Formula&lt;/h1&gt;

&lt;h2 id=&quot;simple-derivation&quot;&gt;Simple Derivation&lt;/h2&gt;

&lt;p&gt;From now on let’s assume our object is in focus, so we don’t have to keep saying object/image &lt;em&gt;focus&lt;/em&gt; distance. There’s a pretty simple relationship between the object distance, the image distance, and the focal length that will help clarify all the virtual object/image business. In this derivation we make some pretty broad assumptions, in particular, that all parallel points pass through the focal point \(f\). We also use a convex lens for illustration, but the derivation works fine for concave lenses too.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/gaussray1.png&quot; alt=&quot;gaussray1.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Using similar triangles, we get:&lt;/p&gt;

\[\begin{align*}
\frac{\vert y_i \vert}{y_o} &amp;amp;= \frac{s_i}{s_o}\\
\frac{\vert y_i \vert}{y_o} &amp;amp;= \frac{s_i-f}{f}\\
\end{align*}\]

&lt;p&gt;Equating and rearranging, we get the &lt;strong&gt;Gaussian lens formula&lt;/strong&gt;:&lt;/p&gt;

\[\frac{1}{s_o} + \frac{1}{s_i} = \frac{1}{f}\]

&lt;h2 id=&quot;paraxial-approximation&quot;&gt;Paraxial Approximation&lt;/h2&gt;

&lt;p&gt;Note: I made some changes to the material on the slides.&lt;sup id=&quot;fnref:change&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:change&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; Levoy justifiably marks this section as optional.&lt;/p&gt;

&lt;p&gt;Here’s a longer derivation of the Gaussian lens formula from more fundamental premises. Again we use a convex lens but it works for concave too.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/paraxial.png&quot; alt=&quot;paraxial.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Above we show a light ray shining from an object point \(O\), hitting a spherical surface with radius \(r_1\) (not labeled above), and landing on the image point \(P\). We use the &lt;strong&gt;paraxial approximation&lt;/strong&gt;, which assumes \(e\approx 0\), or that the incident ray on the lens is nearly horizontal. As a result, we get several simplifications via trigonometry:&lt;/p&gt;

\[\begin{align*}
\sin u &amp;amp;= h/l \approx u \text{ (for } u \text{ in radians)}\\ 
\cos u &amp;amp;= z/l \approx 1\\
\tan u &amp;amp;\approx \sin u \approx u
\end{align*}\]

&lt;p&gt;This is called &lt;strong&gt;first-order optics&lt;/strong&gt;, because we can get the same approximations from the first-degree Taylor polynomial of \(\sin u\) and \(\cos u\) centered at zero. Just to illustrate, the Taylor series expansions around zero are:&lt;/p&gt;

\[\begin{align*}
\sin\phi = \phi - \frac{\phi^3}{3!} + \frac{\phi^5}{5!} - \frac{\phi^7}{7!} + \ldots\\
\cos\phi = 1 - \frac{\phi^2}{2!} + \frac{\phi^4}{4!} - \frac{\phi^6}{6!} + \ldots
\end{align*}\]

&lt;p&gt;And we just use the first term to approximate each function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/paraxial1.png&quot; alt=&quot;paraxial1.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Recall Snell’s Law states \(n\sin \theta = n'\sin \theta'\). The paraxial approximation is:&lt;/p&gt;

\[n\theta \approx n'\theta'\]

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/paraxial2.png&quot; alt=&quot;paraxial2.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We want an equation relating \(z_1'\) with \(z_1\). What follows is a bunch of elementary math. By geometry:&lt;/p&gt;

\[\begin{align*}
\theta&amp;amp;=u+a\\
\theta'&amp;amp;=a-u'\\
\end{align*}\]

&lt;p&gt;We use that to expand our Snell’s Law approximation:&lt;/p&gt;

\[n(u+a) \approx n'(a-u')\]

&lt;p&gt;Then from trigonometry we can say:&lt;/p&gt;

\[\begin{align*}
a \approx \sin(a) &amp;amp;= \frac{h}{r_1}\\[1em]
u \approx \tan(u) &amp;amp;= \frac{h}{z_1}\\[1em]
u' \approx \tan(u') &amp;amp;= \frac{h}{z_1'}\\
\end{align*}\]

&lt;p&gt;We use those to manipulate our Snell’s Law approximation some more:&lt;/p&gt;

\[\begin{align*}
n\left(\frac{h}{z_1}+\frac{h}{r_1}\right) &amp;amp;\approx n'\left(\frac{h}{r_1}-\frac{h}{z_1'}\right)\\
\Rightarrow \frac{n}{z_1}+\frac{n'}{z_1'} &amp;amp;\approx \frac{n'-n}{r_1}\\
\end{align*}\]

&lt;p&gt;Notice how \(h\) has canceled out. No matter where a ray originating from \(O\) lands on the lens (no matter what height \(h\)), it will always hit \(P\). We have shown that there isn’t spherical aberration under the assumptions we made.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/paraxiala.png&quot; alt=&quot;paraxiala.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We calculated light going from air to glass, but we also need to do glass back to air. Let the right side of the lens be a spherical surface of radius \(r_2\) (not labeled). After refracting once on the left side, the ray will hit refract on the right side, and finally land on \(Q\).&lt;/p&gt;

&lt;p&gt;We basically redo the calculations with new object and image points. The new image point is \(Q\), but what is the new object point? Well, the incident ray for the right side is just the transmitted ray from the left side, which intersects the optical axis at \(P\). So the image from the left interface becomes a &lt;em&gt;virtual&lt;/em&gt; object at the right interface. For this reason, we let the distance from the right side of the lens to \(P\) be negative. The resulting equation is:&lt;/p&gt;

\[-\frac{n'}{z_2}+\frac{n}{z_2'} \approx \frac{n-n'}{r_2}\]

&lt;p&gt;Summing the equations for the left and right sides, we get&lt;/p&gt;

\[\begin{align*}
\frac{n}{z_1}+\frac{n}{z_2'} &amp;amp;\approx (n'-n)\left(\frac{1}{r_1}-\frac{1}{r_2} \right) + \frac{n'}{z_2}-\frac{n'}{z_1'}\\
\Rightarrow \frac{n}{z_1}+\frac{n}{z_2'} &amp;amp;\approx (n'-n)\left(\frac{1}{r_1}-\frac{1}{r_2} \right) + \frac{n'd}{(z_1'-d)z_1'}
\end{align*}\]

&lt;p&gt;Where the second line uses \(\vert -z_2\vert = \vert z_1'\vert - d\) to simplify the right terms.&lt;/p&gt;

&lt;p&gt;Let’s use some nicer variables to represent the distances of the object and image to the left and right side of the lens, respectively: \(s_o = z_1\) and \(s_i = z_2'\). As \(d\rightarrow 0\), called the &lt;strong&gt;thin lens approximation&lt;/strong&gt;, the right term approaches zero, and we obtain the &lt;strong&gt;lens maker’s formula&lt;/strong&gt;:&lt;/p&gt;

\[\frac{1}{s_o} + \frac{1}{s_i} = (n' - 1)\left(\frac{1}{r_1} - \frac{1}{r_2}\right) \tag{3}\]

&lt;p&gt;Note: we set \(n=1\), the refractive index of air.&lt;/p&gt;

&lt;p&gt;If we move the object \(s_0\) to infinity, the image distance \(s_i\) approaches the focal length \(f\), so we can say:&lt;/p&gt;

\[\frac{1}{f} = (n' - 1)\left(\frac{1}{r_1} - \frac{1}{r_2}\right) \tag{4}\]

&lt;p&gt;Equating with the previous equation, we obtain the Gaussian lens formula:&lt;/p&gt;

\[\frac{1}{s_o} + \frac{1}{s_i} = \frac{1}{f} \tag{5}\]

&lt;p&gt;An interactive applet can be found &lt;a href=&quot;https://sites.google.com/site/marclevoylectures/applets/gaussian-lens-formula&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;uses-for-the-gaussian-lens-formula&quot;&gt;Uses For The Gaussian Lens Formula&lt;/h1&gt;

&lt;p&gt;The Gaussian lens formula serves the basis for the rest of this note, and much more. Here’s a few important uses.&lt;/p&gt;

&lt;h2 id=&quot;changing-object-distance-revisited&quot;&gt;Changing Object Distance (Revisited)&lt;/h2&gt;

&lt;p&gt;What can the Gaussian lens formula do for us?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/changefocus.png&quot; alt=&quot;changefocus.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It basically dictates what happens in the image above. Again, it models the interactions of object distance, image distance, and focal length, &lt;em&gt;when the object is in focus&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;If the sensor is at the focal point, then \(s_i=f\), and the object is focused at infinity (top drawing in the picture above).&lt;/p&gt;

&lt;p&gt;We can also have \(s_o=s_i=2f\) (third drawing):&lt;/p&gt;

\[\frac{1}{2f} + \frac{1}{2f} = \frac{1}{f}\]

&lt;p&gt;giving us 1:1 imaging, which means the scene and image scales are the same. For example, a 36 mm wide object will fill a 36 mm wide sensor.&lt;sup id=&quot;fnref:size&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:size&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Similar to the first drawing, if \(s_o=f\) then \(s_i=\infty\) (not pictured), meaning we physically can’t focus on objects closer to the lens than \(f\).&lt;/p&gt;

&lt;h2 id=&quot;virtual-images-and-objects&quot;&gt;Virtual Images and Objects&lt;/h2&gt;

&lt;p&gt;What if the object distance is less than the focal length? Say \(s_o=\frac{f}{2}\). Then:&lt;/p&gt;

\[\frac{2}{f} + \frac{1}{s_i} = \frac{1}{f}\]

&lt;p&gt;so \(s_i=-f\).&lt;/p&gt;

&lt;p&gt;In general:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;if \(0&amp;lt;s_o&amp;lt;f\), then \(s_i&amp;lt;0\)&lt;/li&gt;
  &lt;li&gt;if \(0&amp;lt;s_i&amp;lt;f\), then \(s_o&amp;lt;0\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By convention, real objects lie to the left of the lens while real images lie to the right, and both have positive distances. Negative distances, then, correspond with &lt;em&gt;virtual&lt;/em&gt; objects and images.&lt;/p&gt;

&lt;h2 id=&quot;concave-lenses&quot;&gt;Concave Lenses&lt;/h2&gt;

&lt;p&gt;So far we assumed we were working with a convex lens. By convention, convex lenses have positive focal length, while concave lenses have negative focal length. Concave lenses actually behave much nicer, creating a virtual image at any object distance. For example, let \(s_o=2\) and \(f=-1\).&lt;/p&gt;

\[\frac{1}{2} + \frac{1}{s_i} = -1\]

&lt;p&gt;Then \(s_i=-\frac{2}{3}\).&lt;/p&gt;

&lt;p&gt;From the Gaussian lens formula, we can see that if \(s_o&amp;gt;0\) (as it must be for a real object) and \(f&amp;lt;0\), then it must be the case that \(s_i&amp;lt;0\) and \(\vert s_i\vert&amp;lt;s_o\).&lt;/p&gt;

&lt;h2 id=&quot;power-of-a-lens&quot;&gt;Power of a Lens&lt;/h2&gt;

&lt;p&gt;How can we compare the strengths of different lenses? Let’s define the &lt;strong&gt;power&lt;/strong&gt; \(P\) of a lens as:&lt;/p&gt;

\[P=\frac{1}{f}\]

&lt;p&gt;where the units are 1/meters, or &lt;strong&gt;diopters&lt;/strong&gt;. Due to the convention from the previous section, convex lenses have positive diopters while concave lenses have negative diopters.&lt;/p&gt;

&lt;p&gt;If we combine two lenses, we can just add their diopters. The two equations are equivalent:&lt;/p&gt;

\[\begin{align*}
    \frac{1}{f_{tot}} &amp;amp;= \frac{1}{f_1} + \frac{1}{f_2}\\[1em]
    P_{tot} &amp;amp;= P_1 + P_2
\end{align*}\]

&lt;h2 id=&quot;magnification&quot;&gt;Magnification&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/gaussray1.png&quot; alt=&quot;gaussray1.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By similar triangles, magnification can be determined from the image and object distances:&lt;/p&gt;

\[M \triangleq \frac{y_i}{y_o} = -\frac{s_i}{s_o} = -\frac{f}{s_o-f}\]

&lt;p&gt;The triangle symbolizes equality by definition. The negative sign is just due to differing conventions; the heights have opposite signs depending whether they’re above or below the optical axis, unlike the object and image distances if neither is virtual.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Clarification&lt;/strong&gt;: Most people, including Levoy later in the slides, seem to ignore the sign on magnification, effectively taking \(\vert M \vert\). From now on, I’m dropping the sign too.&lt;/p&gt;

&lt;h3 id=&quot;hardware-for-magnification&quot;&gt;Hardware For Magnification&lt;/h3&gt;

&lt;p&gt;Gathered from &lt;a href=&quot;https://www.cambridgeincolour.com/tutorials/macro-extension-tubes-closeup.htm&quot;&gt;this site&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Extension tubes&lt;/strong&gt;: devices fitted between the camera body and the lens, which increase magnification by increasing effective image distance.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Measured in mm.&lt;/li&gt;
  &lt;li&gt;They have no optics, but allow electronics to pass through. They can be stacked.&lt;/li&gt;
  &lt;li&gt;They raise the effective f-number, reducing the amount of light.&lt;/li&gt;
  &lt;li&gt;They’re better for wide-angle lenses.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Close-up filters/lenses&lt;/strong&gt;: a special lens screwed onto the front of the actual lens, increasing magnfication by &lt;em&gt;decreasing&lt;/em&gt; effective focal length.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Measured in diopters.&lt;/li&gt;
  &lt;li&gt;Optical elements may reduce image quality.&lt;/li&gt;
  &lt;li&gt;They’re better for telephoto lenses.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Teleconverters&lt;/strong&gt;: like extension tubes, but with optics. They increase magnification by &lt;em&gt;increasing&lt;/em&gt; effective focal length.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Measured by magnification multiplier (e.g. 1.4x)&lt;/li&gt;
  &lt;li&gt;Optical elements may reduce image quality.&lt;/li&gt;
  &lt;li&gt;They raise the effective f-number, reducing the amount of light.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Macro Lenses&lt;/strong&gt;: the best and most expensive option for close-up photography.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Measured in \(M\).&lt;/li&gt;
  &lt;li&gt;Corrects for aberrations.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Clarification&lt;/strong&gt;: Alright, you have to be wondering: how can close-up filters and teleconverters achieve the same thing in different ways? I’m not 100\% certain but here’s my thinking. Teleconverters work like telephoto lens elements, where a combination of lens elements can achieve the same focal length and object distance at a reduced image distance; see the Telephoto section below. Since the teleconverter is screwed in between the existing lens and the camera, the image distance doesn’t decrease, leading to an increase in the effective focal length.&lt;/p&gt;

&lt;p&gt;Close-up filters allow you to (drum roll) get close-up to the object, i.e. reduce object distance, by &lt;em&gt;lowering&lt;/em&gt; focal length; see the example at the end of this note called Close-Up Filters. Based on the last expression for magnification, we can see that focal length and object distance aren’t related linearly, and I presume in close-up situations, the object distance has a stronger effect on the magnification than the focal length.&lt;/p&gt;

&lt;p&gt;In other words:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;close-up filter: \(f\downarrow\quad s_o\Downarrow\quad s_i – \quad M\uparrow\)&lt;/li&gt;
  &lt;li&gt;teleconverter: \(f\uparrow\quad s_o – \quad s_i – \quad M\uparrow\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;3d-perspective-transform&quot;&gt;3D Perspective Transform&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/frustrum.png&quot; alt=&quot;frustrum.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A lens transforms a 3D object into a 3D image, and the sensor extracts a 2D slice of that image. We can think of this as an extension of the transform going directly from 3D to 2D from [[Image Formation]].&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Clarification&lt;/strong&gt;: How is that different from a pinhole camera? Recall that a pinhole camera performs a projective transformation. We previously showed one object and its image on one sensor.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/frustrum2d.png&quot; alt=&quot;frustrum2d.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here’s two objects and two sensors. A pinhole camera has infinite depth of field, so any placement of the sensor will capture both objects in focus (and in fact, the entirety of the visible scene).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/frustrum2d_lens.png&quot; alt=&quot;frustrum2d_lens.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Replacing the pinhole with a lens, the sensor at any position will still capture both objects. The difference is that only one plane in the object space will be in focus for any given plane in image space (see the blurred arrows). For any square in object space, we get a trapezoid of the &lt;em&gt;corresponding focused objects&lt;/em&gt; in image space, a bijective transformation. Moving from the 2D visualization to 3D, the square and trapezoid become a cube and pyramidal frustrum, as shown in Levoy’s drawing. The latter converges to the focal point, &lt;em&gt;not&lt;/em&gt; the optical center of the lens, and no such transformation exists for the pinhole camera.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/perspectivetransform.png&quot; alt=&quot;perspectivetransform.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The other point Levoy makes is that a linear change in object distance is accompanied by a &lt;em&gt;nonlinear&lt;/em&gt; change in image distance, evident from Gauss’ lens formula, and illustrated by the grid lines in the image above. Note that the warped grid in image space &lt;em&gt;doesn’t&lt;/em&gt; represent a 3D view of a plane sticking out of the page, but a trapezoid on the same plane as everything else in the diagram. The image comes from an &lt;a href=&quot;https://sites.google.com/site/marclevoylectures/applets/operation-of-a-thin-lens&quot;&gt;interactive applet&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;depth-of-field&quot;&gt;Depth of Field&lt;/h1&gt;

&lt;h2 id=&quot;derivation&quot;&gt;Derivation&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/dofformula1.png&quot; alt=&quot;dofformula1.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In [[Image Formation]] we gave a simplified presentation of the circle of confusion using symmetric cones. Now we use a more accurate diagram. The vertical line to the right of the lens signifies the &lt;strong&gt;circle of confusion&lt;/strong&gt;, the in-focus image plane is the vertical line to the right of the lens, the area of acceptable focus is shaded red, and the width of this area is called the &lt;strong&gt;depth of focus&lt;/strong&gt;. We define the &lt;strong&gt;conjugate of the circle of confusion&lt;/strong&gt; and the &lt;strong&gt;depth of field&lt;/strong&gt; similarly, to the left of the lens. It’s clear that the depth of field is asymmetrical around the in-focus object plane. In addition, the circle of confusion is usually smaller than its conjugate, which we can see from its formula; since usually \(0&amp;lt;M&amp;lt;1\), then usually \(\frac{C}{M}&amp;gt;C\).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/dofformula2.png&quot; alt=&quot;dofformula2.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let \(U=s_o\), just for convention’s sake. We assume the object distance (focal distance) is much larger than the focal length, or \(U\gg f\), or equivalently \(f\approx s_i\).&lt;/p&gt;

\[\begin{align*}
    M = \frac{s_i}{U} &amp;amp;\approx \frac{f}{U}\\
    \frac{C}{M} &amp;amp;\approx \frac{CU}{f}
\end{align*}\]

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/dofformula3.png&quot; alt=&quot;dofformula3.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Recall the aperture diameter is \(\frac{f}{N}\). Let \(D_1\) be the &lt;strong&gt;front depth of field&lt;/strong&gt; and \(D_2\) be the &lt;strong&gt;back depth of field&lt;/strong&gt;. By similar triangles we get:&lt;/p&gt;

\[\frac{D_1}{CU/f} = \frac{U-D_1}{f/N}\]

&lt;p&gt;which gives an expression for the front depth of field:&lt;/p&gt;

\[D_1 = \frac{NCU^2}{f^2+NCU}\]

&lt;p&gt;Similarly, the back depth of field is:&lt;/p&gt;

\[D_2 = \frac{NCU^2}{f^2-NCU}\]

&lt;p&gt;We sum those to get the total depth of field:&lt;/p&gt;

\[D = D_1 + D_2 = \frac{2NCU^2f^2}{f^4-N^2C^2U^2} \tag{1}\]

&lt;p&gt;The \(N^2C^2U^2\) term can be ignored when the conjugate circle of confusion is small relative to the aperture,&lt;sup id=&quot;fnref:conj&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:conj&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; giving the final formula for depth of field:&lt;/p&gt;

\[D = \frac{2NCU^2}{f^2} \tag{2}\]

&lt;p&gt;While it may not be the most accurate formula, it does illustrate the relationships between its component variables for reasonable object distances. An interactive depth of field applet can be found &lt;a href=&quot;https://graphics.stanford.edu/courses/cs178-14/applets/dof.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;hyperfocal-distance&quot;&gt;Hyperfocal distance&lt;/h2&gt;

&lt;p&gt;The back depth of field becomes infinite if:&lt;/p&gt;

\[U = \frac{f^2}{NC} \triangleq H\]

&lt;p&gt;In that case, the front depth of field becomes:&lt;/p&gt;

\[D_1 = \frac{NCU^2}{f^2-NCU} = \frac{H}{2}\]

&lt;p&gt;That is, if the object distance is set to the &lt;strong&gt;hyperfocal distance&lt;/strong&gt; \(H\) (equivalently, if you focus at \(H\)), everything from distance \(\frac{H}{2}\) to infinity will be in focus. It’s a good number to know on its own, especially for certain use cases like landscape photography.&lt;/p&gt;

&lt;p&gt;We can also express the depth of field in terms of \(H\). Without disregarding the term \(U^2N^2C^2\) in the denominator of eq. 1, we get:&lt;/p&gt;

\[D = \frac{2HU^2}{H^2-U^2}\]

&lt;p&gt;Now, another perspective on what it means to ignore the term \(U^2N^2C^2\), is that the hyperfocal distance is much larger than the object distance, or \(U^2 \ll H^2\). If so, then the formula reduces to eq. 2, or equivalently:&lt;/p&gt;

\[D = \frac{2U^2}{H}\]

&lt;p&gt;So the simplified depth of field formula, eq 2, relies on the object distance \(U\) falling within a “nice” range, roughly specified by:&lt;/p&gt;

\[f\ll U \ll H\]

&lt;h2 id=&quot;macro-depth-of-field&quot;&gt;Macro Depth of Field&lt;/h2&gt;

&lt;p&gt;Our derivation for the depth of field doesn’t quite work for macro lenses, because of the assumption that \(f=s_i\). Discarding it, the accurate formula for small object distances can be stated, in terms of \(M\):&lt;/p&gt;

\[D_{\text{macro}}=\frac{2NC(M+1)}{M^2} \tag{3}\]

&lt;h2 id=&quot;other-derivations&quot;&gt;Other Derivations&lt;/h2&gt;

&lt;p&gt;The derivations above use the \(f=s_i\) approximation pretty early, which makes the calculations easier but gives less general formulas. In particular, the general versions would better convey where the macro version comes from. Of course there’s even more complicated versions dealing with things like thick lenses. &lt;a href=&quot;https://www.largeformatphotography.info/articles/DoFinDepth.pdf&quot;&gt;Here&lt;/a&gt; and &lt;a href=&quot;https://www.imajtrek.com/new_page_10.htm&quot;&gt;here&lt;/a&gt; are some more complete derivations.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt; &lt;/th&gt;
      &lt;th&gt;N,C,U,f&lt;/th&gt;
      &lt;th&gt;H&lt;/th&gt;
      &lt;th&gt;M&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;any \(U\)&lt;/td&gt;
      &lt;td&gt;\(\frac{2NCUf^2(U-f)}{f^4-N^2C^2(U-f)^2}\)&lt;/td&gt;
      &lt;td&gt;\(\frac{2HU(U-f)}{H^2-(U-f)^2}\)&lt;/td&gt;
      &lt;td&gt;\(\frac{2NC(M+1)}{M^2-\left(\frac{NC}{f}\right)^2}\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\(f\ll U\)&lt;/td&gt;
      &lt;td&gt;\(\frac{2NCU^2f^2}{f^4-N^2C^2U^2}\)&lt;/td&gt;
      &lt;td&gt;\(\frac{2HU^2}{H^2-U^2}\)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\(U\ll H\)&lt;/td&gt;
      &lt;td&gt;\(\frac{2NC(U-f)}{f^2}\)&lt;/td&gt;
      &lt;td&gt;\(\frac{2U(U-H)}{H}\)&lt;/td&gt;
      &lt;td&gt;\(\frac{2NC(M+1)}{M^2}\)&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;\(f\ll U\ll H\)&lt;/td&gt;
      &lt;td&gt;\(\frac{2NCU^2}{f^2}\)&lt;/td&gt;
      &lt;td&gt;\(\frac{2U^2}{H}\)&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The table shows several variations of the formula, depending on the assumptions made on the object distance \(U\), and the particular terms used.&lt;sup id=&quot;fnref:yourself&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:yourself&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Finally, any method relies on the Gaussian Lens formula, and therefore the paraxial approximation.&lt;/p&gt;

&lt;h1 id=&quot;depth-of-field-in-practice&quot;&gt;Depth of Field In Practice&lt;/h1&gt;

&lt;p&gt;For intuition’s sake, depth of field is simplified by necessity. Most of the conclusions drawn below use the simpler versions of the equations, and therefore apply to midrange distances. Some &lt;a href=&quot;https://theonlinephotographer.typepad.com/the_online_photographer/2009/06/depth-of-field-hellthe-sequel.html&quot;&gt;people&lt;/a&gt; will bemoan these approximations, but as long as we’re aware of their limitations, it shouldn’t be a problem.&lt;/p&gt;

&lt;h2 id=&quot;basic-relationships&quot;&gt;Basic Relationships&lt;/h2&gt;

&lt;p&gt;We use eq. 2 for the observations below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/doffnumber.png&quot; alt=&quot;doffnumber.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Depth of field is linear with f-number.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/dofsubjectdistance.png&quot; alt=&quot;dofsubjectdistance.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Depth of field is quadratic with object distance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/doffocallength.png&quot; alt=&quot;doffocallength.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Depth of field is inverse-quadratic with focal length.&lt;/p&gt;

&lt;h2 id=&quot;dolly-zoom&quot;&gt;Dolly Zoom&lt;/h2&gt;

&lt;p&gt;Recall in a dolly zoom, the magnification of the subject stays constant by increasing both object distance and focal length in a principled manner.&lt;/p&gt;

&lt;p&gt;By eq. 3, when magnification is held constant, the depth of field depends on neither focal length nor object distance. Due to perspective distortion, however, the blurriness of the background will change, since the FOV has changed and the blurred elements take up a different amount of screen space.&lt;/p&gt;

&lt;h2 id=&quot;sensor-size&quot;&gt;Sensor Size&lt;/h2&gt;

&lt;p&gt;As the sensor size goes down:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the focal length tends to go down to maintain FOV&lt;/li&gt;
  &lt;li&gt;the pixel size tends to go down to maintain resolution, which makes the circle of confusion smaller&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Focal length has a greater effect (squared, compared to linear, from eq. 2) than circle of confusion, so generally speaking, as sensor size \(\times 2\), depth of field is \(\times \frac{1}{2}\).&lt;/p&gt;

&lt;h2 id=&quot;circle-of-confusion&quot;&gt;Circle of Confusion&lt;/h2&gt;

&lt;p&gt;The circle of confusion depends on many things, including: the sensing medium, reproduction medium, viewing distance, human vision, etc. Some typical cases:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;35 mm film to print: .02 mm (on negative)&lt;/li&gt;
  &lt;li&gt;high-end SLR: 6 μm (1 pixel)&lt;/li&gt;
  &lt;li&gt;can go smaller when downsizing for the web or for a poor lens&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The lectures show example depth of field calculations for different photographs taken by Levoy, including viewing considerations. They’re too large to reproduce, and it would be pointless to show downsized ones, so I urge you to check out the lecture slides.&lt;/p&gt;

&lt;h2 id=&quot;bokeh&quot;&gt;Bokeh&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/bokeh.jpg&quot; alt=&quot;bokeh.jpg&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bokeh&lt;/strong&gt; is the appearance of small out-of-focus features in a photograph with shallow depth of field. The shape of the features are determined by the boundary of the aperture. In the picture above, the slightly closed blades on the aperture produce octagonal lights. Not every image has noticeable bokeh; it is most pronounced with point light sources.&lt;/p&gt;

&lt;p&gt;It’s pronounced &lt;em&gt;bow-cuh&lt;/em&gt;!&lt;/p&gt;

&lt;h2 id=&quot;seeing-through-occlusions&quot;&gt;Seeing Through Occlusions&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/dofocclusion.png&quot; alt=&quot;dofocclusion.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The depth of field blur is not the same as a convolution. Using a shallow enough depth of field, we can eliminate occlusions, which would not be possible by blurring a static image featuring the occlusion. Apparently a chain link fence sat between the camera and the owl in the picture above, but it is not visible. Some of the rays from the owl got blocked, but some made it around the fence wire. A given pixel, then, may have a mixture of colors from the owl and the fence, and the result on the image is not an outright occlusion, but decreased contrast.&lt;/p&gt;

&lt;h1 id=&quot;lens-aberrations&quot;&gt;Lens Aberrations&lt;/h1&gt;

&lt;p&gt;Let’s recap the assumptions we made so far:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Geometric instead of physical optics&lt;/li&gt;
  &lt;li&gt;Spherical lenses&lt;/li&gt;
  &lt;li&gt;Paraxial approximation of ray angles&lt;/li&gt;
  &lt;li&gt;Thin lenses instead of thick, or even compound lenses&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There’s seven main types of lens aberrations. We can divide lens aberrations into:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;chromatic aberrations (CA)&lt;/strong&gt;: two aberrations rooted in the varying wavelengths of light&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;monochromatic aberrations&lt;/strong&gt;: five Seidel aberrations for monochromatic light&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Another way to&lt;/p&gt;

&lt;h2 id=&quot;chromatic-aberrations&quot;&gt;Chromatic Aberrations&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/prism.png&quot; alt=&quot;prism.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The index of refraction varies by wavelength! That’s how an optical prism works after all. The &lt;strong&gt;dispersion&lt;/strong&gt; is the variation in refractive index by wavelength. It’s modeled by some function, for instance the &lt;strong&gt;Sellmeier equation&lt;/strong&gt; (right subimage above), outputting a single refractive index for a given wavelength.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Higher dispersion means greater variation&lt;/li&gt;
  &lt;li&gt;Amount of variation depends on the material&lt;/li&gt;
  &lt;li&gt;Index is typically higher for blue than red, meaning blue light bends more&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There’s two kinds of CA:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;longitudinal&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;cause&lt;/em&gt;: lens has different effective focal lengths for different wavelengths of light&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;effect&lt;/em&gt;: different colors on the image will be out of focus&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;lateral&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;em&gt;cause&lt;/em&gt;: different colors have different magnifications&lt;/li&gt;
      &lt;li&gt;&lt;em&gt;effect&lt;/em&gt;: colors on the image will be magnified (with respect to the optical axis) different amounts, producing “color fringing” towards the edges&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/chromaticaberration.png&quot; alt=&quot;chromaticaberration.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It’s possible to correct longitudinal CA with an additional lens element. The right subimage shows an &lt;strong&gt;achromatic doublet&lt;/strong&gt;, composed of a normal convex element and a complementary concave element with different dispersions (crown and flint refer to the different glass types). By adjusting the dispersions, we can correct at two wavelengths. In the right subimage, the doublet is designed so red and blue light are aligned. Though green wavelengths are imperfectly corrected, the overall dispersion is greatly reduced. Longitudinal CA can be also be corrected by lowering the aperture, so the light hits the lens closer to the normal (I think), limiting dispersion.&lt;/p&gt;

&lt;p&gt;Lateral CA can be somewhat corrected either with a lens construction symmetrical about the aperture, or with software.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/chromaticab.gif&quot; alt=&quot;chromaticab.gif&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The two kinds of chromatic aberration are shown above side-by-side. Notice how longitudinal CA takes the form of out-of-focus elements spread uniformly across the whole image. In lateral CA we can clearly see the radial magnification of different colors, with a stronger effect at the edges.&lt;/p&gt;

&lt;h2 id=&quot;spherical-aberrations&quot;&gt;Spherical Aberrations&lt;/h2&gt;

&lt;p&gt;Remember the goal of a lens is to focus light rays at a point.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/lenstypes.png&quot; alt=&quot;lenstypes.png&quot; width=&quot;250&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The lens shape which focuses incoming rays to a single point is actually a hyperboloid. In contrast, a spherical lens exhibits what’s called &lt;strong&gt;spherical aberration&lt;/strong&gt;, which can be corrected by stopping down the aperture.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/softfocus.jpg&quot; alt=&quot;softfocus.jpg&quot; width=&quot;400&quot; /&gt;
&lt;em&gt;Left: soft focus on; right: soft focus off&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Spherical lenses are more common simply because they’re easier to make. It’s still possible to buy a “soft focus” lens, which intentionally does not correct spherical aberration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/expkernel.png&quot; alt=&quot;expkernel.png&quot; width=&quot;150&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Fun fact: the soft focus effect is equivalent to a post-processing convolution with the exponential kernel.&lt;/p&gt;

&lt;h2 id=&quot;coma&quot;&gt;Coma&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/coma.png&quot; alt=&quot;coma.png&quot; width=&quot;200&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Coma&lt;/strong&gt; is when the magnification varies based on distance from the optical axis, similar to lateral CA. Unlike lateral CA, it can be reduced by stopping down the aperture.&lt;/p&gt;

&lt;h2 id=&quot;astigmatism&quot;&gt;Astigmatism&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/astigatism.png&quot; alt=&quot;astigatism.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Astigmatism&lt;/strong&gt; is when rays in the transverse plane (x-z plane) focus at a different distance from the rays in the sagittal plane (y-z plane). There’s two kinds:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;opthalmic&lt;/strong&gt;: due to radially asymmetric (oblong) lens&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;optical&lt;/strong&gt;: even for radially symmetric lens; the third-order aberration&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It can be reduced by stopping down the aperture.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/astigmatismopthalmic.png&quot; alt=&quot;astigmatismopthalmic.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The effect is shown above. Note that vertical or horizontal focus should be different in strength, or else it’s just a normal, out-of-focus lens.&lt;/p&gt;

&lt;h2 id=&quot;field-curvature&quot;&gt;Field Curvature&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Field curvature&lt;/strong&gt; refers to the fact that spherical objects focus a curved surface in object space onto a curved surface in image space. If the object surface is flat, the corresponding image surface must be even more curved. Only some ring on a flat imaging plane will be in focus. It can be fixed by closing down the aperture.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/fieldcurvature.png&quot; alt=&quot;fieldcurvature.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The top subimage shows the required object and image surfaces for a focused image using a spherical lens. The bottom subimages show different ranges of radii in focus.&lt;/p&gt;

&lt;h2 id=&quot;distortion&quot;&gt;Distortion&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/distortion.jpg&quot; alt=&quot;distortion.jpg&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There’s three kinds of distortion:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;pincusion&lt;/strong&gt;: magnification increases further from the optical axis&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;barrel&lt;/strong&gt;: magnification decreases further from the optical axis&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;mustache&lt;/strong&gt;: mix of the two above&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is easily correctable with software.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Clarification&lt;/strong&gt;: From the image above, it might look like the magnification doesn’t change along the horizontal and vertical axes, but it does, and would be visible with a larger grid.&lt;/p&gt;

&lt;h2 id=&quot;not-perspective&quot;&gt;NOT Perspective&lt;/h2&gt;

&lt;p&gt;Notice how perspective isn’t in the list. It’s &lt;em&gt;not&lt;/em&gt; a lens distortion or aberration! Perspective can only produce “distortion” in a subjective sense, taking into account an object’s typical viewing distance.&lt;/p&gt;

&lt;h1 id=&quot;other-lens-artifacts&quot;&gt;Other Lens Artifacts&lt;/h1&gt;

&lt;p&gt;There are several other effects besides aberrations, that may or may not be desired.&lt;/p&gt;

&lt;h2 id=&quot;veiling-glare&quot;&gt;Veiling Glare&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/veilingglare1.jpg&quot; alt=&quot;veilingglare1.jpg&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Veiling glare&lt;/strong&gt; is a glare lowering contrast throughout the image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/veilingglare.png&quot; alt=&quot;veilingglare.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;At every interface, both refraction and reflection occur. The image above shows how a few reflections can cause a ray from one source to produce two outgoing rays. Usually more light refracts than it reflects, but the light coming from the secondary rays is enough to lower the contrast of the image. It can be reduced by anti-reflection coatings to block unwanted reflections, and lens hoods to block excessive incoming light.&lt;/p&gt;

&lt;h2 id=&quot;lens-flare&quot;&gt;Lens Flare&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/lensflare.jpg&quot; alt=&quot;lensflare.jpg&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lens flares&lt;/strong&gt; (or &lt;strong&gt;ghosting&lt;/strong&gt;) are structured artifacts which move predictably around the image based on the direction of the camera (it’s easier to see what it is than to try to explain it). The image above includes a special case where the light source looks star-shaped, called a &lt;strong&gt;starburst&lt;/strong&gt;. Lens flares can be reduced with a lens hood, or by moving the light source out of the frame.&lt;/p&gt;

&lt;h2 id=&quot;vignetting&quot;&gt;Vignetting&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/vignetting.png&quot; alt=&quot;vignetting.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Vignetting&lt;/strong&gt; is the darkening of an image around the edges. From another perspective, it is the soft boundary of the image circle coming into view.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/vignetting1.png&quot; alt=&quot;vignetting1.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The first kind is &lt;strong&gt;optical vignetting&lt;/strong&gt;, due to the distance traveled from the lens to the sensor. In the diagram above, we see several things happening, all based on the angle \(\theta\) the pixel makes lies above the horizontal:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Irradiance is proportional to the projected area of aperture as seen by the sensor. In other words, at point \(H\), the sensor sees the aperture as an ellipse with less area compared to the circle that point \(A\) would see. This area decreases as \(\cos\theta\).&lt;sup id=&quot;fnref:projellipse&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:projellipse&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;Irradiance is proportional to the projected area of the pixel as seen by the aperture. Basically, the previous effect works backwards, with the aperture seeing an stretched out pixel at \(H\) compared to \(A\), with area also decreasing as \(\cos\theta\).&lt;/li&gt;
  &lt;li&gt;Irradiance is proportional to squared distance from aperture to pixel, and distance rises as \(1/\cos\theta\).&lt;/li&gt;
  &lt;li&gt;Combining all three, light drops as \(\cos^4\theta\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/vignetting2.png&quot; alt=&quot;vignetting2.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There’s also &lt;strong&gt;optical vignetting&lt;/strong&gt;, where the light coming from wide angles is blocked by the barrel of the lens, especially noticeable for wide apertures, as seen in the bottom-left lens from the left subimage above. The main result is once again a decrease in light on the edges. Since the effective shape of the aperture changes, so does the bokeh (if present) towards the edges of an image, known as the &lt;strong&gt;cat’s eye effect&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;There’s &lt;strong&gt;mechanical vignetting&lt;/strong&gt;, caused by incorrect add-ons for the lens, or a lens whose image circle is too small for the sensor. In both cases, the image shows a relatively sharp cutoff to black in the shape of whatever obstacles lie in front of the lens.&lt;/p&gt;

&lt;p&gt;Finally, there’s &lt;strong&gt;pixel vignetting&lt;/strong&gt; due to sensor construction. For light coming in at an angle, the walls of the pixel cast shadows, reducing light captured at the edges of the sensor.&lt;/p&gt;

&lt;p&gt;Unlike the previous two artifacts, vignetting is digitally correctable, although noise might increase as pixels are brightened. Optical vignetting can be lowered by decreasing the aperture, and mechanical vignetting is eliminated by using compatible camera parts.&lt;/p&gt;

&lt;h2 id=&quot;diffraction&quot;&gt;Diffraction&lt;/h2&gt;

&lt;p&gt;Here we’ll briefly dip our feet into the wave aspects of light. &lt;strong&gt;Diffraction&lt;/strong&gt; is when light spreads out as it passes through an aperture. The result is a slight blurring of the image (yet again). When diffraction causes a noticeable blur, the lens is said to have become &lt;em&gt;diffraction limited&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The amount of spreading increase as the aperture diameter &lt;em&gt;decreases&lt;/em&gt; and the distance from the sensor &lt;em&gt;increases&lt;/em&gt;, so the blur varies with \(N=\frac{f}{A}\). It also depends on pixel size, just like depth of field, and the wavelength of the light. In practice, diffraction only comes into play around at&lt;/p&gt;

&lt;p&gt;Note: Diffraction is also responsible for the image circle, and hence vignetting.&lt;/p&gt;

&lt;h1 id=&quot;lens-systems&quot;&gt;Lens Systems&lt;/h1&gt;

&lt;h2 id=&quot;telephoto&quot;&gt;Telephoto&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/telephoto.png&quot; alt=&quot;telephoto.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A &lt;strong&gt;telephoto lens&lt;/strong&gt; has a long focal length at a compressed size. Normally, a convex lens would have to be positioned at the middle line in the diagram above, at distance \(f\) from the sensor. To fit the same lens in a more compact space, the strength of the convex lens is boosted, and it’s combined with a concave lens to refocus the rays at the original point, as in figure (a). Figure (b) shows a &lt;strong&gt;reverse telephoto lens&lt;/strong&gt; using the opposite mechanism, in order to make room for a reflex mirror in an SLR, for instance.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/telephoto1.png&quot; alt=&quot;telephoto1.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here is one of Levoy’s drawings, which might be clearer. The blue lens is the original, and the green is the telephoto combination. They have the same focal length.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/telephoto2.png&quot; alt=&quot;telephoto2.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here’s a comparison.&lt;/p&gt;

&lt;h2 id=&quot;zoom&quot;&gt;Zoom&lt;/h2&gt;

&lt;p&gt;Zoom lenses have many components, but allow for variable focal length.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/zoom.png&quot; alt=&quot;zoom.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Only two elements are shown above, but the point is that the different elements have to move at varying rates to offer a linear zoom.&lt;/p&gt;

&lt;p&gt;You can experiment with zoom on an interactive applet found &lt;a href=&quot;https://graphics.stanford.edu/courses/cs178-14/applets/zoom.html&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;lens-design&quot;&gt;Lens Design&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/lenssoftware.png&quot; alt=&quot;lenssoftware.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Modern lens systems are designed with software, using optimization to craft good lens recipes. Usually they do not optimize the selection and arrangement of lens elements, instead minimizing aberrations by adjusting the surface shapes. The recipes for modern cameras are kept secret; supposedly even the patents for commercial lenses don’t reveal the exact recipe.&lt;/p&gt;

&lt;h1 id=&quot;examples&quot;&gt;Examples&lt;/h1&gt;

&lt;h2 id=&quot;eyeglasses-i&quot;&gt;Eyeglasses I&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; The professor has glasses with the prescription:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;right eye: -.75 diopters&lt;/li&gt;
  &lt;li&gt;left eye: -1.00 diopters&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What’s wrong with the professor’s eyes?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; The professor has myopia (nearsightedness).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/eyeproblems.png&quot; alt=&quot;eyeproblems.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;His eye is stronger than normal, with a focal point in front of the retina (which we know since the diagram shows parallel incoming rays). A concave lens is needed to “weaken” it, so the light hits the retina properly. Note that nearby objects will converge further back based on the Gaussian lens formula, and with the lens they may now focus &lt;em&gt;behind&lt;/em&gt; the retina. The professor will need to remove his glasses to see close things properly.&lt;/p&gt;

&lt;h2 id=&quot;macro-lenses&quot;&gt;Macro Lenses&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; How can the Casio EX-F1 at 73mm and the Canon MP-E 65mm macro, which have similar \(f\)’s, have such different focusing distances?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; Because macro lenses are built to allow long \(s_i\), which requires lower \(s_o\), given similar \(f\).&lt;/p&gt;

&lt;h2 id=&quot;close-up-filters&quot;&gt;Close-Up Filters&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; If we attach a close-up filter of power \(\frac{1}{500\text{mm}}\) to a lens with focal length \(f=200\text{mm}\) and minimum object distance \(s_o=1000\text{mm}\),&lt;sup id=&quot;fnref:wrong&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:wrong&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; how close can we focus on the object?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; By the Gaussian lens formula, in order to change the object distance \(s_o\) we have to change either the image distance \(s_i\) or the focal length \(f\). What a close-up filter allows you to do is reduce the object distance for a &lt;em&gt;fixed image distance&lt;/em&gt; by tweaking the overall focal length. Remember, for a given focal length, a camera’s minimum focus distance is based on the furthest the sensor can get from the lens.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/closeup.png&quot; alt=&quot;closeup.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;By the Gaussian lens formula, the image distance for the lens without the filter, corresponding to the minimum object distance, must be:&lt;/p&gt;

\[s_i = \frac{1}{\frac{1}{f}-\frac{1}{s_o}} = \frac{1}{\frac{1}{200\text{mm}} -\frac{1}{1000\text{mm}}} = 250\text{mm}\]

&lt;p&gt;Now using the close-up filter on the lens (set at the same focal length as before), the combined power is:&lt;/p&gt;

\[\begin{align*}
    \frac{1}{200 \text{mm}} + \frac{1}{500\text{mm}} &amp;amp;= \frac{1}{143\text{mm}}\\[1em]
    5.0 + 2.0 &amp;amp;= 7.0 \text{ diopters}
\end{align*}\]

&lt;p&gt;So at the minimum in-lens image distance, the close-up filter allows the object distance to be:&lt;/p&gt;

\[s_o = \frac{1}{\frac{1}{f}-\frac{1}{s_i}} = \frac{1}{\frac{1}{143\text{mm}} -\frac{1}{250\text{mm}}} = 334\text{mm}\]

&lt;p&gt;That’s about a \(\times 3\) improvement over 1000 mm!&lt;/p&gt;

&lt;p&gt;We can measure the magnification with and without the lens.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;with: \(\frac{250}{1000} = 1:4\)&lt;/li&gt;
  &lt;li&gt;without: \(\frac{250}{334} = 3:4\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/closeup1.png&quot; alt=&quot;closeup1.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;And there’s the side-by-side comparison. It works!&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Clarification:&lt;/strong&gt; the close-up filter reduces the focal length to 145 mm, which is still in the range of the lens (45-200 mm), so why not just reduce the focal length without the filter? For this lens, a lower focal length gives a lower image distance (and it may be true for all zoom lenses), and consequently we just wouldn’t be able to get as close to the object. Here’s the full explanation from the man himself:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;What I may have failed to make clear enough during lecture is, while the modified focal length of 143mm with the close-up filter does lie within the range already offered by the 45-200mm zoom lens, with the filter you are achieving that focal length while the lens is configured for 200mm. In that configuration, the barrel is fully extended, producing an image distance of 250mm, and (applying the Gaussian lens formula) an object distance so of 334mm. If the lens without the closeup filter were configured for this same focal length of 143mm the barrel would not be fully extended, so the image distance would be less than 250mm (I don’t know how much less - it depends on the lens mechanics), and the object distance would be longer than 334mm. Or if the lens without the closeup filter were configured for a focal length of 200mm, even though the barrel is fully extended and the image distance is 250mm, the object distance would be 1000mm. Thus, the filter allows you to combine the maximum barrel extension (hence the longest possible image distance) with an otherwise impossibly short object distance, producing higher magnification.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;eyeglasses-ii&quot;&gt;Eyeglasses II&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; The professor’s eyes are worse than he let on. His glasses actually have the prescription:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;right eye: -0.75 -1.00 axis 135&lt;/li&gt;
  &lt;li&gt;left eye: -1.00 -0.75 axis 180&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What’s wrong with the professor’s eyes?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; The professor has myopia &lt;em&gt;and&lt;/em&gt; astigmatism.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/optics/astigmatism1.png&quot; alt=&quot;astigmatism1.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In glasses, astigmatism can be fixed with an additional cylindrical lens component (in addition to the concave lens needed for myopia); cylindrical refers to the curvature of the surface, not to the shape of the entire lens. Levoy drew the picture above to help explain, but unfortunately I don’t understand it.&lt;/p&gt;

&lt;h2 id=&quot;chromatic-aberrations-for-humans&quot;&gt;Chromatic Aberrations For Humans&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Q:&lt;/strong&gt; Why don’t humans experience chromatic aberration?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; The word goes, our eye produces it, but our brain corrects it. In the lecture, someone asked a followup on the possibility that it has something to do with the reduced resolution of our retinas at the edges. Levoy thought it was an interesting point, and so do I.&lt;/p&gt;

&lt;h1 id=&quot;unresolved-questions&quot;&gt;Unresolved Questions&lt;/h1&gt;

&lt;p&gt;These are questions I had. Skip if you’re clueless ha&lt;/p&gt;

&lt;h2 id=&quot;paraxial-approximation-1&quot;&gt;Paraxial Approximation&lt;/h2&gt;

&lt;p&gt;I’m confused about whether the paraxial approximation means:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;the light ray makes a small angle with the horizontal&lt;/li&gt;
  &lt;li&gt;the light ray hits the lens at a low height, i.e. giving a low \(e\)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If it’s the first one, then I don’t understand how the paraxial approximation is used  (via the Gaussian lens formula) for the derivation of macro depth of field, since the angle could be large. If it’s the second one, then it allows the angle to be large, but that seems to destroy the trigonometric approximations.&lt;/p&gt;

&lt;h2 id=&quot;frustrum-shape&quot;&gt;Frustrum Shape&lt;/h2&gt;

&lt;p&gt;I’m pretty certain that the in-focus planes corresponding to each object plane converge to the focal point and not the optical center. I’m just not sure if Levoy was also talking about a separate frustrum converging to the optical center somehow.&lt;/p&gt;

&lt;h2 id=&quot;chromatic-aberrations-1&quot;&gt;Chromatic Aberrations&lt;/h2&gt;

&lt;p&gt;I’m confused about the mathematical basis for longitudinal and lateral CA. They are supposedly caused by variation in magnification and focal length, based on the wavelength of light. We can draw this connection by:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;relating both magnification and focal length to image and object distance by their respective definitions&lt;/li&gt;
  &lt;li&gt;relating those to refractive index with the lens maker’s formula&lt;/li&gt;
  &lt;li&gt;relating the refractive index to wavelength&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Considering object distance is fixed, what really matters is image distance. So how can the two types of CA occur separately? Why doesn’t an achromatic doublet fix both types of CA, if it corrects the image distances for different wavelengths?&lt;/p&gt;

&lt;h2 id=&quot;aberrations-in-general&quot;&gt;Aberrations In General&lt;/h2&gt;

&lt;p&gt;I have more questions about all the aberrations.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Why can’t we fix lateral CA by stopping down the aperture, while we can do so for monochromatic oblique aberrations?&lt;/li&gt;
  &lt;li&gt;How does stopping down the aperture help for any of the aberrations it can reduce?&lt;/li&gt;
  &lt;li&gt;What’s the difference between spherical aberration and field curvature?&lt;/li&gt;
  &lt;li&gt;What’s the difference between coma and pincushion distortion?&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:cropfactor&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Possibly helpful note: The index of refraction is like the crop factor, in that we’re comparing all measurements to a baseline measurement. For crop factor it was the 35 mm sensor size, and for index of refraction it is the speed of light. &lt;a href=&quot;#fnref:cropfactor&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:change&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;I modified the notation from Levoy so it would be easier to label refractions on both the left and right side, and mostly finished the derivation of the Lensmaker’s formula, which he skips for brevity. I basically follow Hecht, except he makes the transmitted rays from the left-side interface converge to a real image at \(P\) (figure 5.6), but when introducing the right-side interface, he makes the same rays diverge for no apparent reason, making a virtual image at \(P'\) (figure 5.14). &lt;a href=&quot;#fnref:change&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:size&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The formula technically doesn’t say anything about relative sizes, but it’s trivial to show the relationship with magnification, which we show later in this note. &lt;a href=&quot;#fnref:size&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:conj&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Levoy gives that explanation in the lecture, and here I explain his explanation. The denominator is \(f^4-N^2C^2U^2\), and by algebra we can basically discard the right term if \(f^4 \gg N^2C^2U^2\). Here’s the simplification:&lt;/p&gt;

\[\begin{align*}f^4&amp;amp;\gg N^2C^2U^2\\ f^2&amp;amp;\gg NCU\\ \frac{f}{N}&amp;amp;\gg\frac{CU}{f}\end{align*}\]

      &lt;p&gt;The left side is the aperture, and the right is the conjugate circle of confusion. &lt;a href=&quot;#fnref:conj&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:yourself&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;If you just want to jump between formulas, all you really need is (1) \(M=\frac{f}{U-f}\), introduced in the Magnification section, and (2) \(H=\frac{f^2}{NC}+f\approx \frac{f^2}{NC}\); when we introduced they hyperfocal distance we used the approximation to start, but the true expression may be necessary. &lt;a href=&quot;#fnref:yourself&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:projellipse&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;I found this &lt;a href=&quot;https://math.stackexchange.com/questions/2385120/area-of-a-circle-as-it-changes-to-an-ellipse-when-viewing-at-different-angles&quot;&gt;SO thread&lt;/a&gt; on the topic. &lt;a href=&quot;#fnref:projellipse&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:wrong&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;I think the lens has a minimum focus distance of \(1000\text{mm}\), but in the slides Levoy says minimum object distance. There may be some kind of discrepancy in the numbers but the example is still illustrative so I kept it. &lt;a href=&quot;#fnref:wrong&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Michal Porubcin</name></author><category term="note" /><category term="optics" /><summary type="html">This note is based on lectures 3 and 4 of Levoy’s Digital Photography. It covers basic geometric optics starting with refraction, and ends with aberrations and other lens effects. These lectures are more math-heavy to begin with, and on top of that I tried to round out the proofs. Luckily it’s nothing advanced.</summary></entry><entry><title type="html">Digital Photography Notes - Image Formation</title><link href="/note/2021/10/20/image-formation.html" rel="alternate" type="text/html" title="Digital Photography Notes - Image Formation" /><published>2021-10-20T00:00:00-05:00</published><updated>2021-10-20T00:00:00-05:00</updated><id>/note/2021/10/20/image-formation</id><content type="html" xml:base="/note/2021/10/20/image-formation.html">&lt;blockquote&gt;
  &lt;p&gt;This note is based on lecture 1 of Levoy’s Digital Photography. It covers the basics of photography, including perspective, imaging properties, camera anatomy, and tradeoffs. Beyond the bare minimum, I tried to include intuition about things, and clarify common confusions.&lt;/p&gt;

  &lt;p&gt;I expanded the material and rearranged the contents from the original lecture to be a bit more comprehensive. I like how Levoy starts with perspective and basic optics, before diving into all of the camera internals, so I kept that.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#goals-for-this-note&quot; id=&quot;markdown-toc-goals-for-this-note&quot;&gt;Goals For This Note&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#perspective&quot; id=&quot;markdown-toc-perspective&quot;&gt;Perspective&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#vanishing-points&quot; id=&quot;markdown-toc-vanishing-points&quot;&gt;Vanishing Points&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#perspective-distortion&quot; id=&quot;markdown-toc-perspective-distortion&quot;&gt;Perspective Distortion&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#distortion-on-the-periphery&quot; id=&quot;markdown-toc-distortion-on-the-periphery&quot;&gt;Distortion on the Periphery&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#directing-the-light&quot; id=&quot;markdown-toc-directing-the-light&quot;&gt;Directing The Light&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#early-imaging-devices&quot; id=&quot;markdown-toc-early-imaging-devices&quot;&gt;Early Imaging Devices&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#lenses&quot; id=&quot;markdown-toc-lenses&quot;&gt;Lenses&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#camera-anatomy&quot; id=&quot;markdown-toc-camera-anatomy&quot;&gt;Camera Anatomy&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#lens&quot; id=&quot;markdown-toc-lens&quot;&gt;Lens&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#shutter&quot; id=&quot;markdown-toc-shutter&quot;&gt;Shutter&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#sensors&quot; id=&quot;markdown-toc-sensors&quot;&gt;Sensors&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#types-of-photographic-cameras&quot; id=&quot;markdown-toc-types-of-photographic-cameras&quot;&gt;Types of Photographic Cameras&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#view-camera&quot; id=&quot;markdown-toc-view-camera&quot;&gt;View Camera&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#single-lens-reflex-slr-camera&quot; id=&quot;markdown-toc-single-lens-reflex-slr-camera&quot;&gt;Single-Lens Reflex (SLR) Camera&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#point-and-shoot-pns-camera&quot; id=&quot;markdown-toc-point-and-shoot-pns-camera&quot;&gt;Point-and-Shoot (PNS) Camera&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#phone-camera&quot; id=&quot;markdown-toc-phone-camera&quot;&gt;Phone Camera&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#mirrorless-camera&quot; id=&quot;markdown-toc-mirrorless-camera&quot;&gt;Mirrorless Camera&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#lens-and-sensor-interactions&quot; id=&quot;markdown-toc-lens-and-sensor-interactions&quot;&gt;Lens and Sensor Interactions&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#changing-focus-distance&quot; id=&quot;markdown-toc-changing-focus-distance&quot;&gt;Changing Focus Distance&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#maintaining-focus-distance&quot; id=&quot;markdown-toc-maintaining-focus-distance&quot;&gt;Maintaining Focus Distance&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#changing-field-of-view&quot; id=&quot;markdown-toc-changing-field-of-view&quot;&gt;Changing Field of View&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#maintaining-field-of-view&quot; id=&quot;markdown-toc-maintaining-field-of-view&quot;&gt;Maintaining Field of View&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#dolly-zoom&quot; id=&quot;markdown-toc-dolly-zoom&quot;&gt;Dolly Zoom&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-exposure-triangle&quot; id=&quot;markdown-toc-the-exposure-triangle&quot;&gt;The Exposure Triangle&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#the-key-takeaway&quot; id=&quot;markdown-toc-the-key-takeaway&quot;&gt;The Key Takeaway!!&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#shutter-speed&quot; id=&quot;markdown-toc-shutter-speed&quot;&gt;Shutter Speed&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#common-stops&quot; id=&quot;markdown-toc-common-stops&quot;&gt;Common Stops&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#side-effect-motion-blur&quot; id=&quot;markdown-toc-side-effect-motion-blur&quot;&gt;Side Effect: Motion Blur&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#aperture&quot; id=&quot;markdown-toc-aperture&quot;&gt;Aperture&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#common-stops-1&quot; id=&quot;markdown-toc-common-stops-1&quot;&gt;Common Stops&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#side-effect-depth-of-field&quot; id=&quot;markdown-toc-side-effect-depth-of-field&quot;&gt;Side-Effect: Depth of Field&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#iso&quot; id=&quot;markdown-toc-iso&quot;&gt;ISO&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#common-stops-2&quot; id=&quot;markdown-toc-common-stops-2&quot;&gt;Common Stops&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#side-effect-noise&quot; id=&quot;markdown-toc-side-effect-noise&quot;&gt;Side Effect: Noise&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#exposure-triangle-again&quot; id=&quot;markdown-toc-exposure-triangle-again&quot;&gt;Exposure Triangle, Again&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#exercises&quot; id=&quot;markdown-toc-exercises&quot;&gt;Exercises&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;goals-for-this-note&quot;&gt;Goals For This Note&lt;/h1&gt;
&lt;p&gt;This note is based on lecture 1 of Levoy’s Digital Photography. It covers the basics of photography, including perspective, imaging properties, camera anatomy, and tradeoffs. Beyond the bare minimum, I tried to include intuition about things, and clarify common confusions.&lt;/p&gt;

&lt;p&gt;I expanded the material and rearranged the contents from the original lecture to be a bit more comprehensive. I like how Levoy starts with perspective and basic optics, before diving into all of the camera internals, so I kept that.&lt;/p&gt;

&lt;h1 id=&quot;perspective&quot;&gt;Perspective&lt;/h1&gt;
&lt;p&gt;First we have two main assumptions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Light moves in a &lt;strong&gt;straight line&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;The lines &lt;strong&gt;converge to a point&lt;/strong&gt; at the eye.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There’s two versions of perspective:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Natural perspective&lt;/strong&gt; (Euclid): larger objects subtend smaller visual angles&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Linear perspective&lt;/strong&gt; (Brunelleschi): lines of sight intersect a &lt;strong&gt;visual plane&lt;/strong&gt;; larger objects correspond with larger images on the visual plane (aka. image plane, canvas, sensor)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/perspectives.png&quot; alt=&quot;perspectives.png&quot; width=&quot;500&quot; /&gt;
&lt;em&gt;Left: natural perspective; right: linear perspective&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Note that the two versions are not equivalent since \(\frac{y_2}{y_1}\neq\frac{\theta_2}{\theta_1}\). Linear perspective is chosen going forward.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/similartriangles.png&quot; alt=&quot;similartriangles.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Consider a point \(P\) in the scene and correspondent point \(P'\) on the intersecting image plane. Mostly ignoring what the variables mean, we can set up an equation using similar triangles:&lt;/p&gt;

\[\frac{y}{h}=\frac{f}{Z}\]

&lt;p&gt;We can see that the image height \(y\) depends &lt;em&gt;inversely&lt;/em&gt; on the real distance \(z\). That is, farther objects look smaller on the visual plane.&lt;/p&gt;

&lt;h2 id=&quot;vanishing-points&quot;&gt;Vanishing Points&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/alberti.png&quot; alt=&quot;alberti.png&quot; width=&quot;600&quot; /&gt;
&lt;em&gt;Alberti’s Method&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We know that lines of light converge at the eye, and the image above illustrates how these lines are represented on the image plane. Take note of the &lt;strong&gt;vanishing point&lt;/strong&gt;. The left portion of the diagram shows that points further away land higher on the visual plane. In the limit, infinitely far points will be located at the vanishing point in the image. The right portion shows (not as clearly) parallel lines in the real world converging to a vanishing point on the visual plane. Note that if the parallel lines in the scene are &lt;em&gt;also&lt;/em&gt; parallel to the image plane, then they &lt;em&gt;will not&lt;/em&gt; converge to a vanishing point.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/vanishingpoints.png&quot; alt=&quot;vanishingpoints.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the image above, the red lines trace out (some) of the lines parallel in the scene which converge to a vanishing point in the image. Notice how, in 1-point perspective (top-left subimage), the horizontal and vertical lines parallel to the image plane don’t converge, as stated.&lt;/p&gt;

&lt;p&gt;The same image mentions the three well-known modes of perspective, supposedly based on the “number of vanishing points”.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/4thvp.png&quot; alt=&quot;4thvp.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Actually, any pair of parallel lines will converge to a point, so there are technically infinite vanishing points. The reason we usually only consider three modes of perspective is the tendency to picture “blocky” shapes, like buildings.&lt;/p&gt;

&lt;h2 id=&quot;perspective-distortion&quot;&gt;Perspective Distortion&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/portraitdolly.png&quot; alt=&quot;portraitdolly.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Finally, observe the three different pictures taken of the same woman. The left and maybe right images can be considered distorted, while the middle one is normal. What’s going on?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/lowangle.png&quot; alt=&quot;lowangle.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The apparent “distortion” is simply perspective working as usual. The diagram above shows a birds-eye view of a head with ears and a nose. The left side corresponds with the left image of the woman, and the right side corresponds with the right image of the woman. The left image was taken very close to the camera (roughly marked by \(O\)) while the right image was taken far away. &lt;strong&gt;Perspective distortion&lt;/strong&gt; is just perspective applied to objects at distances very different from typical viewing distances. I’d say it’s the least “distorty” distortion.&lt;/p&gt;

&lt;p&gt;Note: The effect on the right is often called &lt;em&gt;lens compression&lt;/em&gt;, which is dumb because the lens has nothing to do with it. Stick with perspective distortion.&lt;/p&gt;

&lt;p&gt;You can quickly replicate this in front of a mirror. Look at your reflection at a normal distance, then move forward as much as possible. Watch your ears and nose specifically.&lt;sup id=&quot;fnref:accommodation&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:accommodation&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h2 id=&quot;distortion-on-the-periphery&quot;&gt;Distortion on the Periphery&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/naturalperspectivedistortion.png&quot; alt=&quot;naturalperspectivedistortion.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There appears to be a kind of perspective distortion which concerns objects on the periphery, which may or may not be separate from the previous section. Supposedly, an object viewed from a different viewing distance than the picture was taken will look distorted near the edges. The same topic was brought up in this &lt;a href=&quot;https://blog.photoshelter.com/2018/06/a-mathematician-weighs-in-on-lens-compression/&quot;&gt;article&lt;/a&gt; but I can’t find a name for the phenomenon. I’ll update this section if I find out more.&lt;/p&gt;

&lt;h1 id=&quot;directing-the-light&quot;&gt;Directing The Light&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/scatter.png&quot; alt=&quot;scatter.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Objects scatter light in many angles, and sensors will integrate light from many angles. Without directing the light from the scene to the sensor in some fashion, the whole sensor would end up recording similar colors.&lt;/p&gt;

&lt;h2 id=&quot;early-imaging-devices&quot;&gt;Early Imaging Devices&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/covsdurer.png&quot; alt=&quot;covsdurer.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The image above shows two devices. The bottom one shows light passing through a glass plane, exactly like our earlier construction of an intersecting visual plane. While it works for tracing, it cannot function as a sensor due to the scattering as previously mentioned. The top device is a &lt;em&gt;camera obscura&lt;/em&gt;, or &lt;strong&gt;pinhole camera&lt;/strong&gt;. The light passes through the pinhole, producing a flipped image on the wall behind it. Except for the flip and some scale differences, the images on the glass and the wall are the same.&lt;/p&gt;

&lt;p&gt;Benefits of pinhole camera:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;No distortion (except diffraction)&lt;/li&gt;
  &lt;li&gt;Infinite depth of field: everything is in focus&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/widerpinhole.png&quot; alt=&quot;widerpinhole.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If the pinhole is too large, the whole image becomes blurred, as we can see above. It is too small, we experience diffraction effects (blurring again). If we avoid both those problems, the pinhole still has the downside of letting in little light.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/nowlens.png&quot; alt=&quot;nowlens.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Instead we can use a convex chunk of glass called a &lt;strong&gt;lens&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;lenses&quot;&gt;Lenses&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/lensoptics.png&quot; alt=&quot;lensoptics.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Observe how a simple lens works in the image above. The &lt;strong&gt;focal length&lt;/strong&gt; \(f\) is:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;a property of the lens&lt;/li&gt;
  &lt;li&gt;cannot be changed (for simple lenses)&lt;/li&gt;
  &lt;li&gt;signifies where parallel light rays converge&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/gausslens1.png&quot; alt=&quot;gausslens1.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Above is an idealized diagram of light passing through a lens. Levoy calls it &lt;strong&gt;Gauss’ lens tracing construction&lt;/strong&gt;, but everyone else seems to call it a ray diagram.&lt;/p&gt;

&lt;p&gt;The focal length is &lt;em&gt;not&lt;/em&gt; where any given object is in focus. The diagram above shows a lens, the focal length (the distance from the lens to either of the two dots), an &lt;strong&gt;object&lt;/strong&gt; (the left arrow), and its flipped &lt;strong&gt;image&lt;/strong&gt; (the right arrow). The horizontal distance from the object to the center of the lens is called the &lt;strong&gt;object distance&lt;/strong&gt;; the &lt;strong&gt;image distance&lt;/strong&gt; is similarly defined.&lt;/p&gt;

&lt;p&gt;The diagram also traces the light rays from &lt;em&gt;a single point&lt;/em&gt; on the object. We can see that the outer rays form a diamond, and the left and right points of the diamond indicate where the object and sensor should be for the object to be in focus. We can actually say that any image point on the same vertical line is in focus (we’d just have to draw lots of diamonds). Since we usually work in three dimensions, the vertical line turns into a vertical plane, called the &lt;strong&gt;focal plane&lt;/strong&gt;, which corresponds to the sensor of the camera. The position of the sensor determines what objects in the world are in focus. The distance from the center of the lens to an in-focus plane in the scene is called the &lt;strong&gt;focus distance&lt;/strong&gt;.&lt;sup id=&quot;fnref:working&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:working&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Clarification&lt;/strong&gt;: The object and image distances can be used outside the context of the sensor. Usually we also discuss objects forming a focused image at the focal plane, but if not it must be explicitly stated.&lt;/p&gt;

&lt;p&gt;The basic relationship between object distance \(s_o\), image distance \(s_i\), and focal length \(f\) is:&lt;/p&gt;

\[\frac{1}{s_o} + \frac{1}{s_i} = \frac{1}{f}\]

&lt;p&gt;To get a good intuition about the formula, here is a &lt;a href=&quot;https://ophysics.com/l12.html&quot;&gt;good website&lt;/a&gt; to play with the optics of a simple lens. In particular:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;see what happens to the image when you move the object past twice the focal length (or past \(2f\)).&lt;/li&gt;
  &lt;li&gt;when you move the object in between \(2f\) and \(f\).&lt;/li&gt;
  &lt;li&gt;when you move the object to \(f\)&lt;/li&gt;
  &lt;li&gt;when you move the object between \(f\) and \(0\).&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;camera-anatomy&quot;&gt;Camera Anatomy&lt;/h1&gt;

&lt;p&gt;Equipped with an intuition about light and lens, we can now learn how an image is captured. Though a camera has many components, we will cover three basic ones: the &lt;strong&gt;lens&lt;/strong&gt;, &lt;strong&gt;shutter&lt;/strong&gt;, and &lt;strong&gt;sensor&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;lens&quot;&gt;Lens&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/lenscross.jpg&quot; alt=&quot;lenscross.jpg&quot; width=&quot;300&quot; /&gt;
&lt;em&gt;Cross section of a lens&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Earlier we made the simplification of a single lens. In fact, a typical camera lens has multiple lens elements mounted on a common axis, forming a &lt;strong&gt;compound lens&lt;/strong&gt;. The advantage over a single lens is a reduction of optical abberations.&lt;/p&gt;

&lt;p&gt;Compound lenses can be classified by whether they can change their focal length:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;zoom&lt;/strong&gt;: the lens elements can move to change the focal length of the compound lens
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;varifocal zoom&lt;/strong&gt;: the objects in focus change as the focal length (zoom amount) changes&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;parfocal zoom&lt;/strong&gt;: the objects in focus &lt;em&gt;don’t&lt;/em&gt; change as the focal length changes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;prime&lt;/strong&gt;: the focal length is fixed&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Prime lenses tend to be smaller, sharper, but more expensive than zoom lenses. Varifocal lenses are more common in photography, while parfocal lenses are used more in cinema and broadcast TV. Remember, we can always change the focus distance, even on prime lenses, by moving the lens closer to or farther from the sensor.&lt;/p&gt;

&lt;p&gt;The lens elements are all contained in a complex housing. In the middle lies the &lt;strong&gt;aperture&lt;/strong&gt;, surrounded by about half the lens elements on both sides. It’s an adjustable opening that affects how much light &lt;em&gt;passively&lt;/em&gt; enters the camera body.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Clarification&lt;/strong&gt;: When we’re talking about a camera “lens”, we’re referring to the whole assembly, including the lens elements, the aperture, and the housing. The lens is sometimes referred to as the “glass”. While the term still refers to the whole assembly, it comes from the large amount of glass in the lens due to the lens elements.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/imagecircle.jpg&quot; alt=&quot;imagecircle.jpg&quot; width=&quot;300&quot; /&gt;
&lt;em&gt;A simulation of the image circle&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The cross section of the cone of light exiting the lens is called the &lt;strong&gt;image circle&lt;/strong&gt;. Until now we’ve assumed the sensor was inscribed within the image circle, but that’s not necessariy true. If the sensor is larger, then &lt;strong&gt;vignetting&lt;/strong&gt; can occur in the image, meaning the corners and/or edges of the sensor won’t be fully exposed. If the sensor is smaller, then it is underutilizing the light provided by the lens, or &lt;strong&gt;cropping&lt;/strong&gt;. Vignetting is usually undesirable, but cropping isn’t necessarily bad.&lt;/p&gt;

&lt;p&gt;Each lens has a &lt;strong&gt;lens format&lt;/strong&gt;, defining the size of the light cone it casts, which determines its sensor compatibility. A lens can be used on sensor sizes less than or equal to the size it was designed for, without vignetting.&lt;/p&gt;

&lt;p&gt;In summary, the lens has three independent, distinguishing features:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;focal length range&lt;/li&gt;
  &lt;li&gt;aperture range&lt;/li&gt;
  &lt;li&gt;lens format&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;shutter&quot;&gt;Shutter&lt;/h2&gt;

&lt;p&gt;The shutter is crucial for letting &lt;em&gt;precise&lt;/em&gt; amounts of light hit the sensor. It begins closed, letting &lt;em&gt;no&lt;/em&gt; light hit the sensor, and only opens briefly when a picture is taken. A shutter cycle (open, close, and reset) is often called an &lt;strong&gt;exposure&lt;/strong&gt; (a term which also refers to the amount of light hitting the sensor &lt;em&gt;during&lt;/em&gt; the exposure; see the section Exposure Triangle). The &lt;strong&gt;shutter speed&lt;/strong&gt; is an adjustable property of the shutter determining the speed of the exposure.&lt;/p&gt;

&lt;p&gt;Shutters can be classified in several ways:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;mechanical&lt;/strong&gt;: a physical mechanism in front of the sensor to block light
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;leaf (or diaphragm) shutter&lt;/strong&gt;: a series of blades in a circle which open and close, located in the lens&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;focal-plane shutter&lt;/strong&gt;: one or two “curtains” which open and close; located just in front of the sensor&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;electronic&lt;/strong&gt;: implemented in the sensor electronically
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;global shutter&lt;/strong&gt;: the entire sensor is exposed at once&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;rolling shutter&lt;/strong&gt;: the sensor is exposed line by line&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/2shutter.jpg&quot; alt=&quot;2shutter.jpg&quot; width=&quot;400&quot; /&gt;
&lt;em&gt;Left: leaf shutter; right: focal-plane shutter&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Note: a leaf shutter looks kind of like an aperture but it’s not the same!&lt;/p&gt;

&lt;p&gt;Leaf shutters are quiet and relatively slow, while focal-plane shutters are loud and fast, and can cause vibrations during the shot. Electronic shutters can be even faster than focal-plane shutters, without any noise or vibrations. Rolling shutters are pretty common on consumer cameras, while global shutters are mostly limited to small resolutions or high-end cameras. Finally, rolling shutters are susceptible to &lt;strong&gt;rolling shutter distortion&lt;/strong&gt;, also called the “jello effect”.&lt;/p&gt;

&lt;h2 id=&quot;sensors&quot;&gt;Sensors&lt;/h2&gt;

&lt;p&gt;Light is directed by the lens onto either a photographic film or a digital sensor. We’ll limit the discussion in this section to the latter.&lt;/p&gt;

&lt;p&gt;First, there are two main types of imaging sensors:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Charge-coupled device (CCD)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Complementary metal–oxide–semiconductor (CMOS)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CMOS sensors are almost ubiquitous in digital consumer cameras, while CCDs have been relegated to niches like scientific imaging or astrophotography.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/sensorsizes.jpg&quot; alt=&quot;sensorsizes.jpg&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;sensor format&lt;/strong&gt; or sensor size, captures the dimensions of the sensor in milimeters. The image above shows many common digital sensor sizes. The 35mm format is called a &lt;strong&gt;full-frame&lt;/strong&gt; sensor.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Terminology Rant&lt;/strong&gt;: Sensor formats have names like Pentax 645D or Micro Four Thirds. These usually have &lt;em&gt;nothing&lt;/em&gt; to do with the sensor dimensions they represent. The 35 mm format is not 35 mm in width, height, or diagonal, and gets its name from 35 mm film. The 1-inch format is closer to half an inch in width, and gets its name from television camera tubes.&lt;/p&gt;

&lt;p&gt;A sensor is a big grid of &lt;strong&gt;pixels&lt;/strong&gt; or &lt;strong&gt;photosites&lt;/strong&gt;, and each pixel is like a bucket for photons. When a picture is taken, light fills the buckets for the desired period of time. The data read off the sensor corresponds to the amount of light gathered in each pixel.&lt;/p&gt;

&lt;p&gt;The number of pixels on a sensor, or the &lt;strong&gt;resolution&lt;/strong&gt;, is measured in megapixels or MP, which stands for a million pixels. The resolution can also be represented by an ordered pair of pixels along the width and height. The ratio of pixels along the width vs height is called the &lt;strong&gt;aspect ratio&lt;/strong&gt;. For example, 24 MP is equivalent to 6000 x 4000 pixels, which gives an aspect ratio of 3:2.&lt;sup id=&quot;fnref:aspectratio&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:aspectratio&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;The same lens can give different aspect ratios via cropping. For example, the 3:2 sensor can be cropped to 16:9. Since pixels are cut off, the resolution decreases.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/aspectratio1.png&quot; alt=&quot;aspectratio1.png&quot; width=&quot;300&quot; /&gt;
&lt;em&gt;Different aspect ratios inscribed on the image circle&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Finally, the pixels themselves can be different sizes, and are measured in microns (μm). Two sensors can have the same size but differing resolutions if the pixel sizes are different.&lt;/p&gt;

&lt;p&gt;In summary, the three independent, distinguishing features of a sensor are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;sensor format&lt;/li&gt;
  &lt;li&gt;resolution&lt;/li&gt;
  &lt;li&gt;pixel size&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;types-of-photographic-cameras&quot;&gt;Types of Photographic Cameras&lt;/h1&gt;

&lt;p&gt;Here is a collection of different kinds of cameras used for taking still pictures, presented in roughly chronological order by the time they were created. We will see how the components from the previous section can be combined.&lt;/p&gt;

&lt;h2 id=&quot;view-camera&quot;&gt;View Camera&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/viewcamera.png&quot; alt=&quot;viewcamera.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A view camera is composed of two parts, a front and rear standard, connected by a pleated box. Modern view cameras are sometimes called &lt;em&gt;large-format&lt;/em&gt; cameras, referring to their large sensors. Though the basic design is fairly old, dating to the nineteenth century, it offers the greatest freedom of positioning for the lens relative to the sensor, which is why it still finds use today.&lt;/p&gt;

&lt;p&gt;The light passes through the lens and aperture (not pictured), casting an inverted image on a glass pane in the back. Once the image is composed, the shutter is closed, the glass is manually replaced with film, and the shutter is triggered, exposing a photographic film. The film is a light-sensitive material used to capture the image.&lt;/p&gt;

&lt;p&gt;View cameras use a leaf shutter, located in the middle of the lens.&lt;/p&gt;

&lt;h2 id=&quot;single-lens-reflex-slr-camera&quot;&gt;Single-Lens Reflex (SLR) Camera&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/slr.png&quot; alt=&quot;slr.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The basic SLR design is shared by most modern cameras.&lt;/p&gt;

&lt;p&gt;Here it’s easier to see that the lens (not labeled) is really a complex assembly of lenses, with an aperture in the middle. Instead of viewing the image on a glass pane, we now have a viewfinder. The light normally passes through the lens, bounces off a mirror, passes through a pentaprism or pentamirror above (not labeled), and exits the viewfinder. Finally, notice how the shutter is located right in front of the film, rather than inside the lens.&lt;/p&gt;

&lt;p&gt;When a picture is taken, the mirror flips up, allowing light to pass to the shutter, and the shutter opens briefly, exposing the film. Finally the shutter closes and the mirror flips back down. The mirror movement makes SLRs louder than other types of cameras.&lt;/p&gt;

&lt;p&gt;SLRs use focal-plane shutters. A digital SLR or DSLR just replaces the film with a digital sensor.&lt;/p&gt;

&lt;h2 id=&quot;point-and-shoot-pns-camera&quot;&gt;Point-and-Shoot (PNS) Camera&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/pns.jpg&quot; alt=&quot;pns.jpg&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;PNS or compact cameras are typically cheap, small, and useful for taking quick photos. Unlike on (D)SLRs and mirrorless cameras, the lenses are usually non-interchangeable. Mobile phones have mostly pushed PNS cameras out of the market.&lt;/p&gt;

&lt;p&gt;Since the light in the viewfinder takes a different path from the the light hitting the film, the viewfinder isn’t a completely accurate representation of the final image.&lt;/p&gt;

&lt;p&gt;There are digital versions, replacing the film with a digital sensor. Unlike the previous two camera types, most PNS cameras use electronic shutters.&lt;/p&gt;

&lt;h2 id=&quot;phone-camera&quot;&gt;Phone Camera&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/phonecamera.jpg&quot; alt=&quot;phonecamera.jpg&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Mobile phones come with one or more small cameras. These cameras have electronic shutters, lenses with fixed focus and tiny focal lengths, and small sensors with small pixels and medium-low resolutions. A recent trend has been to include multiple cameras with different focal length lenses. Instead of optical zoom, most phone cameras use digital zoom, meaning the image is cropped and enlarged &lt;em&gt;after&lt;/em&gt; exposure. The phone screen contains all the controls and serves as the viewfinder.&lt;/p&gt;

&lt;h2 id=&quot;mirrorless-camera&quot;&gt;Mirrorless Camera&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/mirrorless.jpg&quot; alt=&quot;mirrorless.jpg&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A mirrorless camera is very similar to a DSLR, but it ditches the mirror and optical viewfinder. The new electronic viewfinder relies on the light hitting the image sensor in real-time, meaning the shutter must be open in its default state.&lt;/p&gt;

&lt;p&gt;Like (D)SLRs, mirrorless cameras primarily use mechanical shutters. When a picture is taken, the shutter closes in preparation, then opens briefly, closes again, then opens to make the electronic viewfinder available again. Some mirrorless cameras implement hybrid electronic-mechanical shutters to reduce vibrations. For both DSLRs and mirrorless cameras, a live electronic viewfinder with lower resolution will utilize an electronic shutter. A purely electronic shutter doesn’t have fast enough readout speeds for the full resolution, however, and would produce a strong jello effect.&lt;/p&gt;

&lt;p&gt;The term “mirrorless camera” always refers to a digital camera.&lt;/p&gt;

&lt;h1 id=&quot;lens-and-sensor-interactions&quot;&gt;Lens and Sensor Interactions&lt;/h1&gt;

&lt;p&gt;For this section, let’s limit the settings we have direct control over to the following:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;focal length&lt;/li&gt;
  &lt;li&gt;sensor size&lt;/li&gt;
  &lt;li&gt;image distance&lt;/li&gt;
  &lt;li&gt;object distance&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By manipulating those, we can predictably affect properties of our image like:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;focus distance&lt;/li&gt;
  &lt;li&gt;field of view&lt;/li&gt;
  &lt;li&gt;perspective distortion&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;changing-focus-distance&quot;&gt;Changing Focus Distance&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/changefocus.png&quot; alt=&quot;changefocus.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To focus on different things, we change the distance of the sensor relative to the lens (in cameras, it is actually the lens that moves). That is:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;by &lt;em&gt;fixing&lt;/em&gt; the focal length…&lt;/li&gt;
  &lt;li&gt;and &lt;em&gt;changing&lt;/em&gt; the distance between lens and sensor…&lt;/li&gt;
  &lt;li&gt;we are &lt;em&gt;changing&lt;/em&gt; the focus distance&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Looking at the top lens diagram, we put the sensor at the focal length. This gives us parallel lines on the left, which will only converge &lt;em&gt;at infinity&lt;/em&gt;. In other words, if we place the sensor right at the focal length, only objects infinitely far away will be perfectly in focus.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;minimum focus distance&lt;/strong&gt;&lt;sup id=&quot;fnref:mfd&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:mfd&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; represents the closest distance at which a lens can focus, based on the the maximum distance the sensor can move away from the lens. Though it will likely be longer, it’s physically limited by the focal length, since the sensor would have to be placed at infinity.&lt;/p&gt;

&lt;h2 id=&quot;maintaining-focus-distance&quot;&gt;Maintaining Focus Distance&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/tree.png&quot; alt=&quot;tree.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The diagram above&lt;sup id=&quot;fnref:focallength&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:focallength&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt; illustrates lenses with different focal lengths. &lt;strong&gt;Stronger lenses&lt;/strong&gt;, so called because they bend light more strongly, have smaller focal lengths. &lt;strong&gt;Weaker lenses&lt;/strong&gt; have longer focal lengths. We can also say:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;if we want to &lt;em&gt;fix&lt;/em&gt; the focus distance…&lt;/li&gt;
  &lt;li&gt;and &lt;em&gt;change&lt;/em&gt; the focal length (changing the lens)…&lt;/li&gt;
  &lt;li&gt;we need to &lt;em&gt;change&lt;/em&gt; the distance between lens and sensor&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;changing-field-of-view&quot;&gt;Changing Field of View&lt;/h2&gt;

&lt;p&gt;The extent of the world captured in the image is the &lt;strong&gt;field of view (FOV)&lt;/strong&gt;, and is represented as an angle. There’s two ways to change the field of view:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;change the focal length&lt;/li&gt;
  &lt;li&gt;change the sensor size&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/sensorfov1.png&quot; alt=&quot;sensorfov1.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the original tree diagram, we ignored the sensor size. Now we show a fixed sensor size, shown in red. Notice how a longer focal length corresponds with a lower fov, which makes sense since a weaker lens can bend less light to hit the same sensor.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/focallengthfov.png&quot; alt=&quot;focallengthfov.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The image above offers an even better visualization of the same relationship.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/sensorfov.png&quot; alt=&quot;sensorfov.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The other way to change FOV is to change the sensor size. This version of the diagram includes a red and a yellow sensor, with corresponding fovs. Obviously, if the sensor is smaller, it will capture a smaller angle of the world.&lt;/p&gt;

&lt;p&gt;Both relationships are captured in the following equation, derivable from simple trigonometry:&lt;/p&gt;

\[FOV = 2\arctan\left(\frac{h}{2f}\right)\]

&lt;p&gt;where \(h\) is the height of the sensor and \(f\) is the focal length.&lt;/p&gt;

&lt;h2 id=&quot;maintaining-field-of-view&quot;&gt;Maintaining Field of View&lt;/h2&gt;

&lt;p&gt;Sensor size and focal length should increase proportionally to keep the same FOV. This principle is the basis for a popular way of comparing lens-sensor systems, described below.&lt;/p&gt;

&lt;p&gt;Sensor sizes can be compared using a &lt;strong&gt;crop factor&lt;/strong&gt;, which can be expressed as a formula:&lt;/p&gt;

\[\text{crop factor} = \frac{\text{diagonal of full-frame sensor}}{\text{diagonal of this sensor}}\]

&lt;p&gt;A full-frame sensor has a crop factor of 1, and serves as a reference format to which other sizes are compared. For example, the smaller micro 4/3 sensor has a crop factor of ~2. Besides facilitating sensor comparison, the crop factor is used to convert true focal lengths to a focal length which would produce the same FOV on a full-frame sensor.&lt;/p&gt;

&lt;p&gt;Let’s say we’re using a 50 mm lens with a micro 4/3 sensor, which has a crop factor of 2.0. To “erase” the cropping effect due to the sensor, we can multiply the focal length by the crop factor. So a 50 mm micro 4/3 lens is equivalent to a 75 mm full-frame lens.&lt;/p&gt;

&lt;h2 id=&quot;dolly-zoom&quot;&gt;Dolly Zoom&lt;/h2&gt;

&lt;p&gt;If we change the focal length and the position of the camera (object distance) in the right way, we can get a nice effect called the &lt;strong&gt;dolly zoom&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/dolly1.png&quot; alt=&quot;dolly1.png&quot; width=&quot;150&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here’s a picture of the Dolly Zoom in action. The camera moves away from the subject, while increasing focal length. The key is to combine these in such a way that the size of the subject in the image remains &lt;em&gt;fixed&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/dolly.png&quot; alt=&quot;dolly.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here’s a sample result, starting from the left and ending with the right. Notice how we can only fix the size of objects in one plane parallel to the sensor. The woman stays the same size, but the apparent size of the man in the back (in a further plane) changes. In other words, perspective relationships are &lt;em&gt;not preserved&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Note: the same effect was used to capture the pictues of the woman in the Perspective Distortion section.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Clarification&lt;/strong&gt;: Both focal length and object distance contribute to keeping the woman the same size. However, it can be wrongly concluded that a higher focal length produces more lens compression. As we established earlier, changes in perspective relationships are due to changing object distance. We just happen to place the camera closer to the subjects when using a wide-angle (low focal length) lens.&lt;/p&gt;

&lt;h1 id=&quot;the-exposure-triangle&quot;&gt;The Exposure Triangle&lt;/h1&gt;

&lt;p&gt;From now on we assume we’re working with digital sensors instead of film, since the differences are minor.&lt;/p&gt;

&lt;p&gt;As we saw, the captured image depends on the amount of light hitting the sensor. This quantity is known as the &lt;strong&gt;exposure&lt;/strong&gt; \(H\), and is related to &lt;strong&gt;irradiance&lt;/strong&gt; \(E\) and &lt;strong&gt;exposure time&lt;/strong&gt; \(T\) like so:&lt;/p&gt;

\[H=E\times T\]

&lt;ul&gt;
  &lt;li&gt;Irradiance, controlled by aperture, is the amount of light falling on a unit area of the sensor per second.&lt;/li&gt;
  &lt;li&gt;Exposure time, controlled by the shutter speed, is the time in seconds in which the sensor is exposed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Finally, &lt;em&gt;after&lt;/em&gt; the light hits the sensor, the brightness of the final image can be increased electronically via a parameter called the &lt;strong&gt;ISO&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Aperture, shutter speed, and ISO are closely connected, forming the &lt;strong&gt;exposure triangle&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Terminology Rant&lt;/strong&gt;: The term exposure is confusing. First, as stated earlier, it can refer to a single shutter cycle or the quantity of light hitting the sensor during that shutter cycle. Second, ISO &lt;em&gt;doesn’t affect exposure at all&lt;/em&gt;, but people still include it in the so-called “exposure triangle”. Any triangle including ISO should be about brightness, but the “brightness triangle” hasn’t caught on yet…&lt;/p&gt;

&lt;h2 id=&quot;the-key-takeaway&quot;&gt;The Key Takeaway!!&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/exptriangle.jpg&quot; alt=&quot;exptriangle.jpg&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The image above shows different scales for each component of the triangle. Each component has predictable effects on the brightness of the resulting image, as we’ll see in the coming sections, and we can standardize the scales of the components against the change in brightness. What this means is that, for example, one step, or “stop”, on the ISO scale &lt;em&gt;has the same relative effect on brightness&lt;/em&gt; as one step on the aperture scale.&lt;/p&gt;

&lt;h2 id=&quot;shutter-speed&quot;&gt;Shutter Speed&lt;/h2&gt;

&lt;p&gt;The shutter speed or exposure time \(T\), measured in fractions of a second, is the time the shutter is open. It has a &lt;em&gt;linear effect&lt;/em&gt; on the exposure, until the sensor saturates (it cannot integrate any more light). Below are sample shutter speeds, and common uses:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;1/2000: birds in flight&lt;/li&gt;
  &lt;li&gt;1/1000: sports&lt;/li&gt;
  &lt;li&gt;1/250: portraits&lt;/li&gt;
  &lt;li&gt;1/125: landscapes&lt;/li&gt;
  &lt;li&gt;5: blurring water&lt;/li&gt;
  &lt;li&gt;20: astrophotography&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Terminology Rant&lt;/strong&gt;: Why is shutter &lt;em&gt;speed&lt;/em&gt; measured in seconds? Why is it not called shutter &lt;em&gt;time&lt;/em&gt;? Now we’re stuck with higher shutter speeds having smaller units, like 1/2000, and lower shutter speeds having larger units, like 1/125. (…this is just the beginning – wait until we get to aperture)&lt;/p&gt;

&lt;p&gt;Humans can hand-hold down to 1/60. There is sometimes an additional mode called bulb (often symbolized as B), which allows one to keep the shutter open manually as long as desired.&lt;/p&gt;

&lt;p&gt;The shutter speed abstracts away many details about the shutter itself. In particular, it applies to mechanical and electronic shutters.&lt;/p&gt;

&lt;h3 id=&quot;common-stops&quot;&gt;Common Stops&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/shutterstop.png&quot; alt=&quot;shutterstop.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The shutter speeds increase regularly by \(\times 2\), increasing brightness by \(\times 2\).&lt;sup id=&quot;fnref:stopconspiracy&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:stopconspiracy&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h3 id=&quot;side-effect-motion-blur&quot;&gt;Side Effect: Motion Blur&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Motion blur&lt;/strong&gt; is self-explanatory. It is usually treated qualitatively.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/motionblur.png&quot; alt=&quot;motionblur.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Shutter speed is &lt;em&gt;directly proportional&lt;/em&gt; to &lt;strong&gt;motion blur&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;aperture&quot;&gt;Aperture&lt;/h2&gt;

&lt;p&gt;Recall the aperture is a hole in the lens that determines how much light enters the body, or the irradiance \(E\). The irradiance is proportional to:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;the &lt;em&gt;square&lt;/em&gt; of the aperture diameter \(A\)&lt;/li&gt;
  &lt;li&gt;the &lt;em&gt;inverse square&lt;/em&gt; of the distance between the aperture and the sensor (~ the focal length \(f\))&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;(1) is because the diameter is related to the area of a circle as \(\text{area}=\pi(\frac{A}{2})^2\). If the diameter is \(\times 2\), the light coming through the circular aperture is \(\times 4\).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/irradiancedistance.png&quot; alt=&quot;irradiancedistance.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(2) can be visualized using a cone of light coming from the lens (see the image above). If the distance to the sensor increases \(\times 2\), by similar triangles, the diameter of the circular base of the cone increases \(\times 2\). From the same principle as (1), the area of the base of the cone increases \(\times 4\). Assuming the light exiting the aperture is the same amount, by conservation of energy we can say the amount of light hitting the base of the cone, or the irradiance, decreases \(\times 4\).&lt;/p&gt;

&lt;p&gt;In order for the aperture values to capture both (1) and (2), a unitless quantity called the &lt;strong&gt;f-number&lt;/strong&gt; or f-stop is defined as:&lt;/p&gt;

\[N=\frac{f}{A}\]

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/aperturesize1.png&quot; alt=&quot;aperturesize1.png&quot; width=&quot;400&quot; /&gt;
&lt;em&gt;Two lenses with the same aperture N&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We’re usually interested in the aperture diameter relative to whatever focal length we’re using, so it is customary to write the f-number as a fraction \(f/N\). For example, referring to the image above:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;\(f\)/2.0 on a 50mm lens means the aperture diameter is 25mm&lt;/li&gt;
  &lt;li&gt;\(f\)/2.0 on a 100mm lens means the aperture diameter is 50mm&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Following the pattern, it’s clear that a telephoto lens (large focal length) with a small f-number requires fat lenses.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Terminology Rant&lt;/strong&gt;: The notation is complete trash! When you change the aperture, you are only controlling the diameter, and not the distance to the sensor. However, the aperture is usually expressed as an f-number instead of the diameter. The f-number is technically \(N\), but is almost always expressed as \(f/N\), for some value of \(N\). The “\(f/\)” is pure notation for the f-number, like a “$” represents dollars. What is supposedly clever and cool about this notation is that, from the formula for the f-number, \(f/N\) actually produces the aperture diameter \(A\). So when we’re talking about “aperture”, we really mean a ratio of focal length to aperture diameter, which we write to look like the aperture diameter.&lt;/p&gt;

&lt;p&gt;It gets worse when we’re communicating aperture: consider a) \(f/2.0\) and b) \(f/2.8\). We say:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;(a) and (b) are called apertures &lt;em&gt;or&lt;/em&gt; f-numbers interchangeably&lt;/li&gt;
  &lt;li&gt;(a) is a smaller f-number than (b)&lt;/li&gt;
  &lt;li&gt;(a) is a higher aperture than (b)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To summarize, when stating f-numbers or apertures (“set the f-number to …”), we use the &lt;em&gt;notation&lt;/em&gt; \(f/N\), but when comparing f-numbers (“this f-number is higher/lower than that f-number” or “increase/decrease the f-number”) we use \(N\), BUT when comparing apertures, we use the &lt;em&gt;fraction&lt;/em&gt; \(f/N\). &lt;em&gt;Capiche&lt;/em&gt;?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/aperture1.png&quot; alt=&quot;aperture1.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Clarification&lt;/strong&gt;: It might seem that a smaller aperture would decrease the image circle by “blocking” the outer edge, but it does not. Notice how some rays from any point on the object will always reach the sensor. The edge of the image circle may actually get sharper.&lt;/p&gt;

&lt;p&gt;How does the f-number affect brightness? From the formula, if \(N\) increases \(\times 2\), then \(A\) is reduced \(\times 2\), which by (1) reduces the light by \(\times 4\).&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;\(f\)/2.0 to \(f\)/4.0 reduces light by \(\times 4\), as described&lt;/li&gt;
  &lt;li&gt;To reduce light by \(\times 2\), increase \(N\) by \(\times \sqrt{2}\)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;common-stops-1&quot;&gt;Common Stops&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/aperturestop.png&quot; alt=&quot;aperturestop.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The f-numbers increase regularly by \(\times \sqrt{2}\), decreasing brightness by \(\times 2\).&lt;/p&gt;

&lt;h3 id=&quot;side-effect-depth-of-field&quot;&gt;Side-Effect: Depth of Field&lt;/h3&gt;

&lt;p&gt;The &lt;strong&gt;depth of field&lt;/strong&gt; is the range of acceptable focus. Smaller depth of fields produce more blurring of the background and foreground. Like motion blur, it is usually treated qualitatively.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/depthoffield.png&quot; alt=&quot;depthoffield.png&quot; width=&quot;500&quot; /&gt;
&lt;em&gt;Left: low depth of field; right: high depth of field&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The aperture diameter is &lt;em&gt;inversely proportional&lt;/em&gt; to the depth of field.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Increasing \(N\) by \(\times 2\) increases depth of field by \(\times 2\).&lt;/li&gt;
  &lt;li&gt;Changing \(N\) or \(f\) may not have any effect if the other compensates.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/depthoffield1.png&quot; alt=&quot;depthoffield1.png&quot; width=&quot;300&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here’s a simplified explanation of how depth of field works:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;As we saw in the section “Changing Focus Distance”, there is always a point in the scene (or, really, a plane) in focus on the sensor.&lt;/li&gt;
  &lt;li&gt;We can visualize this as a series of light cones (&lt;em&gt;top subimage&lt;/em&gt;).&lt;/li&gt;
  &lt;li&gt;Moving the sensor causes a defocusing or blurring effect, equivalent to moving the point in the scene.&lt;/li&gt;
  &lt;li&gt;The blurring must be smaller than our allowable &lt;strong&gt;circle of confusion&lt;/strong&gt; (&lt;em&gt;middle subimage&lt;/em&gt;)&lt;/li&gt;
  &lt;li&gt;The circle of confusion corresponds with the &lt;strong&gt;depth of focus&lt;/strong&gt;, which corresponds with the depth of field.&lt;/li&gt;
  &lt;li&gt;Notice how decreasing the aperture  \(\times 2\) increases the depth of field \(\times 2\) (&lt;em&gt;bottom subimage&lt;/em&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;iso&quot;&gt;ISO&lt;/h2&gt;

&lt;p&gt;Whereas shutter speed and aperture are physical mechanisms, the ISO is a gain parameter. The ISO multiplies the analog signal &lt;em&gt;after&lt;/em&gt; light integration but &lt;em&gt;before&lt;/em&gt; conversion to a digital signal. It ultimately boosts the brightness of the final image.&lt;/p&gt;

&lt;p&gt;Note that the ISO is different from raising exposure in post-processing (e.g. Photoshop), since the latter boosts the digital signal. Some cameras offer “expanded ISO”, which just boosts the digital signal.&lt;/p&gt;

&lt;h3 id=&quot;common-stops-2&quot;&gt;Common Stops&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/isostop.png&quot; alt=&quot;isostop.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The ISO increases regularly by \(\times 2\), increasing brightness by \(\times 2\).&lt;/p&gt;

&lt;h3 id=&quot;side-effect-noise&quot;&gt;Side Effect: Noise&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/iso.png&quot; alt=&quot;iso.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The greater the ISO, the noisier the image. The picture above shows increasing ISO while keeping brightness constant via other means, highlighting the increasing noise.&lt;/p&gt;

&lt;h2 id=&quot;exposure-triangle-again&quot;&gt;Exposure Triangle, Again&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/image-formation/talvala.jpg&quot; alt=&quot;talvala.jpg&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The green blobs represent changes to component(s) on the exposure triangle, while yellow blobs represent changes in brightness or side effects. Each green blob affects &lt;em&gt;two&lt;/em&gt; adjacent yellow blobs simultaneously. If we move the same number of stops for two components of the triangle at once, we can keep the brightness constant.&lt;/p&gt;

&lt;p&gt;Here’s an interactive &lt;a href=&quot;https://sites.google.com/site/marclevoylectures/applets/variables-that-affect-exposure&quot;&gt;applet&lt;/a&gt; demonstrating these tradeoffs.&lt;/p&gt;

&lt;h1 id=&quot;exercises&quot;&gt;Exercises&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;Q: Does an \(f/2\) cell phone lens gather as much light from each patch of the scene as an \(f/2\) SLR lens?&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;A:&lt;/strong&gt; A phone camera lens has a smaller focal length because the sensor is small, which allows the FOV to stay about the same as an SLR (also to fit the form factor of a phone ). With \(f\) and \(A\) both smaller, the f-number (\(N=\frac{f}{A}\)) stays constant.&lt;/p&gt;

&lt;p&gt;Another way to think about it is that the smaller phone lens gathers less light, but it is a stronger lens (smaller focal length), meaning it concentrates the light more, so the amount of light &lt;em&gt;per unit area&lt;/em&gt; remains the same.&lt;/p&gt;

&lt;p&gt;Now, a phone lens has smaller pixels to keep the resolution, or pixel count, the same (or similar). The same number of pixels are exposed, but each captures fewer photons, so the result is noisier.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:accommodation&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;You might object that the experiment is ruined by various factors, including acommodation of the eyeball. The thing is, no matter how much you focus or defocus, your nose will still look fat and your ears will still be covered by the sides of your head. &lt;a href=&quot;#fnref:accommodation&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:working&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;There is another term called working distance, which measures the distance from the front of the lens to the subject. &lt;a href=&quot;#fnref:working&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:aspectratio&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Mathematically one resolution in MP can be satisfied by multiple aspect ratios, and I’m not sure if there’s some kind of standardization so we know which aspect ratio we mean by a given resolution. &lt;a href=&quot;#fnref:aspectratio&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:mfd&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Confusingly, minimum focus distance is measured from the closest object to the &lt;em&gt;sensor&lt;/em&gt;, not the lens. &lt;a href=&quot;#fnref:mfd&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:focallength&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Here we have a tree whose distance from the lens is much larger than the focal length \(f\). It’s not super clear in the diagram but the tree forms an image &lt;em&gt;just past&lt;/em&gt; \(f\) (not &lt;em&gt;at&lt;/em&gt; \(f\), since it’s not infinitely far away). &lt;a href=&quot;#fnref:focallength&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:stopconspiracy&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;The ticks on the shutter speed scale don’t &lt;em&gt;exactly&lt;/em&gt; progress by multiples of 2. Some are rounded off since the error is inconsequential for such small fractions and it makes mental math easier. &lt;a href=&quot;https://photo.stackexchange.com/questions/49860/is-there-a-sane-reason-why-%C2%B9%E2%81%84%E2%82%81%E2%82%82%E2%82%85-is-not-instead-exactly-half-of-%C2%B9%E2%81%84%E2%82%86%E2%82%80&quot;&gt;SO&lt;/a&gt; &lt;a href=&quot;#fnref:stopconspiracy&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Michal Porubcin</name></author><category term="note" /><category term="photography," /><category term="perspective," /><category term="camera" /><summary type="html">This note is based on lecture 1 of Levoy’s Digital Photography. It covers the basics of photography, including perspective, imaging properties, camera anatomy, and tradeoffs. Beyond the bare minimum, I tried to include intuition about things, and clarify common confusions. I expanded the material and rearranged the contents from the original lecture to be a bit more comprehensive. I like how Levoy starts with perspective and basic optics, before diving into all of the camera internals, so I kept that.</summary></entry><entry><title type="html">Folding Paper</title><link href="/article/2021/10/02/folding-paper.html" rel="alternate" type="text/html" title="Folding Paper" /><published>2021-10-02T00:00:00-05:00</published><updated>2021-10-02T00:00:00-05:00</updated><id>/article/2021/10/02/folding-paper</id><content type="html" xml:base="/article/2021/10/02/folding-paper.html">&lt;blockquote&gt;
  &lt;p&gt;I argue that the so-called folding paper puzzle fails to say anything about our ability to comprehend exponential growth.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;
&lt;p&gt;A common illustration of exponential growth begins with a piece of paper:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Q1: If you fold a piece of paper 50 times, how tall would it be?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;You might guess a few feet, but if you do the math, the result is pretty surprising. Assuming a sheet of paper is .1 mm thick, then \(.0000001*2^{50} = 112589990.684 \text{km}\) which is about three-quarters the distance from the earth to the sun. Wow!&lt;/p&gt;

&lt;p&gt;Is it really true that humans are incapable of thinking about exponents? Are we forever doomed to linear thinking? Here’s my thoughts as to why it’s a bit overblown.&lt;/p&gt;

&lt;h2 id=&quot;folding-bad&quot;&gt;Folding Bad&lt;/h2&gt;
&lt;p&gt;First, semantics.&lt;/p&gt;

&lt;p&gt;Assuming a US Letter size sheet, aka “printer paper,” the dimensions are standardized at 8.5 x 11 in, or 215.9 x 279.4 mm. The only reputable source I could find for thickness was this random &lt;a href=&quot;https://files.support.epson.com/docid/cpd4/cpd43251/source/printers/source/specifications/reference/scp800/spex_paper_printer_scp800.html&quot;&gt;Epson support page&lt;/a&gt;, claiming roughly .1 mm. People have experience folding a piece of paper in half, let’s say at most five times, and assuming a typical folding pattern&lt;sup id=&quot;fnref:fold&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:fold&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;, the final dimensions of the folded paper are 53.975 x 34.925 x 3.2 mm. While the thickness is noticeably larger than before, we can see the width and length are still over ten times as large as the thickness.&lt;/p&gt;

&lt;p&gt;The truth is, the paper is already difficult to fold. After a couple more folds, the paper cannot be folded without tearing at the crease. At eight folds, the thickness has already surpassed both width and height (13.49375 x 17.4625 x 25.6 mm). We started by creasing a flat object, but soon we’re slicing a paper tower on the vertical axis and stacking the halves on top of each other.&lt;sup id=&quot;fnref:decay&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:decay&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Some potential problems:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The concept of folding only works for thin objects! If we were to fold something with any thickness, then we would rarely think to fold it along its longest dimension.&lt;/li&gt;
  &lt;li&gt;It might not occur to someone that they are &lt;em&gt;doubling&lt;/em&gt; the thickness every fold, which is key to understanding the problem, even without reference to exponents.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It’s just a bad example. Here’s a better question:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Q2: What is the height of a stack of paper after doubling it 50 times?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Turns out, people will also give a hopelessly incorrect answer for stacking. What gives?&lt;/p&gt;

&lt;h2 id=&quot;linear-case&quot;&gt;Linear Case&lt;/h2&gt;
&lt;p&gt;Maybe it would help to look at a question with the same answer, but removing any exponentiation.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Q3: How high would a stack of one trillion papers be?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In general, for any exponential question where the stack is doubled \(n\) times, we can ask a linear question about a stack of \(2^n\) papers.&lt;/p&gt;

&lt;p&gt;Some of the people I asked this question gave a wide range of estimates, ranging from the height of a skyscraper or Mt. Everest – still massive underestimates. These people can imagine the thickness of a single sheet, but seem to be using their gut, instead of running any kind of calculations, to extend this knowledge to the height of a large stack.&lt;/p&gt;

&lt;p&gt;Some clever people will make much better guesses for Q3 than Q2. Let’s call a &lt;strong&gt;ballpark answer&lt;/strong&gt; one that’s the around the correct order of magnitude. The trick is, a ballpark answer doesn’t require hardly any calculation besides keeping track of zeros (powers of 10). A trillion has 12 zeros, and the thickness of paper has -1 zero in millimeters. Just add the zeros. In contrast, most people cannot exponentiate in their head without repeated multiplication, and there doesn’t seem to be a quick way to get the number of digits in the final answer (i.e. get a ballpark answer), without using the log function (which just takes us back to square one, since we lack a way to quickly calculate log).&lt;/p&gt;

&lt;p&gt;So with a little thinking, we can easily solve a linear problem, but not the exponential problem. Humans are linear monkeys confirmed?&lt;/p&gt;

&lt;h2 id=&quot;ban-arithmetic&quot;&gt;Ban Arithmetic&lt;/h2&gt;
&lt;p&gt;Not so fast! The ease with which we can mentally calculate the correct number of digits in Q3 relies on reasonably-sized inputs. Let’s say we had a different linear question where we needed to multiply a number with 8962 digits by another number with 3899 digits. Summing the zeros is no longer trivial (not with my mental math abilities anyway). It would probably still be harder to mentally exponentiate enough times to arrive at the same answer. Soon we start considering efficient exponentiation algorithms, encodings etc…&lt;/p&gt;

&lt;p&gt;All that goes against the spirit of the question. It would be a bit disappointing if the phrase “humans are not exponential, but linear, thinkers” boils down to “humans can’t exponentiate in their heads, but they can add zeros, sometimes.” I don’t think anyone can argue that exponentiation is a more complex operation than multiplication or addition. The question is supposed to reveal some structure in our intuition, not elementary observations about math, otherwise we wouldn’t be talking about the heights of paper towers at all.&lt;/p&gt;

&lt;p&gt;I took it upon myself to come up with a new version of Q3 that discourages mental arithmetic:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Q4: In TWO SECONDS tell me how high a stack of one trillion papers would be.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;inverse-case&quot;&gt;Inverse Case&lt;/h2&gt;
&lt;p&gt;Fine, now we have linear and exponential versions of the paper-stacking riddle we are about equally bad at answering. Now what?&lt;/p&gt;

&lt;p&gt;The previous questions, and especially the first two, were too focused on dumbfounding you with the final answer, warping your perfectly normal inability to perform fast, accurate mental math into some shocking moral about our poor linear minds. There’s no reason to construct the question in such a way that the final paper stack reaches three quarters of the distance from the Earth to the sun, other than to a) make absolutely certain you will get the answer incorrect so that b) whichever smart guy asks it can take pleasure in correcting you.&lt;/p&gt;

&lt;p&gt;I present a much more modest version (disclaimer 1: it’s not mine; disclaimer 2: there’s no paper in this one):&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Q5: Imagine a lake, with a lily pad population which doubles every day. Say it takes 50 days for the lily pad to cover a lake. On what day is half the lake covered?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Some might guess around 30 days. In fact, no guessing or math is necessary. The lake is half-covered on the 49th day. Duh.&lt;/p&gt;

&lt;p&gt;Q5 avoids asking us to project fifty iterations into the future, so it’s not setting us up for failure.&lt;sup id=&quot;fnref:simplest&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:simplest&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; Instead of showing us that we &lt;em&gt;don’t&lt;/em&gt; tend to think exponentially and leaving it at that, it gives us a simple, alternative perspective so we &lt;em&gt;can&lt;/em&gt;: &lt;strong&gt;most of the gains are stuffed at the end&lt;/strong&gt;! In the case of daily doubling, &lt;em&gt;half&lt;/em&gt; the value of &lt;em&gt;any&lt;/em&gt; given day was obtained within one day.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;You might say, “that’s a lot of words just to admit that the puzzle is useful after all.” Listen, I didn’t say it was useless, just overblown. It isn’t necessarilly stated that we &lt;em&gt;can’t&lt;/em&gt; think exponentially, but it’s always said with a shrug and a sigh, like some eternal truth. Even worse, we arrive at that conclusion through some paper-folding puzzle, when in reality there’s several things going on:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Exponentiation is inherently more complicated than multiplication and addition.&lt;/li&gt;
  &lt;li&gt;We can’t intuit even linear approximations if they’re literally astronomical.&lt;/li&gt;
  &lt;li&gt;We are perfectly capable of thinking exponentially. It just takes some practice.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Alright I’m done criticizing a silly puzzle. Off to more productive things.&lt;/p&gt;

&lt;h2 id=&quot;appendix&quot;&gt;Appendix&lt;/h2&gt;
&lt;p&gt;Easy Python code to test out the folds yourself:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;paper&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;215.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;279.4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# US Letter dimensions in mm
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;folds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;                 &lt;span class=&quot;c1&quot;&gt;# change to desired number of folds
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;folds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;paper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;paper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;paper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;paper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;paper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;paper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:fold&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;I assume folds parallel to the shortest of width \(w\) and height \(h\). If \(w&amp;lt;h&amp;lt;2w\) like US Letter sheets, then folds will always alternate along width and height. &lt;a href=&quot;#fnref:fold&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:decay&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;And just for fun, the width and height experience exponential decay: \(215.9*.5^{25}=.00000643\text{mm}\) and \(279.4*.5^{25}=.00000833\text{mm}\), both ending up close to the diameter of a DNA strand. &lt;a href=&quot;#fnref:decay&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:simplest&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;It’s the simplest too, and can technically be solved without even understanding exponentiation. In most cases though, people will visualize the growth of the lillies, leading them to think in some way about it. &lt;a href=&quot;#fnref:simplest&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Michal Porubcin</name></author><category term="article" /><category term="exponential-growth" /><summary type="html">I argue that the so-called folding paper puzzle fails to say anything about our ability to comprehend exponential growth.</summary></entry><entry><title type="html">VI-sensor Project: Accompanying Tutorials</title><link href="/tutorial/2021/08/07/sensor-tutorials.html" rel="alternate" type="text/html" title="VI-sensor Project: Accompanying Tutorials" /><published>2021-08-07T00:00:00-05:00</published><updated>2021-08-07T00:00:00-05:00</updated><id>/tutorial/2021/08/07/sensor-tutorials</id><content type="html" xml:base="/tutorial/2021/08/07/sensor-tutorials.html">&lt;blockquote&gt;
  &lt;p&gt;Here’s the tutorials I put together for my VI-sensor project. I wrote them for myself several years ago so they were pretty minimal. I tried to flesh them out a bit more so maybe they can still be useful to others.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#summary&quot; id=&quot;markdown-toc-summary&quot;&gt;Summary&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#prerequisites&quot; id=&quot;markdown-toc-prerequisites&quot;&gt;Prerequisites&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#directory-tree&quot; id=&quot;markdown-toc-directory-tree&quot;&gt;Directory Tree&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#imu-guide&quot; id=&quot;markdown-toc-imu-guide&quot;&gt;IMU Guide&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#generic-usb-camera-guide&quot; id=&quot;markdown-toc-generic-usb-camera-guide&quot;&gt;Generic USB Camera Guide*&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#requirements&quot; id=&quot;markdown-toc-requirements&quot;&gt;Requirements&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#steps&quot; id=&quot;markdown-toc-steps&quot;&gt;Steps&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#flir-camera-guide-flycap-sdk--ros-driver&quot; id=&quot;markdown-toc-flir-camera-guide-flycap-sdk--ros-driver&quot;&gt;FLIR Camera Guide (flycap sdk &amp;amp; ros driver)&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#background-info&quot; id=&quot;markdown-toc-background-info&quot;&gt;Background Info&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#install&quot; id=&quot;markdown-toc-install&quot;&gt;Install&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#running&quot; id=&quot;markdown-toc-running&quot;&gt;Running&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#camera-calibration-given-photo-collection&quot; id=&quot;markdown-toc-camera-calibration-given-photo-collection&quot;&gt;Camera Calibration Given Photo Collection*&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#wires&quot; id=&quot;markdown-toc-wires&quot;&gt;Wires&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#triggering-camera-from-imu&quot; id=&quot;markdown-toc-triggering-camera-from-imu&quot;&gt;Triggering Camera From IMU&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#flycap&quot; id=&quot;markdown-toc-flycap&quot;&gt;Flycap&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#imu-serial&quot; id=&quot;markdown-toc-imu-serial&quot;&gt;IMU Serial&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#imu-ros&quot; id=&quot;markdown-toc-imu-ros&quot;&gt;IMU ROS&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#camera-ros-receive-trigger&quot; id=&quot;markdown-toc-camera-ros-receive-trigger&quot;&gt;Camera ROS (Receive Trigger)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#camera-ros-publish-image&quot; id=&quot;markdown-toc-camera-ros-publish-image&quot;&gt;Camera ROS (Publish Image)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#wrap-up&quot; id=&quot;markdown-toc-wrap-up&quot;&gt;Wrap Up&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#camera-calibration-with-kalibr&quot; id=&quot;markdown-toc-camera-calibration-with-kalibr&quot;&gt;Camera Calibration with Kalibr&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#record-rosbag&quot; id=&quot;markdown-toc-record-rosbag&quot;&gt;Record rosbag&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#calibrate-cameras&quot; id=&quot;markdown-toc-calibrate-cameras&quot;&gt;Calibrate Cameras&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#calibrate-cameraimu&quot; id=&quot;markdown-toc-calibrate-cameraimu&quot;&gt;Calibrate Camera+IMU&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#using-in-maplab&quot; id=&quot;markdown-toc-using-in-maplab&quot;&gt;Using In Maplab&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#rovioli--maplab&quot; id=&quot;markdown-toc-rovioli--maplab&quot;&gt;ROVIOLI + maplab&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#background&quot; id=&quot;markdown-toc-background&quot;&gt;Background&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#build-map&quot; id=&quot;markdown-toc-build-map&quot;&gt;Build Map&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#refine-calibration&quot; id=&quot;markdown-toc-refine-calibration&quot;&gt;Refine Calibration&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#visualization&quot; id=&quot;markdown-toc-visualization&quot;&gt;Visualization&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#relocalization&quot; id=&quot;markdown-toc-relocalization&quot;&gt;Relocalization&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#point-cloud-and-mesh-visualization&quot; id=&quot;markdown-toc-point-cloud-and-mesh-visualization&quot;&gt;Point cloud and mesh visualization*&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#some-fixes-for-dynamic-reconfigure-service&quot; id=&quot;markdown-toc-some-fixes-for-dynamic-reconfigure-service&quot;&gt;Some Fixes For Dynamic Reconfigure Service&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#pose-and-image-to-unity&quot; id=&quot;markdown-toc-pose-and-image-to-unity&quot;&gt;Pose and Image to Unity&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;While making the &lt;a href=&quot;/project/2021/07/21/sensor.html&quot;&gt;VI-sensor&lt;/a&gt;, I ran through lots of different tutorials and tried lots of things, and I wrote down some of the steps so I wouldn’t get lost later. They actually saved me a couple times when I had to start over one time. Several years later, even though some things have been obsoleted, maybe these refurbished tutorials will be helpful to someone starting on similar hardware or software.&lt;/p&gt;

&lt;p&gt;As I experimented a lot, I didn’t end up using all the tutorials for the final VI-sensor. The ones that were not necessary are marked with an asterisk. The tutorials are in a rough order. Some previous tutorials may need to be completed before starting a later one.&lt;/p&gt;

&lt;h3 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Tech&lt;/strong&gt;: A computer generally speaking, specific requirements mentioned in sub tutorials below.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Skills&lt;/strong&gt;: terminal (command prompt), light C++/Python/general programming, ROS, basic knowledge of hardware and software (what is an IMU for, what is Arduino).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Though I had some exposure to all the above, I strengthened a lot of skills, especially ROS, as I went along. I would say the core prereqs are some terminal and programming familiarity.&lt;/p&gt;

&lt;h3 id=&quot;directory-tree&quot;&gt;Directory Tree&lt;/h3&gt;

&lt;p&gt;There are tons of files and workspaces all over the place. I didn’t organize things that well, but for reference here’s the hierarchy I had, with the home directory (~/) as root. Bolded files are important. It might help to check back here if I’m referring to some errant launch or config file.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Arduino/Razor_AHRS/&lt;/li&gt;
  &lt;li&gt;catkin_ws/src/
    &lt;ul&gt;
      &lt;li&gt;pointgrey_camera_driver/pointgrey_camera_driver/
        &lt;ul&gt;
          &lt;li&gt;launch/&lt;strong&gt;camera.launch&lt;/strong&gt;&lt;/li&gt;
          &lt;li&gt;src/&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;razor_imu_9dof/
        &lt;ul&gt;
          &lt;li&gt;config/&lt;strong&gt;my_razor.yaml&lt;/strong&gt;&lt;/li&gt;
          &lt;li&gt;launch/&lt;strong&gt;razor-pub.launch&lt;/strong&gt;&lt;/li&gt;
          &lt;li&gt;nodes/&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;kalibr_ws/&lt;/li&gt;
  &lt;li&gt;maplab_ws/src/maplab/applications/rovioli/
    &lt;ul&gt;
      &lt;li&gt;scripts/&lt;strong&gt;my_run_rovioli&lt;/strong&gt;&lt;/li&gt;
      &lt;li&gt;share/
        &lt;ul&gt;
          &lt;li&gt;&lt;strong&gt;my_imu-sigmas-rovio.yaml&lt;/strong&gt;&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;my_imu-sparkfun.yaml&lt;/strong&gt;&lt;/li&gt;
          &lt;li&gt;&lt;strong&gt;my_ncamera.yaml&lt;/strong&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;imu-guide&quot;&gt;IMU Guide&lt;/h2&gt;
&lt;p&gt;I used the &lt;a href=&quot;https://learn.sparkfun.com/tutorials/9dof-razor-imu-m0-hookup-guide/all&quot;&gt;9dof Razor IMU&lt;/a&gt; from Sparkfun, which has been discontinued :( Specific version is M0 14001. Mostly follow the guide at the &lt;a href=&quot;http://wiki.ros.org/razor_imu_9dof#Software_Installation&quot;&gt;ROS wiki&lt;/a&gt;, but with modifications which I’ll list below:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Before everything:
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ sudo adduser &amp;lt;username&amp;gt; dialout&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;logout, log back in&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;On step 3, no breakout is required for this version of the board. Just a USB cable.&lt;/li&gt;
  &lt;li&gt;For step 4, I used Used Arduino 1.8.6 (latest available at the time)&lt;/li&gt;
  &lt;li&gt;I think I did step 4.1.1 instead of 4.1.2. I had to build from source.&lt;/li&gt;
  &lt;li&gt;On step 4.2, follow above tutorial until opening Razor_AHRS in Arduino. Go to the &lt;a href=&quot;sfe.io/t567&quot;&gt;Sparkfun tutorial&lt;/a&gt; and follow from “Installing the 9DoF Razor Arduino Core” until the end of “Select the Board and Serial Port.” Also, using the link at the beginning of the following section, download Sparkfun MPU-9250 DMP Library and install it (add it as Arduino library). Finally, upload code to the board. (Note: do NOT &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;include&lt;/code&gt; the library in the code. Adding the library into Arduino IDE is enough.). Return to ROS wiki tutorial (skip the rest of 4.2).&lt;/li&gt;
  &lt;li&gt;Edit the USER SETUP AREA in Razor_AHRS.ino, in the ~/Arduino directory! (NOT ~/catkin_ws)&lt;/li&gt;
  &lt;li&gt;On step 4.3: Create config file (in ~/catkin_ws), rename the port in my_razor.yaml to whatever the port is called in Arduino IDE (eg /dev/ttyACM0)&lt;/li&gt;
  &lt;li&gt;Step 6.2 will run a visualization demo. Prior to running the given command, run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ source ~/catkin_ws/devel/setup.bash&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;On step 7.1.2, don’t adjust the gyro (all ~0)&lt;/li&gt;
  &lt;li&gt;I only did hard iron correction in 7.1.3, using the Arduino serial monitor (then in my_razor.yaml, setting calibration_magn_use_extended parameter to false). Add changes to Razor_AHRS. These values must also be placed in the my_razor.yaml file in the catkin workspace and sourced. The razor-launch-and-display.launch (and possibly other launch files in the launch directory) all take my_razor.yaml as a parameter, overriding the values we wrote in Arduino and flashed onto the IMU board (yes, dumb).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That should be the end of the ros wiki tutorial. The polling rate is probably still low, so go back in Razor_AHRS.ino, and change &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;OUTPUT__DATA_INTERVAL&lt;/code&gt; to 10 (to increase the rate to 100 Hz), and change the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;read_sensors&lt;/code&gt; function as follows:&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;uint32_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lastDisplayMs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;read_sensors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
&lt;span class=&quot;cp&quot;&gt;#if HW__VERSION_CODE == 14001
&lt;/span&gt;    &lt;span class=&quot;kt&quot;&gt;uint32_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;currMs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;millis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;currMs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lastDisplayMs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;lastDisplayMs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;currMs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loop_imu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;//...rest of function omitted&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Reflash, and now if you launch razor-pub.launch, then in another terminal window run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ rostopic hz imu&lt;/code&gt;, you will see average rate: 100 Hz!&lt;/p&gt;

&lt;p&gt;Extra notes:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Edit+flash the Razor_AHRS.ino file from the Arduino directory, NOT the Razor_AHRS.ino file in the catkin workspace! (Note from 2021: I’m not sure why this is necessary…) However, the my_razor.yaml is configured in the catkin workspace and sourced.&lt;/li&gt;
  &lt;li&gt;The “Using the MPU-9250 DMP Arduino Library” section of the &lt;a href=&quot;sfe.io/t567&quot;&gt;tutorial&lt;/a&gt; is useful for understanding some of the Arduino code, particularly Sensors.ino&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Here’s a sketch of the important sections of the razor_9dof code:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;main()
    read_sensors()
        Sensors.loop_imu() //get data
        hardware trigger //every 33ms
    else if (output_mode == OUTPUT_MODE_ANGLES_AG_SENSORS)
        Sensors.output_both_angles_and_sensors_text() //Logging to serial
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;generic-usb-camera-guide&quot;&gt;Generic USB Camera Guide*&lt;/h2&gt;
&lt;p&gt;The first camera I tried was a GoPro camera. I thought it would at least be a step up from a webcam, but I was wrong. My VI-sensor was just the IMU taped on top of the GoPro…it did not work for several reasons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;not global shutter&lt;/li&gt;
  &lt;li&gt;resolution was too large making the algorithm run too slowly&lt;/li&gt;
  &lt;li&gt;IMU would wobble around making calibration useless&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But I’ll leave this tutorial here anyways.&lt;/p&gt;

&lt;h3 id=&quot;requirements&quot;&gt;Requirements&lt;/h3&gt;
&lt;p&gt;The GoPro requires extra hocus pocus like an external capture card. It’s possible to just use a webcam.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;GoPro 2018
    &lt;ul&gt;
      &lt;li&gt;double ended type-A USB 3.0 cable&lt;/li&gt;
      &lt;li&gt;HDMI to micro HDMI cable&lt;/li&gt;
      &lt;li&gt;HDMI to USB converter/capture card (high end / low latency) (USB 3.0 allows for 1080p 60fps I think?)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Or just a webcam&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;steps&quot;&gt;Steps&lt;/h3&gt;
&lt;p&gt;Turn on the GoPro. Set to 1080p and 30fps. Plug in the hdmi cable &amp;lt;-&amp;gt; capture card &amp;lt;-&amp;gt; usb cable &amp;lt;-&amp;gt; laptop usb port. The gopro screen should go black.&lt;/p&gt;

&lt;p&gt;Run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ sudo apt-get install ros-kinetic-usb-cam&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Create folder(s) called usb_camera/launch in your catkin workspace (just to have a place to put the usb_cam-related launch files). Create a launch file with the usbcam and optionally an image display to view the live video on the computer. The value for the video_device param can be found with the following terminal commands:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt-get install v4l-utils
$ v4l2-ctl --list-devices
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note: v4l-utils may already be installed&lt;/p&gt;

&lt;p&gt;Note: The GoPro may automatically shut off (another reason why a GoPro was a bad idea) so turn it back on as necessary.&lt;/p&gt;

&lt;p&gt;Now run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ roslaunch &amp;lt;/path/to/usb_launchfile&amp;gt;&lt;/code&gt;. Boom done.&lt;/p&gt;

&lt;p&gt;Notes:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;USB C cable must be disconnected from GoPro&lt;/li&gt;
  &lt;li&gt;If image is glitched somehow, more often than not it’s one of the wired connections. Usually just jiggle the cables a bit.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ rostopic hz &amp;lt;cam topic&amp;gt;&lt;/code&gt; in a separate terminal and notice that the fps is low. This is likely due to autoexposure. I tried several settings:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Autoexposure on (default)&lt;/li&gt;
  &lt;li&gt;Turn off autoexposure on GoPro&lt;/li&gt;
  &lt;li&gt;Turn off autoexposure on GoPro and in launch file&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In terms of fps: 3 &amp;gt; 2 &amp;gt; 1. To achieve (2) or (3), set add new autoexposure param and set to false in launch file. Also use image_raw not image_compressed.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;flir-camera-guide-flycap-sdk--ros-driver&quot;&gt;FLIR Camera Guide (flycap sdk &amp;amp; ros driver)&lt;/h2&gt;
&lt;p&gt;I used a Firefly MV monochrome global shutter USB2.0 camera from FLIR (Point Grey). I’m pretty sure it’s been discontinued and it’s probably for the best. I think the closest thing FLIR offers now is the &lt;a href=&quot;https://www.flir.com/products/firefly-s/&quot;&gt;FireFly S&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This guide uses FlyCapture2 sdk for FLIR Firewire and USB2 cams! For USB3, GigE, and 10GigE cams, use Spinnaker sdk and the &lt;a href=&quot;https://github.com/ros-drivers/flir_camera_driver&quot;&gt;ros driver&lt;/a&gt; instead. I will refer to this URL as FDP (flir downloads page): www.ptgrey.com/support/downloads/&lt;/p&gt;

&lt;p&gt;I used Firefly MV monochrome, amd64, Ubuntu 16 (FLIR does not offer flycap sdk for Ubuntu 14 and below)&lt;/p&gt;

&lt;h3 id=&quot;background-info&quot;&gt;Background Info&lt;/h3&gt;
&lt;p&gt;Go to the &lt;a href=&quot;https://github.com/ros-drivers/pointgrey_camera_driver&quot;&gt;point grey camera ros drivers&lt;/a&gt;
Navigate to pointgrey_camera_driver/cmake/download_flycap&lt;/p&gt;

&lt;p&gt;Find something like below:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;'x86_64'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'current'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'https://www.ptgrey.com/support/downloads/10767/'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;'flycapture2-2.11.3.121-amd64/libflycapture-2.11.3.121_amd64.deb'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;'flycapture2-2.11.3.121-amd64/libflycapture-2.11.3.121_amd64-dev.deb'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'usr/lib/libflycapture.so.2.11.3.121'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As you can see, the flycap sdk version used is flycapture2-2.11.3.121. However, in this very version, FFMV/FMVU cameras cannot start!! (according to release notes; available on FDP) This bug is fixed in 2.11.3-164 and above.&lt;/p&gt;

&lt;blockquote class=&quot;callout&quot;&gt;
  &lt;div class=&quot;flexy&quot;&gt;
    
      &lt;div class=&quot;callout-icon&quot;&gt;😐&lt;/div&gt;
    
    &lt;div class=&quot;callout-text&quot;&gt;Looking at the repo in 2021, it's possible they fixed this with commit e27f3a0.&lt;/div&gt;
  &lt;/div&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;install&quot;&gt;Install&lt;/h3&gt;

&lt;p&gt;Go to FDP. Download latest sdk: 2.13.3-31 for me. Follow instructions in README.txt to install.&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ cd ~/catkin_ws/src
$ git clone https://github.com/ros-drivers/pointgrey_camera_driver
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Open pointgrey_camera_driver/cmake/download_flycap and find the same block as above. Edit like so:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;'x86_64'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'current'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'https://www.ptgrey.com/support/downloads/AAAAA/'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;'flycapture2-BBBBB-amd64/libflycapture-BBBBB_amd64.deb'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;'flycapture2-BBBBB-amd64/libflycapture-BBBBB_amd64-dev.deb'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'usr/lib/libflycapture.so.BBBBB'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Where AAAAA is the link to the file on the FDP (go to FDP hover over sdk link, right click, copy link address), and BBBBB is version number. So mine looks like this:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;s&quot;&gt;'x86_64'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'current'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'https://www.ptgrey.com/support/downloads/11176/'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;'flycapture2-2.13.3.31-amd64/libflycapture-2.13.3.31_amd64.deb'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;s&quot;&gt;'flycapture2-2.13.3.31-amd64/libflycapture-2.13.3.31_amd64-dev.deb'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;'usr/lib/libflycapture.so.2.13.3.31'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Run:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ cd ~/catkin_ws
$ catkin_make
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Due to the version bs, you cannot just &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ sudo apt-get install ros-kinetic-ptgrey-camera-drivers&lt;/code&gt;; you must build from (modified) source.&lt;/p&gt;

&lt;p&gt;If you mess up, run the uninstall script located in the sdk folder downloaded from FDP, and remove the ros package (depending whether you built from source or downloaded binary: either remove cloned repo and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;catkin clean&lt;/code&gt;, OR &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ sudo apt-get remove ros-kinetic-blabla&lt;/code&gt; and maybe deal with orphaned packages).&lt;/p&gt;

&lt;h3 id=&quot;running&quot;&gt;Running&lt;/h3&gt;

&lt;p&gt;Tada! Now plug in the camera and type:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ flycap&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Select your camera and press ok to view a video feed! Press the gear icon to adjust imaging settings. Changing settings in ROS instead of Flycap can be done with dynamic reconfigure; see tutorial for that further down. Some settings in dynamic reconfigure, however, seem to be bugged (afaik)! See the comment at the end of the dynamic reconfigure tutorial.&lt;/p&gt;

&lt;p&gt;BIG NOTE: If you are already publishing on camera/image_raw, then you will not see a video feed if you open flycap as well. However, changing the settings from flycap will still take effect.&lt;/p&gt;

&lt;p&gt;Note: To view with img_view, use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ rosrun image_view image_view image:=camera/image_raw&lt;/code&gt;. I recall this not working before but now it does and I’m accepting it.&lt;/p&gt;

&lt;p&gt;Note: I cannot get this to work with kalibr validator.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;camera-calibration-given-photo-collection&quot;&gt;Camera Calibration Given Photo Collection*&lt;/h2&gt;
&lt;p&gt;Before finding kalibr I think I used this method to calibrate my camera.&lt;/p&gt;

&lt;p&gt;Mostly follow &lt;a href=&quot;https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_calib3d/py_calibration/py_calibration.html#calibration&quot;&gt;this guide&lt;/a&gt;&lt;/p&gt;

&lt;blockquote class=&quot;callout&quot;&gt;
  &lt;div class=&quot;flexy&quot;&gt;
    
      &lt;div class=&quot;callout-icon&quot;&gt;😐&lt;/div&gt;
    
    &lt;div class=&quot;callout-text&quot;&gt;That link is dead. This should be the same guide: https://docs.opencv.org/4.5.2/dc/dbb/tutorial_py_calibration.html&lt;/div&gt;
  &lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;Print a 9x6 checkerboard pattern on any size paper (Letter/A4/etc works). Paste/tape paper flatly onto a flat surface. (emphasis on flat)&lt;/p&gt;

&lt;p&gt;Here’s the code to get calibration params.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cv2&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;glob&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# import pdb; pdb.set_trace()
&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#Protip: python reprojection_error.py &amp;gt; error.txt
#Explanation: https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_calib3d/py_calibration/py_calibration.html#calibration
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;PATH&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;/Users/user/Documents/Prog/temp/&quot;&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;#For displaying large images
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;namedWindow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;output&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;WINDOW_NORMAL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# termination criteria
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;criteria&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TERM_CRITERIA_EPS&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TERM_CRITERIA_MAX_ITER&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.001&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;objp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;objp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mgrid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# Arrays to store object points and image points from all the images.
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;objpoints&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 3d point in real world space
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imgpoints&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 2d points in image plane.
&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;images&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;glob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;glob&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'*.JPG'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Reading %d images from %s&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;PATH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fname&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PATH&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fname&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;gray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cvtColor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;COLOR_BGR2GRAY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# Find the chess board corners
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;corners&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;findChessboardCorners&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# If found, add object points, image points (after refining them)
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;objpoints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;objp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;corners2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cornerSubPix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corners&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;criteria&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;imgpoints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;corners2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# Draw and display the corners
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drawChessboardCorners&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;corners2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;imS&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;720&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;480&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;img&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;imS&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;waitKey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;destroyAllWindows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mtx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rvecs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tvecs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;calibrateCamera&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;objpoints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;imgpoints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[::&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Projection (Intrinsic)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mtx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Distortion (Extrinsic)&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PATH&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;GOPR0003.JPG&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;newcameramtx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;roi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getOptimalNewCameraMatrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mtx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# undistort
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dst&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;undistort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mtx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;newcameramtx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# crop the image
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;roi&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dst&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imwrite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PATH&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'calibresult.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;tot_error&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;objpoints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;imgpoints2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;projectPoints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;objpoints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rvecs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tvecs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mtx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;norm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imgpoints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imgpoints2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;NORM_L2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imgpoints2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tot_error&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;mean reprojection error: &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tot_error&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;objpoints&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Changes from the code in the tutorial:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Code assumes (9,6) pattern (from link above).&lt;/li&gt;
  &lt;li&gt;Prints relevant matrices and error&lt;/li&gt;
  &lt;li&gt;Fixes reprojection error calculation&lt;/li&gt;
  &lt;li&gt;Better image display for very large images (exceeding screen size)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It should output a calibresult.jpg, which is one undistorted image (the image name must be specified in code).&lt;/p&gt;

&lt;p&gt;Add calibration results to my_config.yaml.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;wires&quot;&gt;Wires&lt;/h2&gt;

&lt;p&gt;In order to set up the triggering, we need to stably mount the camera and IMU and connect them. I 3D-printed a holder which I included in the middle of the &lt;a href=&quot;/project/2021/07/21/sensor.html&quot;&gt;project summary&lt;/a&gt;. The design is weird because I had other plans for it and also because, well, that’s the best I could come up with. The correct wiring can be found on appropriate data sheets.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;triggering-camera-from-imu&quot;&gt;Triggering Camera From IMU&lt;/h2&gt;

&lt;p&gt;The most important tutorial! Fun fact I couldn’t find this one so I looked back at the old code (which I had luckily) and whipped up a 2021 version just for this post. There may be errors or omissions though.&lt;/p&gt;

&lt;p&gt;I basically worked off another tutorial which I’ll call &lt;a href=&quot;https://grauonline.de/wordpress/?page_id=1951&quot;&gt;grau&lt;/a&gt;. There are two important differences. First, I swapped the bluefox2 library with the pointgrey library, just because I used a different camera. My IMU was different as well, incorporating Arduino onboard, so I didn’t need mpu6050_serial_to_imu. Those two differences require me to explain the steps separately from grau, unlike the IMU tutorial, but I still recommend his tutorial for background information.&lt;/p&gt;

&lt;p&gt;Here are the main steps:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Before everything, use Flycap to set parameters on camera&lt;/li&gt;
  &lt;li&gt;Output trigger data from IMU to serial&lt;/li&gt;
  &lt;li&gt;Publish IMU and trigger data over ROS&lt;/li&gt;
  &lt;li&gt;Receive trigger messages with pointgrey camera driver&lt;/li&gt;
  &lt;li&gt;Send images over ROS&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;ROS is the bridge between the IMU, camera, and the PC. Trigger data is only used to sync the IMU and camera. At the end, the PC should see synced IMU and camera messages over ROS, which will be fed into the SLAM algorithm.&lt;/p&gt;

&lt;h3 id=&quot;flycap&quot;&gt;Flycap&lt;/h3&gt;

&lt;p&gt;Follow the Flycap tutorial above. In the Flycap GUI, there should be settings for enabling trigger and disabling autoexposure.&lt;/p&gt;

&lt;h3 id=&quot;imu-serial&quot;&gt;IMU Serial&lt;/h3&gt;

&lt;p&gt;Assuming the camera and imu are wired together, first revisit Arduino/Razor_AHRS/Razor_AHRS.ino from the IMU tutorial above, and include camera triggering every 33ms. Note that the camera has a minimum triggering frequency. See the data sheet for details.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lastDisplayMs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;triggerMs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;triggerCounter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;read_sensors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
&lt;span class=&quot;cp&quot;&gt;#if HW__VERSION_CODE == 14001
&lt;/span&gt;  &lt;span class=&quot;kt&quot;&gt;unsigned&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;long&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;currMs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;millis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;currMs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lastDisplayMs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;lastDisplayMs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;currMs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loop_imu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;//raise 33 for lower fps&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;currMs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;triggerMs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;33&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;digitalWrite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LOW&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;digitalWrite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HIGH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;triggerCounter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;triggerMs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;currMs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;cp&quot;&gt;#else
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;Read_Gyro&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// Read gyroscope&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;Read_Accel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// Read accelerometer&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;Read_Magn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// Read magnetometer&lt;/span&gt;
&lt;span class=&quot;cp&quot;&gt;#endif // HW__VERSION_CODE
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Add the following to Arduino/Razor_AHRS/Output.ino &lt;em&gt;at the end&lt;/em&gt; of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;output_both_angles_and_sensors_text()&lt;/code&gt;, in order to log necessary trigger data.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;LOG_PORT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;triggerCounter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LOG_PORT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;,&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;LOG_PORT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;triggerMs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LOG_PORT&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;println&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Remember if you change the Arduino code you have to reflash the IMU.&lt;/p&gt;

&lt;h3 id=&quot;imu-ros&quot;&gt;IMU ROS&lt;/h3&gt;

&lt;p&gt;In razor_imu_9dof/nodes/imu_node.py, publish &lt;a href=&quot;http://docs.ros.org/en/melodic/api/sensor_msgs/html/msg/TimeReference.html&quot;&gt;TimeReference&lt;/a&gt; messages. This replaces the mpu6050_serial_to_imu from grau.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;/// after init_node()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pub_trigger&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rospy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Publisher&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imu&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trigger_time&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TimeReference&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;queue_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;imuMsg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Imu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;triggerMsg&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TimeReference&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;...&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;/// in the while loop&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;triggerCounter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;kt&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;triggerCounter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lastTriggerCounter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;triggerMsg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;triggerCounter&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;triggerMsg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stamp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rospy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kt&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;words&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;triggerMsg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time_ref&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rospy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Time&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pub_trigger&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;publish&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;triggerMsg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;lastTriggerCounter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;triggerCounter&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The serial output is read line by line with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;line = ser.readline()&lt;/code&gt;, and split by comma into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;words&lt;/code&gt;. &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;words[9]&lt;/code&gt;, is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;triggerCounter&lt;/code&gt; and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;words[10]&lt;/code&gt;, is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;triggerMs&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;camera-ros-receive-trigger&quot;&gt;Camera ROS (Receive Trigger)&lt;/h3&gt;

&lt;p&gt;Add a ring buffer to pointgrey_camera_driver/pointgrey_camera_driver/src/nodelet.cpp. The idea is if the write head is running laps around the read head then it’s out of sync.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;callback&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sensor_msgs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TimeReference&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ConstPtr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;time_ref&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;//if ( (time_ref-&amp;gt;header.seq &amp;amp; 63) == 0){&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;//  ROS_WARN(&quot;recv triggertime seq %10u&quot;, time_ref-&amp;gt;header.seq);&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;//}&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;//  ros::Duration(0.001).sleep();&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;TriggerPacket_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pkt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;pkt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;triggerTime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time_ref&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;pkt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;triggerCounter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;time_ref&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;fifoWrite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pkt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;void&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fifoWrite&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TriggerPacket_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pkt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;fifo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fifoWritePos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pkt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;fifoWritePos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fifoWritePos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FIFO_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fifoWritePos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fifoReadPos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ROS_WARN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;FIFO overflow!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fifoRead&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TriggerPacket_t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pkt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fifoReadPos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fifoWritePos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;pkt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fifo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fifoReadPos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;fifoReadPos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fifoReadPos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FIFO_SIZE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;kt&quot;&gt;bool&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fifoLook&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TriggerPacket_t&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pkt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fifoReadPos&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fifoWritePos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;pkt&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fifo&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fifoReadPos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;camera-ros-publish-image&quot;&gt;Camera ROS (Publish Image)&lt;/h3&gt;

&lt;p&gt;Move down to the try block of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;STARTED&lt;/code&gt; case and create a new case to handle the camera triggering mode. Now the camera will only take and publish frames when it receives the trigger signal. Code again heavily from grau.&lt;/p&gt;

&lt;div class=&quot;language-cpp highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;/// this case already existed -- no triggering&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;triggerMode&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// Publish the full message&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;pub_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;publish&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wfov_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;// Publish the message using standard image transport&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;it_pub_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getNumSubscribers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sensor_msgs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ImagePtr&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sensor_msgs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wfov_image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;it_pub_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;publish&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ci_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;/// new case for triggering&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// wait for new trigger packet to receive&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;TriggerPacket_t&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pkt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fifoLook&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pkt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// ros::Duration(0.001).sleep();&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;c1&quot;&gt;// a new video frame was captured - check if we need to skip it if one trigger packet was lost&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pkt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;triggerCounter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nextTriggerCounter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fifoRead&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pkt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;// uint shutter = pg_.getShutter();&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ros&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Duration&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;expose_duration&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ros&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Duration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SHUTTER&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1e-6&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;wfov_image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stamp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pkt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;triggerTime&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;expose_duration&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ci_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stamp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;wfov_image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stamp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;wfov_image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;info&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ci_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// Publish the full message&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;pub_&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;publish&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wfov_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;// Publish the message using standard image transport&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;it_pub_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;getNumSubscribers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;sensor_msgs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ImagePtr&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sensor_msgs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;::&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;wfov_image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;it_pub_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;publish&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ci_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;//ros::Duration(0.001).sleep();&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;outOfSyncCounter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;      
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outOfSyncCounter&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;){&lt;/span&gt;
      &lt;span class=&quot;c1&quot;&gt;//ROS_WARN(&quot;trigger not in sync (seq expected %10u, got %10u)!&quot;, nextTriggerCounter, pkt.triggerCounter);  &lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;NODELET_WARN&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;trigger not in sync (%d)!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outOfSyncCounter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;   
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;nextTriggerCounter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// ros::spin();&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;wrap-up&quot;&gt;Wrap Up&lt;/h3&gt;

&lt;p&gt;To run, launch the camera and IMU in separate terminals. There may have been a need to start one before the other but I forgot.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;camera-calibration-with-kalibr&quot;&gt;Camera Calibration with Kalibr&lt;/h2&gt;
&lt;p&gt;Kalibr can calibrate cameras, as well as imu-camera interaction. I can’t remember for sure if it requires imu-camera synchronization…&lt;/p&gt;

&lt;p&gt;Build from source! The other option has limited capabilities.&lt;/p&gt;

&lt;p&gt;Some commands (e.g. kalibr_camera_validator) fail (infuriatingly) on Ubuntu 16. They must be run on Ubuntu 14. This is an obstacle for validating with FLIR firefly camera.&lt;/p&gt;

&lt;p&gt;Follow the youtube tutorial on the &lt;a href=&quot;https://github.com/ethz-asl/kalibr/wiki&quot;&gt;wiki&lt;/a&gt;. You have to calibrate imu parameters outside of Kalibr before doing imu-camera calibration.&lt;/p&gt;

&lt;h3 id=&quot;record-rosbag&quot;&gt;Record rosbag&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Plug in imu and camera&lt;/li&gt;
  &lt;li&gt;Source the kalibr workspace&lt;/li&gt;
  &lt;li&gt;Run razor_9dof_imu/launch/razor-pub.launch and pointgrey_camera_driver/launch/camera.launch in separate windows&lt;/li&gt;
  &lt;li&gt;In another window, run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ rosbag record -O output.bag camera/image_raw imu&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;Follow instructions in video to correctly move cam+imu for good calibration. Run for ~60 seconds.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To review the rosbag, close the imu and image streams, and run the 2 lines in different terminals:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ roscore
$ rosbag play output.bag
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The video can then be viewed with img_view.&lt;/p&gt;

&lt;h3 id=&quot;calibrate-cameras&quot;&gt;Calibrate Cameras&lt;/h3&gt;

&lt;p&gt;First, calibrate camera(s):
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ kalibr_calibrate_cameras --models pinhole-radtan --topics camera/image_raw --bag output.bag --target april_6x6_80cm.yaml&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;For more info on the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--models&lt;/code&gt; flag see &lt;a href=&quot;https://github.com/ethz-asl/kalibr/wiki/supported-models&quot;&gt;supported models&lt;/a&gt;. The format is the camera model abbreviated name and distortion model abbreviated name separated by a dash (e.g. pinhole-radtan). Topic must NOT have preceding slash for whatever reason (e.g. imu not /imu). Target is recommended to be some kind of april grid, since it’s more robust.&lt;/p&gt;

&lt;h3 id=&quot;calibrate-cameraimu&quot;&gt;Calibrate Camera+IMU&lt;/h3&gt;

&lt;p&gt;Then get camera+imu calibration:
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ kalibr_calibrate_imu_camera --cam camchain-output.yaml --imu imu.yaml --bag output.bag --target april_6x6_80cm.yaml&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Cam flag is output of kalibr_calibrate_cameras. Imu must be calibrated externally and imu.yaml must be filled in appropriately.&lt;/p&gt;

&lt;h3 id=&quot;using-in-maplab&quot;&gt;Using In Maplab&lt;/h3&gt;

&lt;p&gt;To convert kalibr yaml to maplab yaml (ncamera_calibration file), use the following command:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ kalibr_maplab_config --to-ncamera \
--label cam_name \
--cam camchain-imucam-output.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Label flag is arbitrary. Cam flag is the result of cam-imu calibration (output of kalibr_calibrate_imu_camera command).&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;rovioli--maplab&quot;&gt;ROVIOLI + maplab&lt;/h2&gt;

&lt;h3 id=&quot;background&quot;&gt;Background&lt;/h3&gt;
&lt;p&gt;I’m using the maplab framework. ROVIO exists as a separate repository if desired. See &lt;a href=&quot;https://github.com/ethz-asl/maplab&quot;&gt;maplab github&lt;/a&gt;, &lt;a href=&quot;https://github.com/ethz-asl/maplab/wiki&quot;&gt;maplab wiki&lt;/a&gt;, and &lt;a href=&quot;https://github.com/ethz-asl/maplab/wiki/Installation-Ubuntu&quot;&gt;install instructions&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Maplab is a VI mapping framework, which supports operations such as map merging, visual-inertial batch optimization, and loop closure. ROVIOLI, which is based on an estimator called ROVIO, is a VI mapping front end for maplab, with added maplab modules for map building and localization. maplab can just be used with ROVIOLI as a ready-to-go (and quite good) VI mapping/localization system, but other estimators can be integrated as well.&lt;/p&gt;

&lt;h3 id=&quot;build-map&quot;&gt;Build Map&lt;/h3&gt;

&lt;p&gt;You need three calibration files (see &lt;a href=&quot;https://github.com/ethz-asl/maplab/wiki/Sensor-Calibration-Format&quot;&gt;wiki&lt;/a&gt;). Put calibration files into maplab/applications/rovioli/share.&lt;/p&gt;

&lt;p&gt;Be sure to convert kalibr output to maplab cam format (see &lt;a href=&quot;https://github.com/ethz-asl/maplab/wiki/Initial-sensor-calibration-with-Kalibr&quot;&gt;wiki&lt;/a&gt;, or the last note in the previous section).&lt;/p&gt;

&lt;p&gt;Follow &lt;a href=&quot;https://github.com/ethz-asl/maplab/wiki/Running-ROVIOLI-in-VIO-mode&quot;&gt;this tutorial&lt;/a&gt; to build a map from a rosbag or rostopic. Instead of a launch file we use a bash script to run everything. Some examples are in &lt;a href=&quot;https://github.com/ethz-asl/maplab/tree/master/applications/rovioli/scripts&quot;&gt;maplab/applications/rovioli/scripts&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;refine-calibration&quot;&gt;Refine Calibration&lt;/h3&gt;

&lt;p&gt;You can optionally refine the calibration, only after obtaining a decent map (see &lt;a href=&quot;https://github.com/ethz-asl/maplab/wiki/sensor-calibration-refinement&quot;&gt;wiki&lt;/a&gt;).&lt;/p&gt;

&lt;h3 id=&quot;visualization&quot;&gt;Visualization&lt;/h3&gt;

&lt;p&gt;To visualize with rviz, download &lt;a href=&quot;https://github.com/ethz-asl/maplab/blob/pre_release_public/july-2018/applications/rovioli/share/rviz-rovioli.rviz&quot;&gt;this rviz config&lt;/a&gt;. Then run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ rosrun rviz rviz -d rviz-rovioli.rviz&lt;/code&gt;. Of course, you will need to set all the appropriate visualization flags in the rovioli launch script. Examples of these can be found in the same link above, for building a map from rosbag/rostopic. An overview of viz rostopics can be found &lt;a href=&quot;https://github.com/ethz-asl/maplab/wiki/Map-visualization&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In standalone ROVIO, some core parameters could be set at compile time with flags, but in ROVIOLI, these ROVIO parameters unfortunately require editing the code directly.&lt;/p&gt;

&lt;h3 id=&quot;relocalization&quot;&gt;Relocalization&lt;/h3&gt;

&lt;p&gt;Something to note about the map built by ROVIOLI. In ROVIO, the mapping part of SLAM is achieved with photometric features (visualized as green rectangles), whose count must be carefully managed. While these features are robust for local state estimation, they are not efficient for building maps suitable for loop closure, state estimation, etc., so in ROVIOLI, a separate module is run in parallel to grab landmarks better suited for these tasks. These are finalized at shutdown, meaning mapping is not done in real time, and online localization from this map is impossible.&lt;/p&gt;

&lt;p&gt;The map will be saved to the location specified with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--save_map_folder&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;This map should be optimized for use in relocalization. There are two ways to do this:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;launch script flag: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--optimize_map_to_localization_map&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/ethz-asl/maplab/wiki/Preparing-a-single-session-map&quot;&gt;maplab commands&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A smaller “summary map” can be generated from this, using this command: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ generate_summary_map_and_save_to_disk --summary_map_save_path path/to/save/localization/summary/map&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;The next time you run ROVIOLI with relocalization, use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--vio_localization_map_folder&lt;/code&gt; flag.&lt;/p&gt;

&lt;p&gt;Basic map manipulation can be done in the &lt;a href=&quot;https://github.com/ethz-asl/maplab/wiki/Console-map-management&quot;&gt;maplab console&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;point-cloud-and-mesh-visualization&quot;&gt;Point cloud and mesh visualization*&lt;/h2&gt;

&lt;p&gt;no rviz, thank you&lt;/p&gt;

&lt;p&gt;pcd viz:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt install pcl-tools
$ pcl_viewer &amp;lt;pointcloud.pcd&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;mesh viz:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ sudo apt-get install meshlab
$ meshlab
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;open obj file in meshlab gui&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;some-fixes-for-dynamic-reconfigure-service&quot;&gt;Some Fixes For Dynamic Reconfigure Service&lt;/h2&gt;
&lt;p&gt;Assuming everything is setup in the code, run the node, then run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$ rosrun rqt_reconfigure rqt_reconfigure&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;Specific to the pointgrey_camera_driver, the default params are set in pointgrey_camera_driver/cfg/PointGrey.cfg&lt;/p&gt;

&lt;p&gt;Some parameters in the config seem to be bugged (afaik). So far I have found this true for frame rate and brightness. So go in PointGreyCamera.cpp, into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;setNewConfiguration()&lt;/code&gt;, and comment out the relevant calls to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;setProperty()&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;To update the config (and of course, if you make changes to the driver files, e.g. PointGreyCamera.cpp), you must recompile them, i.e. rerun &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;catkin_make&lt;/code&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;pose-and-image-to-unity&quot;&gt;Pose and Image to Unity&lt;/h2&gt;

&lt;p&gt;I’m sorry to disappoint, but I can’t seem to find the tutorial or the code for this part. All I have are the last couple of entries in my dev notes.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;–/–/– Decided ros sharp was overkill because all of the URDF stuff. Opted for ROSBridgeLib, which is just client implementation for Unity of all (most) ros messsages. It sucks and no working tutorial provided, and no active development, and simple data values are behind accessors, and everything has to be static?!?!?! Switch back to ros sharp, but have “Duplicate Assembly” errors, even after restarting in new project. Discovered I was dumb and copied the wrong folder. Unity client successfully connects to rosbridgeserver, but images are not updated in Unity.&lt;/p&gt;

  &lt;p&gt;–/–/– Discovered first 12 pixels in image data were bogus; turns out it is embedded info from camera. So no problem with ros side. On Unity side, texture and pose weren’t being updated because turns out can’t do stuff with textures outside render thread (ros sharp creates new thread). So moved rendering to IEnumerator and used UnityMainThreadDispatcher. Also, using LoadImage() on compressed image rostopic is muuuuch slower than LoadRawTextureData() on regular image topic. Result is image stream shows! Some opacity issue. Deep profile helps a ton. Position works! Rotation works! Turns out need to create new Position and Rotation and assign, rather than editing the values directly (?). At least for pose, could maybe get away with less frequent position updates + smoothing/lerping.&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name>Michal Porubcin</name></author><category term="tutorial" /><category term="vio" /><category term="slam" /><category term="sensor" /><summary type="html">Here’s the tutorials I put together for my VI-sensor project. I wrote them for myself several years ago so they were pretty minimal. I tried to flesh them out a bit more so maybe they can still be useful to others.</summary></entry><entry><title type="html">What Isn’t Model-Based Reinforcement Learning?</title><link href="/article/2021/08/06/model-based-rl.html" rel="alternate" type="text/html" title="What Isn’t Model-Based Reinforcement Learning?" /><published>2021-08-06T00:00:00-05:00</published><updated>2021-08-06T00:00:00-05:00</updated><id>/article/2021/08/06/model-based-rl</id><content type="html" xml:base="/article/2021/08/06/model-based-rl.html">&lt;blockquote&gt;
  &lt;p&gt;Having only encountered model-free RL, I had a hard time pinning down what made a model-based RL algorithm for some reason, and here is my shot at the difference. This is not exactly a tutorial or anything, just an attempt to clear up some confusions I had with various terms. I try not to be too prescriptive.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;lets-start-with-what-it-is&quot;&gt;Let’s Start With What It Is&lt;/h2&gt;

&lt;p&gt;We start with an environment that can be represented as an MDP. Our goal is to find the optimal policy of an agent in this MDP.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;dynamics&lt;/strong&gt; of an MDP are the state transitions and rewards. A &lt;strong&gt;model&lt;/strong&gt; is just some representation of the dynamics. Knowing the dynamics is equivalent to having a model.&lt;/p&gt;

&lt;p&gt;Sutton and Barto &lt;a class=&quot;citation&quot; href=&quot;#Sutton1998&quot;&gt;[1]&lt;/a&gt; confirms this:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;By a model of the environment we mean anything that an agent can use to predict how the environment will respond to its actions.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There are two types of a models:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Distribution model&lt;/strong&gt;: gives probabilities of all transition events as the full distribution \(p(r,s'\vert s,a)\).&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Sampling model&lt;/strong&gt;: provides transition samples, i.e. a reward \(r\) and next state \(s'\), given state \(s\) and action \(a\).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The model may or may not be learned by the agent. Unlike the dynamics which are inherent in the MDP, the model is an aspect of the agent or algorithm, and is totally optional.&lt;sup id=&quot;fnref:complete&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:complete&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;If we have (and use) a model, then it becomes a &lt;strong&gt;planning&lt;/strong&gt; problem. If we don’t have a model, then we have a &lt;strong&gt;learning&lt;/strong&gt; problem.&lt;/p&gt;

&lt;p&gt;We can see the same definition in Sutton and Barto:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;The heart of both learning and planning methods is the estimation of value functions by backing-up update operations. The difference is that whereas planning uses simulated experience generated by a model, learning methods use real experience generated by the environment &lt;a class=&quot;citation&quot; href=&quot;#Sutton1998&quot;&gt;[1]&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;They emphasize the usage of simulated vs real experience. Models are clearly simulators, as the experiences they generate are not from the real world dynamics themselves, i.e. real life.&lt;/p&gt;

&lt;blockquote class=&quot;callout&quot;&gt;
  &lt;div class=&quot;flexy&quot;&gt;
    
      &lt;div class=&quot;callout-icon&quot;&gt;🤨&lt;/div&gt;
    
    &lt;div class=&quot;callout-text&quot;&gt;Then if model == planning, and no model == learning, what the heck is model-based reinforcement learning?&lt;/div&gt;
  &lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;In model-based reinforcement learning (RL), we have to &lt;strong&gt;learn&lt;/strong&gt; a model from &lt;strong&gt;real experience&lt;/strong&gt;, and use it to &lt;strong&gt;plan&lt;/strong&gt; the optimal policy from &lt;strong&gt;simulated experience&lt;/strong&gt;. It quite simply smashes planning and learning together.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/mbrl.png&quot; alt=&quot;mbrl&quot; /&gt;
&lt;em&gt;Source: David Silver &lt;a class=&quot;citation&quot; href=&quot;#silver2015&quot;&gt;[2]&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;blockquote class=&quot;callout&quot;&gt;
  &lt;div class=&quot;flexy&quot;&gt;
    
      &lt;div class=&quot;callout-icon&quot;&gt;🤨&lt;/div&gt;
    
    &lt;div class=&quot;callout-text&quot;&gt;Why not model-free reinforcement planning?&lt;/div&gt;
  &lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;Probably just a quirk of history. Check out section 1.7 of &lt;a class=&quot;citation&quot; href=&quot;#Sutton1998&quot;&gt;[1]&lt;/a&gt;! Nowadays, “reinforcement” in RL probably serves to separate it from supervised learning as a learning paradigm, and the addition of “model-based” identifies a class of RL that incorporates planning.&lt;sup id=&quot;fnref:rp&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:rp&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h2 id=&quot;sanity-check&quot;&gt;Sanity Check&lt;/h2&gt;
&lt;p&gt;First some simpler conclusions:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Model-based is &lt;em&gt;not&lt;/em&gt; the same as model-based RL. Out of the four classes of model-based methods in &lt;a href=&quot;https://bair.berkeley.edu/blog/2019/12/12/mbpo/&quot;&gt;this blog post&lt;/a&gt;, only the last two seem to be model-based RL.&lt;sup id=&quot;fnref:familiar&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:familiar&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;The kind of model we have (distribution or sampling) is separate from knowledge of the dynamics, and in fact assumes we &lt;em&gt;do&lt;/em&gt; know the dynamics.&lt;/li&gt;
  &lt;li&gt;If we know the dynamics, then there is no room for learning. On the other hand, if we refuse to know anything about the dynamics, there is no room for a model. For this reason, we could say &lt;em&gt;model-based&lt;/em&gt; RL is an attempt to &lt;em&gt;learn the dynamics&lt;/em&gt;, in addition to learning the optimal policy.&lt;/li&gt;
  &lt;li&gt;RL in general is &lt;em&gt;not&lt;/em&gt; distinguished by the goal of learning the dynamics.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;what-it-isnt&quot;&gt;What It Isn’t&lt;/h2&gt;
&lt;h3 id=&quot;other-senses-of-model&quot;&gt;Other Senses of Model&lt;/h3&gt;
&lt;p&gt;Since I brought up supervised learning, I ought to clarify an unfortunate overlap in terminology. “Model” as described above is &lt;em&gt;not&lt;/em&gt; the same as a supervised machine learning “model,” which rather refers to the output of a machine learning algorithm, without (necessarily) any connection to dynamics. The term “function approximator” is used instead to avoid confusion when we’re dealing with RL algorithms, for example in Deep Q-Learning.&lt;/p&gt;

&lt;p&gt;Yet another possible usage is a description of the MDP as a “model” of the environment. The assumption of an environment represented as an MDP characterizes the class of algorithms we consider in the first place, and therefore anticipates the model-based/model-free distinction in RL.&lt;/p&gt;

&lt;h3 id=&quot;value-functions&quot;&gt;Value Functions&lt;/h3&gt;
&lt;p&gt;Both model-based and model-free RL may estimate value functions, which are not models. They influence how new experience is obtained, but value functions represent expected cumulative returns instead of the dynamics.&lt;/p&gt;

&lt;p&gt;A model could still be necessary, due to our goal of finding the optimal policy. An RL algorithm could learn model-free all the way up to outputting a state-value function \(v\). Strictly speaking, to finish the problem, a model (such as four-argument \(p\)) is still required to extract the policy (if the situation allows it).&lt;/p&gt;

\[\pi^*(s) = \text{argmax}_a \sum_{r,s'}p(r,s'\vert s,a)(r + \gamma v^*(s'))\]

&lt;p&gt;The action-value function \(q\) does not have this problem:&lt;/p&gt;

\[\pi^*(s) = \text{argmax}_a q^*(s,a)\]

&lt;h3 id=&quot;other-senses-of-simulator&quot;&gt;Other Senses of Simulator&lt;/h3&gt;
&lt;p&gt;The colloquial definition of a simulator from Merriam-Webster is:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;a device that enables the operator to reproduce or represent under test conditions phenomena likely to occur in actual performance&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Confusion is likely to arise due to the two senses of the word “simulator”: one as programs or devices, like physics simulators and flight simulators, and the other as models of an MDP’s dynamics. I’ll just go ahead and claim &lt;em&gt;all simulators in the first sense are also simulators in the second sense&lt;/em&gt;. Is a simulation program not a handcrafted model of the real world? Then within a simulator, there is technically no learning involved, as all examples are simulated and the simulator itself is not learned. If we want to keep thinking of it as RL, we could instead learn a policy within the simulator &lt;em&gt;for&lt;/em&gt; the simulator, and just cross our fingers that it transfers well to the real world.&lt;/p&gt;

&lt;p&gt;Is that really a good reason to call something model-based? Is A3C done in a physics engine all of a sudden a planning algorithm? Well, A3C could work on real-world data &lt;em&gt;in principle&lt;/em&gt; so maybe that’s a good enough reason to continue to call it a model-free RL algorithm, absent any context. Then I think it would be permissible to say A3C can be used for planning or model-free RL depending how it’s used.&lt;/p&gt;

&lt;p&gt;Yet another source of confusion is the fact that some “simulated” environments are actually the target environment – no finger-crossing needed. When we’re training an AI to learn Go, we aren’t interested in learning to move the physical pieces on a physical board. All we care about is mastering the mathematical formulation of the game. All gameplay occuring in a Go simulator should therefore be considered real experience for our particular setup. Maybe we could instead use “virtual” to desribe these situations, as it’s different enough from the word “simulated” while still capturing the non-reality of the domain of interest.&lt;/p&gt;

&lt;h3 id=&quot;data-storage-and-reuse&quot;&gt;Data Storage and Reuse&lt;/h3&gt;
&lt;p&gt;An isolated experience \((s, a, r, s')\), encountered, used once, and discarded by the agent, &lt;em&gt;does not count&lt;/em&gt; as a model. That should be obvious: if any interaction with the environment whatsoever counted as a model, then all RL would be model-based. Temporarily storing several experiences or even whole episodes before calculating returns, like in Monte Carlo methods, is not much different.&lt;/p&gt;

&lt;p&gt;Inching closer to model-based methods, we have experience replay (ER), which stores experiences in a buffer, and samples from them later, either individually or in mini-batches, randomly or with a heuristic priority. The experiences are potentially used &lt;em&gt;multiple times&lt;/em&gt; before being deleted to make room for new ones. The reuse of experience constitutes a sort of data augmentation, which is really what a model does during simulation. Unfortunately, we can’t query it at arbitrary states unless they are present in the buffer, which means we can’t use the buffer to run trajectories forward or backward &lt;a class=&quot;citation&quot; href=&quot;#Pan9780999241127&quot;&gt;[3]&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Are those essential for a model? Vanilla Dyna-Q &lt;a class=&quot;citation&quot; href=&quot;#Sutton1998&quot;&gt;[1]&lt;/a&gt; is supposedly model-based, yet it only samples randomly from previous experiences.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/dynaq.png&quot; alt=&quot;dynaq&quot; /&gt;
&lt;em&gt;Just looks like ER to me.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Meanwhile, &lt;a class=&quot;citation&quot; href=&quot;#NEURIPS2019_1b742ae2&quot;&gt;[4]&lt;/a&gt; seems to think it’s a salient distinction:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;On the other hand, a replay memory is less flexible than a model, since we cannot query it at arbitrary states that are not present in the replay memory.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;They go on to use Dyna-Q as an example of model-based RL, &lt;em&gt;but&lt;/em&gt; use a multi-layer perceptron (or more importantly, a parametric function approximator, in opposition to nonparametric ER) to model transitions, terminations, and rewards, allowing it to sample unseen states. Of course they do that, but then their definition of planning is so loose that it includes ER again &lt;a class=&quot;citation&quot; href=&quot;#NEURIPS2019_1b742ae2&quot;&gt;[4]&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;any algorithm that uses additional computation to improve its predictions or behaviour without consuming additional data.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There are bound to be more examples I haven’t seen that straddle the boundary between temporary storage and model, so I won’t really offer my opinion here.&lt;/p&gt;

&lt;blockquote class=&quot;callout&quot;&gt;
  &lt;div class=&quot;flexy&quot;&gt;
    
      &lt;div class=&quot;callout-icon&quot;&gt;🤨&lt;/div&gt;
    
    &lt;div class=&quot;callout-text&quot;&gt;I don't care about your opinion. Is ER a model?&lt;/div&gt;
  &lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;Based on my extensive survey of two papers, I’ll conclude that the literature overall separates ER from models, though with much hedging.&lt;/p&gt;

&lt;h2 id=&quot;examples&quot;&gt;Examples&lt;/h2&gt;

&lt;p&gt;Now I’m going to pretend like my definitions inform my classification of existing RL algorithms, and not the other way around. Summarizing:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;DP methods use models, and are not RL. The dynamics are explicitly used to calculate an optimal policy.&lt;sup id=&quot;fnref:dynamic&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:dynamic&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;MC, TD, and policy gradient methods are model-free RL assuming Q functions are used instead of V functions. Dynamics not required.&lt;/li&gt;
  &lt;li&gt;MCTS uses a model for the rollout phase, but it’s not learned, so it’s a planning algorithm (the use of a virtual target environment like a Go program doesn’t change this).&lt;/li&gt;
  &lt;li&gt;Deep Q-Learning is model-free RL, since we decided ER isn’t a model.&lt;/li&gt;
  &lt;li&gt;Dyna-Q is model-based RL. Because “model” is written in the pseudocode.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:complete&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Barto+Sutton use the game of blackjack as an example to distinguish a distribution model (full dynamics) from some other “complete knowledge”: “Although we have complete knowledge of the environment in this task, it would not be easy to apply DP methods to compute the value function. DP methods require the distribution of next events—in particular, they require the environments dynamics as given by the four-argument function \(p\)—and it is not easy to determine this for blackjack. For example, suppose the player’s sum is 14 and he chooses to stick. What is his probability of terminating with a reward of +1 as a function of the dealer’s showing card?” To be honest I’m still not sure what “complete knowledge of the environment” refers to. Maybe it refers to fully observable state, which is true of blackjack, and allows us to use an MDP. Maybe he’s saying the rules can be used as a sampling model but not a distribution model. Or it could be a game theory term I’m unfamiliar with. &lt;a href=&quot;#fnref:complete&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:rp&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Then unless there are other major planning paradigms, the term “reinforcement planning” is not so useful anymore. &lt;a href=&quot;#fnref:rp&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:familiar&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;I’m not familiar with value-equivalence prediction though so I could be wrong. &lt;a href=&quot;#fnref:familiar&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:dynamic&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;So “dynamic” in dynamic programming has nothing to do with wham bam presto forte after all. See &lt;a href=&quot;https://cstheory.stackexchange.com/a/5643&quot;&gt;this answer&lt;/a&gt; on StackOverflow. &lt;a href=&quot;#fnref:dynamic&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Michal Porubcin</name></author><category term="article" /><category term="reinforcement-learning" /><category term="model" /><summary type="html">Having only encountered model-free RL, I had a hard time pinning down what made a model-based RL algorithm for some reason, and here is my shot at the difference. This is not exactly a tutorial or anything, just an attempt to clear up some confusions I had with various terms. I try not to be too prescriptive.</summary></entry><entry><title type="html">BibTeX For Jekyll Using jekyll-scholar</title><link href="/tutorial/2021/08/06/bibtex-jekyll.html" rel="alternate" type="text/html" title="BibTeX For Jekyll Using jekyll-scholar" /><published>2021-08-06T00:00:00-05:00</published><updated>2021-08-06T00:00:00-05:00</updated><id>/tutorial/2021/08/06/bibtex-jekyll</id><content type="html" xml:base="/tutorial/2021/08/06/bibtex-jekyll.html">&lt;blockquote&gt;
  &lt;p&gt;I made a long Jekyll tutorial covering lots of different features, but the references mini-tutorial became not-so-mini so I moved it to its own post.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#referencescitations-in-jekyll&quot; id=&quot;markdown-toc-referencescitations-in-jekyll&quot;&gt;References/Citations in Jekyll&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#bibtex&quot; id=&quot;markdown-toc-bibtex&quot;&gt;BibTeX&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#basics&quot; id=&quot;markdown-toc-basics&quot;&gt;Basics&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#custom-citation-style&quot; id=&quot;markdown-toc-custom-citation-style&quot;&gt;Custom Citation Style&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#link-to-resource&quot; id=&quot;markdown-toc-link-to-resource&quot;&gt;Link To Resource&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#adding-to-post-layout&quot; id=&quot;markdown-toc-adding-to-post-layout&quot;&gt;Adding To Post Layout&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#unresolved-issues&quot; id=&quot;markdown-toc-unresolved-issues&quot;&gt;Unresolved Issues&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;referencescitations-in-jekyll&quot;&gt;References/Citations in Jekyll&lt;/h2&gt;

&lt;p&gt;Say you want to add references on your page. Instead of generating them outside of jekyll and copying them to the bottom of your post, or worse, typing them out by hand, you can let a package called jekyll-scholar take care of it all within jekyll.&lt;/p&gt;

&lt;h2 id=&quot;bibtex&quot;&gt;BibTeX&lt;/h2&gt;

&lt;p&gt;&lt;a href=&quot;http://www.bibtex.org/&quot;&gt;BibTeX&lt;/a&gt; is “a tool and a file format which are used to describe and process lists of references, mostly in conjunction with LaTeX documents.” A BibTeX entry looks something like this:&lt;/p&gt;

&lt;div class=&quot;language-bib highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nc&quot;&gt;@book&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;nl&quot;&gt;Sutton1998&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;added-at&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;{2019-07-13T10:11:53.000+0200}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;author&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;{Sutton, Richard S. and Barto, Andrew G.}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;biburl&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;{https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;edition&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;{Second}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;interhash&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;{ac6b144aaec1819919a2fba9f705c852}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;intrahash&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;{f46601cf8b13d39d1378af0d79438b12}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;publisher&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;{The MIT Press}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;timestamp&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;{2019-07-13T10:11:53.000+0200}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;title&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;{Reinforcement Learning: An Introduction}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;url&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;{http://incompleteideas.net/book/the-book-2nd.html}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;year&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;{2018}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;link&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;{http://incompleteideas.net/book/RLbook2020.pdf}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You don’t usually have to worry about constructing these. The entry comes all in one piece wherever the article is published so you can easily copy/paste it. You can still modify it; I added &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;link&lt;/code&gt; to point to a pdf version of the book.&lt;/p&gt;

&lt;blockquote class=&quot;callout&quot;&gt;
  &lt;div class=&quot;flexy&quot;&gt;
    
      &lt;div class=&quot;callout-icon&quot;&gt;😐&lt;/div&gt;
    
    &lt;div class=&quot;callout-text&quot;&gt;If you get a bunch of warnings with the message 'Warning: Empty 'slug' generated for ''.' in the terminal, then it's probably due to empty items in a bibtex entry. See this issue: https://github.com/jekyll/jekyll/issues/7881. Try not to have entries that look like {} or ''.&lt;/div&gt;
  &lt;/div&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;basics&quot;&gt;Basics&lt;/h2&gt;

&lt;p&gt;Add it to the Gemfile and run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bundle install&lt;/code&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-gemfile&quot;&gt;group :jekyll_plugins do
  gem &quot;jekyll-scholar&quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Add it to _config.yml to use it:&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;plugins&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;jekyll-scholar&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Restart the server and it should be ready to go.&lt;/p&gt;

&lt;p&gt;Jekyll-scholar allows you to generate references and citations from bibtex files, much like in a LaTeX environment. It’s pretty customizable, and the &lt;a href=&quot;https://github.com/inukshuk/jekyll-scholar&quot;&gt;repo’s readme&lt;/a&gt; has good documentation (for once). The basics are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Keep all your references in _bibliography/references.bib&lt;/li&gt;
  &lt;li&gt;Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{% bibliography %}&lt;/code&gt; wherever you want to list your references (in the post layout for example).&lt;/li&gt;
  &lt;li&gt;Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{% cite bibliography-entry %}&lt;/code&gt; whenever you want to cite a specific entry, replacing bibliography-entry with the entry id. Unfortunately it may have to go in the Markdown of your post.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;custom-citation-style&quot;&gt;Custom Citation Style&lt;/h2&gt;

&lt;p&gt;Some things to consider when you’re customizing your references. The tag &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{{reference}}&lt;/code&gt; gives the core reference element. The plugin will always output an ordered list, and customization will modify each list item inclusive of the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{{reference}}&lt;/code&gt;, but the format of the actual reference element is determined by a cls file. The &lt;a href=&quot;https://github.com/citation-style-language/styles&quot;&gt;csl repo&lt;/a&gt; has a list of presets to choose from. For example, I like the &lt;a href=&quot;https://libguides.murdoch.edu.au/IEEE/home&quot;&gt;IEEE&lt;/a&gt; style better than APA, so I swapped it by adding this to \config.yml:&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;scholar&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ieee&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The value of the style parameter is just a style from the csl repo without the .csl file extension.&lt;/p&gt;

&lt;p&gt;I ran into some problems though. First, IEEE is a numbered style, so the csl file would generate a number redundant with the number in the HTML list.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ref-redundant.png&quot; alt=&quot;ref-redundant&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I found a solution buried in &lt;a href=&quot;https://martinopilia.com/posts/2020/02/22/migration.html&quot;&gt;here&lt;/a&gt;. First, copy ieee.csl from the csl repo into _bibliography/ieee_custom.csl and customize it. It looks pretty hefty but here’s the gist of the structure:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;a bunch of macros&lt;/li&gt;
  &lt;li&gt;a citation block&lt;/li&gt;
  &lt;li&gt;and a bibliography block&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Down in the bibliography block, comment out the citation number:&lt;/p&gt;
&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;bibliography&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;entry-spacing=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;0&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;second-field-align=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;flush&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;layout&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;suffix=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;.&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;&amp;lt;!-- Citation Number --&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;&amp;lt;!-- &amp;lt;text variable=&quot;citation-number&quot; prefix=&quot;[&quot; suffix=&quot;] &quot;/&amp;gt; --&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;&amp;lt;!-- Author(s) --&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;text&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;macro=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;author&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;suffix=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;, &quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;/&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Point to the new csl file in _config.yml:&lt;/p&gt;
&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;scholar&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;style&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;_/bibliography/ieee_custom.csl&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now edit the CSS of the numerals generated by jekyll-scholar, to satisfy the IEEE style:&lt;/p&gt;
&lt;div class=&quot;language-css highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;ol&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;.bibliography&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;counter-reset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;ol&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;.bibliography&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;li&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;list-style-type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;none&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;ol&lt;/span&gt;&lt;span class=&quot;nc&quot;&gt;.bibliography&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;li&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;:before&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&quot;[&quot;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;counter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&quot;] &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;counter-increment&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;position&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;absolute&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;text-align&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;right&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;4em&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;margin-left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;-4.4em&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ref-fixed.png&quot; alt=&quot;ref-fixed&quot; /&gt;
&lt;em&gt;That does the trick.&lt;/em&gt;&lt;/p&gt;

&lt;blockquote class=&quot;callout&quot;&gt;
  &lt;div class=&quot;flexy&quot;&gt;
    
      &lt;div class=&quot;callout-icon&quot;&gt;😐&lt;/div&gt;
    
    &lt;div class=&quot;callout-text&quot;&gt;Warning: edits to the csl file seem to require restarting the jekyll server.&lt;/div&gt;
  &lt;/div&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;link-to-resource&quot;&gt;Link To Resource&lt;/h2&gt;

&lt;p&gt;I also wanted to make the csl file add an HTML link around the title pointing to the resource (like the url for a pdf of a research paper), as long as the reference had a link field in the bibtex (references.bib). Unfortunately the solutions I found looked pretty hacky so I settled for putting the link outside the csl-generated reference. To do this, just modify the list element using a &lt;a href=&quot;https://github.com/inukshuk/jekyll-scholar#bibliography-template&quot;&gt;bibliography template&lt;/a&gt; at _layouts/bibitem.html.&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;span&amp;gt;&lt;/span&gt;{{reference}}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
{% if entry.link %} 
&lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{{ entry.link }}&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;[link]&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
{% endif %}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Add it to the config:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;scholar&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;...&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;bibliography_template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;bibitem&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ref-link.png&quot; alt=&quot;ref-link&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;adding-to-post-layout&quot;&gt;Adding To Post Layout&lt;/h2&gt;

&lt;p&gt;One last thing I wanted was the ability to put the references in the post layout so a) I could place it below the footnotes and b) I wouldn’t have to manually add it in every post. The first step is to put &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{% bibliography --cited %}&lt;/code&gt; in the post template, to show only elements of the bibliography you have cited on the page. Unfortunately if you have a header like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;h2&amp;gt;References&amp;lt;/h2&amp;gt;&lt;/code&gt;, then it will show up even if there are no citations in the post. Here’s my solution:&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{% capture count %}{% bibliography_count --cited %}{% endcapture %}
{% assign count = count | plus: 0 %}
{% if count &amp;gt; 0 %}
&lt;span class=&quot;nt&quot;&gt;&amp;lt;h2&amp;gt;&lt;/span&gt;References&lt;span class=&quot;nt&quot;&gt;&amp;lt;/h2&amp;gt;&lt;/span&gt;
{% endif %}
&lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;post-bibliography&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
{% bibliography --cited %}
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The first line gets a string representing the number of references cited on the page and stores it in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;count&lt;/code&gt;. It gets converted to a number in the second line, and used in the third to show the header only if there are one or more references cited on the page.&lt;/p&gt;

&lt;h2 id=&quot;unresolved-issues&quot;&gt;Unresolved Issues&lt;/h2&gt;

&lt;p&gt;When I cited a reference using a BibTeX key with two periods, the citation wouldn’t render. That is, something like &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{% cite 10.55553304652.3304675 %}&lt;/code&gt; turned into “(missing reference)”. For now I just change the key in the BibTeX entry to something without two periods.&lt;/p&gt;</content><author><name>Michal Porubcin</name></author><category term="tutorial" /><category term="bibtex" /><category term="jekyll" /><summary type="html">I made a long Jekyll tutorial covering lots of different features, but the references mini-tutorial became not-so-mini so I moved it to its own post.</summary></entry><entry><title type="html">Jekyll Blog Tutorial (+ Lots of Features)</title><link href="/tutorial/2021/08/01/jekyll-tutorial.html" rel="alternate" type="text/html" title="Jekyll Blog Tutorial (+ Lots of Features)" /><published>2021-08-01T00:00:00-05:00</published><updated>2021-08-01T00:00:00-05:00</updated><id>/tutorial/2021/08/01/jekyll-tutorial</id><content type="html" xml:base="/tutorial/2021/08/01/jekyll-tutorial.html">&lt;blockquote&gt;
  &lt;p&gt;Here are the notes and external tutorials I used and gathered while creating this blog. Includes everything from Gemfiles to plugins to LaTeX. No life story, no fluff!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#summary&quot; id=&quot;markdown-toc-summary&quot;&gt;Summary&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#prerequisites&quot; id=&quot;markdown-toc-prerequisites&quot;&gt;Prerequisites&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#jekyll-overview&quot; id=&quot;markdown-toc-jekyll-overview&quot;&gt;Jekyll Overview&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#setup&quot; id=&quot;markdown-toc-setup&quot;&gt;Setup&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-gemfile&quot; id=&quot;markdown-toc-the-gemfile&quot;&gt;The Gemfile&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#github-pages&quot; id=&quot;markdown-toc-github-pages&quot;&gt;GitHub Pages&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-config&quot; id=&quot;markdown-toc-the-config&quot;&gt;The Config&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#adding-content&quot; id=&quot;markdown-toc-adding-content&quot;&gt;Adding Content&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#file-organization&quot; id=&quot;markdown-toc-file-organization&quot;&gt;File Organization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#themes&quot; id=&quot;markdown-toc-themes&quot;&gt;Themes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#kramdown&quot; id=&quot;markdown-toc-kramdown&quot;&gt;Kramdown&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#other-features&quot; id=&quot;markdown-toc-other-features&quot;&gt;Other Features&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#callouts&quot; id=&quot;markdown-toc-callouts&quot;&gt;Callouts&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#code&quot; id=&quot;markdown-toc-code&quot;&gt;Code&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#comments-disqus&quot; id=&quot;markdown-toc-comments-disqus&quot;&gt;Comments (Disqus)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#excerpts&quot; id=&quot;markdown-toc-excerpts&quot;&gt;Excerpts&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#fonts&quot; id=&quot;markdown-toc-fonts&quot;&gt;Fonts&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#footnotes&quot; id=&quot;markdown-toc-footnotes&quot;&gt;Footnotes&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#images&quot; id=&quot;markdown-toc-images&quot;&gt;Images&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#image-captions&quot; id=&quot;markdown-toc-image-captions&quot;&gt;Image Captions&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#latex&quot; id=&quot;markdown-toc-latex&quot;&gt;LaTeX&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#links&quot; id=&quot;markdown-toc-links&quot;&gt;Links&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#pagination&quot; id=&quot;markdown-toc-pagination&quot;&gt;Pagination&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#references&quot; id=&quot;markdown-toc-references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#table-of-contents&quot; id=&quot;markdown-toc-table-of-contents&quot;&gt;Table of Contents&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#tags&quot; id=&quot;markdown-toc-tags&quot;&gt;Tags&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#publishing-the-site&quot; id=&quot;markdown-toc-publishing-the-site&quot;&gt;Publishing the Site&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#misc-troubleshooting&quot; id=&quot;markdown-toc-misc-troubleshooting&quot;&gt;Misc. Troubleshooting&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#changes-not-visible&quot; id=&quot;markdown-toc-changes-not-visible&quot;&gt;Changes not visible&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#404-error&quot; id=&quot;markdown-toc-404-error&quot;&gt;404 Error&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#bundle-install-fails&quot; id=&quot;markdown-toc-bundle-install-fails&quot;&gt;Bundle Install Fails&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#browser-problems&quot; id=&quot;markdown-toc-browser-problems&quot;&gt;Browser Problems&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;
&lt;p&gt;I built this blog with Jekyll, and this is a compilation of tutorials I used, along with my own notes on the steps I took. Lilian’s &lt;a href=&quot;https://github.com/lilianweng/lil-log&quot;&gt;site&lt;/a&gt; is my inspiration. I use Github Pages to host the site, but I do use incompatible plugins.&lt;/p&gt;

&lt;h3 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Tech&lt;/strong&gt;: Intel macOS Big Sur&lt;sup id=&quot;fnref:forthis&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:forthis&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Skills&lt;/strong&gt;: terminal (command prompt) and git competency, basic HTML knowledge&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;jekyll-overview&quot;&gt;Jekyll Overview&lt;/h2&gt;
&lt;p&gt;What is Jekyll? This &lt;a href=&quot;https://docs.github.com/en/pages/setting-up-a-github-pages-site-with-jekyll/about-github-pages-and-jekyll&quot;&gt;page&lt;/a&gt; is a quick intro. Maybe read through it once.&lt;/p&gt;

&lt;p&gt;I’ll still try to convey the appeal of Jekyll (to me anyways). With Jekyll, I can scaffold my blog once using reusable templates, and reduce repeated content creation, like blog posts, to simple markdown files. Jekyll gives you a static site, so you don’t have to worry about backends and databases and all this over-engineering, and you can host it on GitHub for free.&lt;/p&gt;

&lt;h2 id=&quot;setup&quot;&gt;Setup&lt;/h2&gt;

&lt;p&gt;Run the laptop &lt;a href=&quot;https://github.com/monfresh/laptop&quot;&gt;script&lt;/a&gt; to setup a Ruby environment and Jekyll. Make sure to follow the prerequisites and the install section in that tutorial. While going through those steps, rather than fixing my Homebrew I just reinstalled it fresh.&lt;/p&gt;

&lt;p&gt;In this GitHub &lt;a href=&quot;https://docs.github.com/en/pages/setting-up-a-github-pages-site-with-jekyll/about-github-pages-and-jekyll&quot;&gt;tutorial&lt;/a&gt;, follow “Creating site with Jekyll,” minus prerequisites, since those were taken care of with the laptop script. This will set up a remote GitHub repo and a local repo, and set up Jekyll in the local repo. STOP before pushing to remote (unless you don’t care that your barebones website will be live). At this point we can test locally.&lt;/p&gt;

&lt;p&gt;We now see several files in our repo. Gemfile and Gemfile.lock are due to Bundler. I found this explanation about Bundler: “Bundler manages an application’s dependencies through its entire life across many machines systematically and repeatably.” In the Gemfile we write the gem dependencies we want to use for Ruby programs, and the autogenerated Gemfile.lock lists every dependency and version installed. More info &lt;a href=&quot;https://medium.com/never-hop-on-the-bandwagon/gemfile-and-gemfile-lock-in-ruby-65adc918b856&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Basically, run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bundle install&lt;/code&gt; now, and later whenever you change the Gemfile.&lt;/p&gt;

&lt;p&gt;To host the site locally, run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bundle exec jekyll serve&lt;/code&gt;. If successful, you should be able to go to &lt;a href=&quot;http://localhost:4000/&quot;&gt;&lt;/a&gt;&lt;a href=&quot;http://localhost:4000&quot;&gt;http://localhost:4000&lt;/a&gt; in the browser and view the default Jekyll blog.&lt;/p&gt;

&lt;p&gt;When we’re ready to deploy, run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bundler exec jekyll build&lt;/code&gt; to generate the site files. This is called &lt;em&gt;building&lt;/em&gt; the site (wow!) and I will refer to it as such from here on – I will not use building to mean the colloquial sense of just creating the blog.&lt;/p&gt;

&lt;blockquote class=&quot;callout&quot;&gt;
  &lt;div class=&quot;flexy&quot;&gt;
    
      &lt;div class=&quot;callout-icon&quot;&gt;😐&lt;/div&gt;
    
    &lt;div class=&quot;callout-text&quot;&gt;The following do the same thing: 'bundle' 'bundler' 'bundle install' 'bundle i'.&lt;/div&gt;
  &lt;/div&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;the-gemfile&quot;&gt;The Gemfile&lt;/h2&gt;

&lt;p&gt;The default Gemfile has good comments to help you figure things out but let’s quickly run through it anyway. A gem will be listed like this: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gem &quot;jekyll&quot;, &quot;~&amp;gt; 4.2.0&quot;&lt;/code&gt; . First comes the gem name, followed by a version number.&lt;/p&gt;

&lt;p&gt;We see a gem for the default theme, &lt;em&gt;minima&lt;/em&gt;. It’s pretty minimal as we will see.&lt;/p&gt;

&lt;p&gt;Further down is a group called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;:jekyll_plugins&lt;/code&gt; . We should throw all our jekyll plugins in here, pretty self-explanatory.&lt;/p&gt;

&lt;p&gt;The rest of the default Gemfile is Windows stuff.&lt;/p&gt;

&lt;h2 id=&quot;github-pages&quot;&gt;GitHub Pages&lt;/h2&gt;

&lt;p&gt;Since we’re using GitHub Pages to host our site, it’s important to know that Github Pages runs Jekyll to build the site by default, but it doesn’t support most Jekyll plugins and themes. Supported themes are listed &lt;a href=&quot;https://pages.github.com/themes/&quot;&gt;here&lt;/a&gt;, and supported plugins are listed &lt;a href=&quot;https://pages.github.com/versions/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You have to decide whether or not to have full compatibility. I chose not to since the convenience offered by plugins outweighed the inconvenience of building my site locally. If you go to the last section of this tutorial before troubleshooting, maybe you’ll share my opinion.&lt;/p&gt;

&lt;h2 id=&quot;the-config&quot;&gt;The Config&lt;/h2&gt;

&lt;p&gt;The default _config.yml stores settings used by Jekyll to build the site. These are site-wide settings, plugins, and themes. Anything defined here can be accessed as a variable in the content.&lt;/p&gt;

&lt;h2 id=&quot;adding-content&quot;&gt;Adding Content&lt;/h2&gt;

&lt;p&gt;A blog needs content! There’s two types of content: pages and posts. Pages have no dates and are standalone, like the About page. Posts have a date, and they form the core of the blog. Some &lt;a href=&quot;https://stackoverflow.com/questions/15095625/what-are-the-differences-between-a-post-and-a-page-in-jekyll&quot;&gt;more&lt;/a&gt; on the difference.&lt;/p&gt;

&lt;p&gt;The posts must be stored in _posts, found in the root directory. The posts must have the format yyyy-mm-dd-name.markdown. That is, the date in the given format followed by the name, and finally the markdown file extension.&lt;/p&gt;

&lt;blockquote class=&quot;callout&quot;&gt;
  &lt;div class=&quot;flexy&quot;&gt;
    
      &lt;div class=&quot;callout-icon&quot;&gt;😐&lt;/div&gt;
    
    &lt;div class=&quot;callout-text&quot;&gt;Note that a post's date corresponds with the date in the filename, but can be overridden by the date given in the _front matter_ of the post — more on that immediately.&lt;/div&gt;
  &lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;A simple post will look like this:&lt;/p&gt;

&lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;layout&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;post&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Name&quot;&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;2021-07-10 14:10:58 -0700&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;categories&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;jekyll&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;

This is a post. Blablablablablabla
Thank you for your time.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The text between the triple dashes is called the front matter, and it holds non-content like title, date, tags, etc. Content, or the actual blog text, goes below the front matter. Jekyll takes care of formatting and preparing the content for the web, leaving us to focus on the content.&lt;/p&gt;

&lt;p&gt;The front matter is in YAML format, like the config file. The rest of the content is in markdown format. From now on, anything done in markdown is implicitly referring to the posts!&lt;/p&gt;

&lt;h2 id=&quot;file-organization&quot;&gt;File Organization&lt;/h2&gt;

&lt;p&gt;Markdown handles basic text formatting like italics, lists, links, and tables, but it is possible to customize the overall &lt;strong&gt;style&lt;/strong&gt; and &lt;strong&gt;structure&lt;/strong&gt; of the site as a whole. A theme is a drop-in solution for this.&lt;/p&gt;

&lt;p&gt;The default theme is minima. Take a look at the minima repo &lt;a href=&quot;https://github.com/jekyll/minima&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;There are several important directories here:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;_includes: secondary templates to “plug into” other templates, or includes&lt;/li&gt;
  &lt;li&gt;_layouts: primary templates, or layouts&lt;/li&gt;
  &lt;li&gt;_posts: posts&lt;/li&gt;
  &lt;li&gt;_sass/minima: styling&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Styling is handled by sass files in _sass/minima and assets.&lt;/p&gt;

&lt;p&gt;Structure is handled by &lt;a href=&quot;https://jekyllrb.com/docs/layouts/&quot;&gt;layouts&lt;/a&gt; and &lt;a href=&quot;https://jekyllrb.com/docs/includes/&quot;&gt;includes&lt;/a&gt; (found in _layouts and _includes). In summary, both layouts and includes are templates, which are html documents with additional special syntax so they can be reused in different contexts. “Includes” are templates meant to be plugged into designated spots in another template. “Layouts” on the other hand are templates that can stand on their own.&lt;/p&gt;

&lt;p&gt;It is important to note that template inheritance works differently than template includes. In inheritance, the parent template marks a single spot to plug in the child with the string &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{{ content }}&lt;/code&gt;. The template or markdown post to be plugged in puts the parent template in the front matter, for example &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;layout: default&lt;/code&gt;. Includes are when a template marks a specific template to be plugged in, for example &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;{% include header.html %}&lt;/code&gt;. The template to be plugged in does &lt;em&gt;not&lt;/em&gt; have to specify what it is being plugged into. The braces syntax is part of the Liquid template language.&lt;/p&gt;

&lt;p&gt;Generally, keep base templates in _layouts, and reusable components in _includes.&lt;/p&gt;

&lt;h2 id=&quot;themes&quot;&gt;Themes&lt;/h2&gt;

&lt;p&gt;Returning to themes, while they aren’t necessary I highly recommend at least using one as a base, especially if you’re not super savvy with styling. Themes can also bundle together some nice features on top of a custom look and feel, like search and archiving.&lt;/p&gt;

&lt;p&gt;To use a theme as-is, see &lt;a href=&quot;https://jekyllrb.com/docs/themes/&quot;&gt;this guide&lt;/a&gt;. Note that a gem-based theme may be incompatible with GitHub Pages. To build on a custom theme, copy the files from the theme’s repo to your repository. To customize the styling, you need to write custom CSS/Sass. &lt;a href=&quot;https://jekyllrb.com/docs/step-by-step/07-assets/&quot;&gt;This tutorial&lt;/a&gt; should get you started. You should do your styling in the Sass files under _sass.&lt;/p&gt;

&lt;h2 id=&quot;kramdown&quot;&gt;Kramdown&lt;/h2&gt;

&lt;p&gt;The default Markdown renderer for Jekyll is Kramdown, and the default processor is the GitHub Flavored Markdown (GFM) parser. It’s enabled by default, and I assume it’s set for the remainder of the tutorial. See the Kramdown &lt;a href=&quot;https://kramdown.gettalong.org/syntax.html&quot;&gt;documentation&lt;/a&gt; for more info on specific functions.&lt;/p&gt;

&lt;h2 id=&quot;other-features&quot;&gt;Other Features&lt;/h2&gt;

&lt;p&gt;The remaining features are optional so I’ll list them alphabetically.&lt;/p&gt;

&lt;h3 id=&quot;callouts&quot;&gt;Callouts&lt;/h3&gt;

&lt;p&gt;I’m calling a callout any blockquote with an emoji to the side. I like the way Notion does it so I’m copying their look.&lt;/p&gt;

&lt;p&gt;Unlike the clever solution with image captions (scroll down), I couldn’t find a way to make callouts without using Liquid syntax in the markdown. It’s not a big deal but if the markdown file is used for something else, then the Liquid probably won’t be supported.&lt;/p&gt;

&lt;p&gt;Put this in the markdown:&lt;/p&gt;

&lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{% include callout.html content=&quot;Put the callout message here.&quot; icon=&quot;neutral&quot;%}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The content variable is the message in the callout. The icon variable is the emoji you use for the callout. Then make a new file _includes/callout.html:&lt;/p&gt;

&lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;blockquote&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;callout&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;flexy&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    {% if include.icon == &quot;neutral&quot; %}
      &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;callout-icon&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;😐&lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
    {% elsif include.icon == &quot;eyeroll&quot; %}
      &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;callout-icon&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;🙄&lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
    {% elsif include.icon == &quot;browraise&quot; %}
      &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;callout-icon&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;🤨&lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
    {% else %}
      &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;callout-icon&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;{{ include.icon }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
    {% endif %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;callout-text&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;{{ include.content }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/blockquote&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Add more emojis as desired. In a scss file, style it something like so:&lt;/p&gt;

&lt;div class=&quot;language-css highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;override&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;blockquote&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;left&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;border&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;color&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;padding&lt;/span&gt;
&lt;span class=&quot;nc&quot;&gt;.callout&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;padding-left&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;12px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;border-color&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;flex&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;container&lt;/span&gt;
&lt;span class=&quot;nc&quot;&gt;.flexy&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;display&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;icon&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;fixed&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;left&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;flex&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;item&lt;/span&gt;
&lt;span class=&quot;nc&quot;&gt;.callout-icon&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;30px&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;flex-shrink&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;remaining&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;width&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;right&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;flex&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;item&lt;/span&gt;
&lt;span class=&quot;nc&quot;&gt;.callout-text&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;flex-grow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I tweaked things like padding and font size until it looked good. Note: markdown processing won’t work inside callouts.&lt;/p&gt;

&lt;h3 id=&quot;code&quot;&gt;Code&lt;/h3&gt;

&lt;p&gt;Syntax highlighting is provided in minima. Just copy the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.highlight&lt;/code&gt; section from &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_sass/minima/skins/classic.scss&lt;/code&gt; .&lt;/p&gt;

&lt;p&gt;When typing code, use spaces instead of tabs, or else the indent length on the site will be too large (in my opinion). In my Markdown editor, there is an option to put four spaces automatically when I hit tab.&lt;/p&gt;

&lt;h3 id=&quot;comments-disqus&quot;&gt;Comments (Disqus)&lt;/h3&gt;

&lt;p&gt;Comments would be pretty nice right? The problem is they clash with the whole static-site thing. Number one concern: where are the comments stored? You have two options:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;host your own comments&lt;/li&gt;
  &lt;li&gt;let someone do it for you.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The first option is a pain and sort of nullifies some of Jekyll’s more appealing features, like free hosting on GitHub. Now think of what actually goes into a comment system. Do users make an account to post? Can they edit and delete comments? What about threads, sorting, mentions, reactions, email notifications, and moderation?&lt;/p&gt;

&lt;p&gt;What? Disqus does all that? For &lt;em&gt;free&lt;/em&gt;? In spite of a handful of blogposts critical of ad-ridden, tracker-obsessed, privacy-trampling Disqus, that seductive string of features pulled me to the dark side. Plus there’s no free, cloud-hosted alternatives as far as I know (I tried something called Social9 but it’s clearly still in the works).&lt;/p&gt;

&lt;p&gt;Just follow steps 1 and 2 of &lt;a href=&quot;https://jreel.github.io/setting-up-disqus-comments-on-jekyll/&quot;&gt;this tutorial&lt;/a&gt;. If you used the minima theme as a base like I did, then you should already have Disqus boilerplate code. Otherwise finish the tutorial. Note that you won’t be able to see the comments when testing locally.&lt;/p&gt;

&lt;p&gt;We’re not done yet. We can turn off some ads in the Disqus dashboard. See the images:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/jekyll-tutorial/adsettings.png&quot; alt=&quot;adsettings.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/jekyll-tutorial/advancedsettings.png&quot; alt=&quot;advancedsettings.png&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;excerpts&quot;&gt;Excerpts&lt;/h3&gt;

&lt;p&gt;If we want to add post excerpts, add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;excerpt_separator: &amp;lt;!--more--&amp;gt;&lt;/code&gt; to _config.yaml. The excerpt in each post will go from the beginning to the excerpt separator. Add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;!--more--&amp;gt;&lt;/code&gt; to each post at the desired end of the excerpt.&lt;/p&gt;

&lt;blockquote class=&quot;callout&quot;&gt;
  &lt;div class=&quot;flexy&quot;&gt;
    
      &lt;div class=&quot;callout-icon&quot;&gt;😐&lt;/div&gt;
    
    &lt;div class=&quot;callout-text&quot;&gt;'excerpt_separator: &lt;!--more--&gt;' can also be added per post in the front matter but you have to be more careful about using the separator in the main page (index.html or similar)&lt;/div&gt;
  &lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;I like the idea from Lil’log of blockquote summaries as the excerpts, so I stole it.&lt;/p&gt;

&lt;h3 id=&quot;fonts&quot;&gt;Fonts&lt;/h3&gt;

&lt;p&gt;Clarification on terms:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Typeface: Visual design of the symbols (what you see)&lt;/li&gt;
  &lt;li&gt;Font: Implementation of the symbols (what you use)&lt;/li&gt;
  &lt;li&gt;Style: Single typeface, e.g. bold&lt;/li&gt;
  &lt;li&gt;Family: Complete set of styles&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a href=&quot;https://web.dev/variable-fonts/&quot;&gt;More info&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Go to &lt;a href=&quot;https://fonts.google.com/&quot;&gt;Google Fonts&lt;/a&gt;, find a nice font family, select the desired styles (or create your own if it’s a variable font), (optionally at this stage, click the icon in the top right to open a sidebar, if it doesn’t happen automatically), from the sidebar select “@import”, copy the text INSIDE the style tags, and paste it in one of the scss files in _sass. Here’s a picture of the sidebar:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/jekyll-tutorial/font.png&quot; alt=&quot;font.png&quot; width=&quot;250&quot; /&gt;
&lt;em&gt;I dare you to use this font on your site.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Last step is to actually use it. For example, if we’re following closely to the minima styling files, we might see &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$base-font-family: &quot;Your Old Font&quot;, &quot;Helvetica Neue&quot;, &quot;Times New Roman&quot;, serif !default;&lt;/code&gt;. Replace the first option with the name of the font family we just imported.&lt;/p&gt;

&lt;blockquote class=&quot;callout&quot;&gt;
  &lt;div class=&quot;flexy&quot;&gt;
    
      &lt;div class=&quot;callout-icon&quot;&gt;😐&lt;/div&gt;
    
    &lt;div class=&quot;callout-text&quot;&gt;Some fonts will break LaTeX. See the LaTeX section.&lt;/div&gt;
  &lt;/div&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h3&gt;
&lt;p&gt;Footnotes are supported by Kramdown! See this &lt;a href=&quot;https://stackoverflow.com/a/48250535&quot;&gt;SO answer&lt;/a&gt; for how to use them and about compatibility.&lt;/p&gt;

&lt;p&gt;To write multi-line footnotes, indent the paragraphs after the first one. This allows you to add LaTeX math blocks in the footnotes, in addition to inline math.&lt;/p&gt;

&lt;p&gt;Although I ended up not using a footnotes title, here’s how I did it when I had one:&lt;/p&gt;

&lt;div class=&quot;language-css highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nc&quot;&gt;.footnotes&lt;/span&gt;&lt;span class=&quot;nd&quot;&gt;::before&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;display&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;content&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;&quot;Footnotes:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;images&quot;&gt;Images&lt;/h3&gt;

&lt;p&gt;Images are added like this in markdown:&lt;/p&gt;

&lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;![&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;alt_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;](&lt;/span&gt;&lt;span class=&quot;sx&quot;&gt;/assets/images/image.png&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can also separate the path from the rest like so:&lt;/p&gt;

&lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;![alt_text]
...
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;alt_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;sx&quot;&gt;/assets/images/image.png&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It didn’t work for me without the first forward slash in the link (before “assets”).&lt;/p&gt;

&lt;p&gt;To fix the size of the image, add some CSS:&lt;/p&gt;

&lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;![&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;alt_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;](&lt;/span&gt;&lt;span class=&quot;sx&quot;&gt;/assets/images/image.png&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;{: width=&quot;500&quot;}

OR

![alt_text]{: width=&quot;500&quot;}
...
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;alt_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;sx&quot;&gt;/assets/images/image.png&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can also add a CSS class instead, and style it separately.&lt;/p&gt;

&lt;h3 id=&quot;image-captions&quot;&gt;Image Captions&lt;/h3&gt;

&lt;p&gt;Want to have image captions but don’t want to pollute your markdown files with specialized Liquid syntax? You’ve come to the right place :)&lt;/p&gt;

&lt;p&gt;Use this in markdown:&lt;/p&gt;

&lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;![&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;alt_text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;](&lt;/span&gt;&lt;span class=&quot;sx&quot;&gt;path_to_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;ge&quot;&gt;*image_caption*&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then use this in Sass:&lt;/p&gt;

&lt;div class=&quot;language-css highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;img&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;em&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;display&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
  &lt;span class=&quot;nl&quot;&gt;text-align&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;center&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This selects the first em element after an img. When the above markdown is processed it will generate an img followed by an em as desired.&lt;/p&gt;

&lt;p&gt;Source: this &lt;a href=&quot;https://stackoverflow.com/questions/19331362/using-an-image-caption-in-markdown-jekyll&quot;&gt;SO thread&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;latex&quot;&gt;LaTeX&lt;/h3&gt;

&lt;p&gt;There are many outdated tutorials for LaTeX out there, so don’t follow them!&lt;/p&gt;

&lt;p&gt;In includes/head.html (or somewhere in the head section of your site), add:&lt;/p&gt;

&lt;div class=&quot;language-css highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;script&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&quot;text/javascript&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&quot;MathJax-script&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;async&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&quot;&amp;lt;https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js&amp;gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;lt;/&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;script&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Notice we are using mathjax 3, which is faster than mathjax 2. Notice we are not using the mathjax cdn (&lt;a href=&quot;http://cdn.mathjax.org&quot;&gt;cdn.mathjax.org&lt;/a&gt;) since it was &lt;a href=&quot;https://www.mathjax.org/cdn-shutting-down/&quot;&gt;shut down&lt;/a&gt;. It may still work due to a redirect to &lt;a href=&quot;http://cdnjs.cloudflare.com&quot;&gt;cdnjs.cloudflare.com&lt;/a&gt;. We use jsdelivr instead. Finally, be sure to use https instead of http or mathjax may be blocked for security reasons.&lt;/p&gt;

&lt;p&gt;Finally, in markdown, surround LaTeX expressions with double dollar signs: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;$$y=x^2$$&lt;/code&gt;, for both inline and block expressions. Instead of using dedicated syntax, block expressions have to be separated from surrounding text by a newline on each side.&lt;/p&gt;

&lt;p&gt;If you use a font called Crimson Text (and possibly other fonts), from Google Fonts, the LaTeX expressions may be too small on Firefox. If you use mathjax 2 for whatever reason, see &lt;a href=&quot;https://groups.google.com/g/mathjax-users/c/v3W-daBz87k/m/xjxFFdfQBQAJ&quot;&gt;here&lt;/a&gt; for an explanation and two possible fixes. Those solutions did not work for me with mathjax 3, so I just switched to a different font.&lt;/p&gt;

&lt;p&gt;Random note: if you want to type a vertical bar in inline LaTeX, use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;\vert&lt;/code&gt;, because otherwise Jekyll might think it’s the start of a table.&lt;/p&gt;

&lt;h3 id=&quot;links&quot;&gt;Links&lt;/h3&gt;

&lt;p&gt;Links are added like this in markdown:&lt;/p&gt;

&lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;link text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;](&lt;/span&gt;&lt;span class=&quot;sx&quot;&gt;http://www.example.com/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can also separate the path from the rest like so:&lt;/p&gt;

&lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;reference style link&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;linkid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
...
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;ss&quot;&gt;linkid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]:&lt;/span&gt; &lt;span class=&quot;sx&quot;&gt;http://www.example.com/&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;&quot;Optional Title&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The “reference style link” text should be replaced by your own text.&lt;/p&gt;

&lt;p&gt;An &lt;strong&gt;internal link&lt;/strong&gt;, or a link to another post on your site, looks like this:&lt;/p&gt;

&lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;link text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;](&lt;/span&gt;&lt;span class=&quot;sx&quot;&gt;{{&lt;/span&gt; site.baseurl }}{% post_url 2021-07-19-post %})
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;What comes after &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;post_url&lt;/code&gt;  is the name of the post file, without the file extension (typically &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.md&lt;/code&gt;).&lt;/p&gt;

&lt;h3 id=&quot;pagination&quot;&gt;Pagination&lt;/h3&gt;

&lt;p&gt;Pagination is supported. Modify the Gemfile like so:&lt;/p&gt;

&lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;group :jekyll_plugins do
	gem &quot;jekyll-paginate&quot;
	...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Modify the _config.yaml like so:&lt;/p&gt;

&lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;plugins:
&lt;span class=&quot;p&quot;&gt;  -&lt;/span&gt; jekyll-paginate
	...

paginate: 10
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The number after &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;paginate&lt;/code&gt; is the max number of posts per page.&lt;/p&gt;

&lt;p&gt;Run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bundler&lt;/code&gt; to install.&lt;/p&gt;

&lt;p&gt;Now to implement the pagination in the site files. jekyll-paginate only works on index.html. In particular, the default jekyll project uses an &lt;a href=&quot;http://index.md&quot;&gt;index.md&lt;/a&gt; which extends an html template, but that won’t work. After appropriate refactoring (using an index.html), replace all mention of &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;site.posts&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;paginator.posts&lt;/code&gt;. Finally, add this blob towards the bottom to add page navigation buttons.&lt;/p&gt;

&lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{% if paginator.total_pages &amp;gt; 1 %}
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;pagination&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
    {% if paginator.previous_page %}
      &lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ml-1 mr-1&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{{ paginator.previous_page_path | prepend: site.baseurl | replace: '//', '/' }}&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;ni&quot;&gt;&amp;amp;laquo;&lt;/span&gt; Prev&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
    {% else %}
      &lt;span class=&quot;nt&quot;&gt;&amp;lt;span&amp;gt;&lt;/span&gt;&lt;span class=&quot;ni&quot;&gt;&amp;amp;laquo;&lt;/span&gt; Prev&lt;span class=&quot;nt&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
    {% endif %}
    {% for page in (1..paginator.total_pages) %}
      {% if page == paginator.page %}
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;span&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ml-1 mr-1&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;{{ page }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
      {% elsif page == 1 %}
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ml-1 mr-1&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{{ paginator.previous_page_path | prepend: site.baseurl | replace: '//', '/' }}&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;{{ page }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
      {% else %}
        &lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ml-1 mr-1&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{{ site.paginate_path | prepend: site.baseurl | replace: '//', '/' | replace: ':num', page }}&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;{{ page }}&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
      {% endif %}
    {% endfor %}
    {% if paginator.next_page %}
      &lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;ml-1 mr-1&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;{{ paginator.next_page_path | prepend: site.baseurl | replace: '//', '/' }}&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Next &lt;span class=&quot;ni&quot;&gt;&amp;amp;raquo;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
    {% else %}
      &lt;span class=&quot;nt&quot;&gt;&amp;lt;span&amp;gt;&lt;/span&gt;Next &lt;span class=&quot;ni&quot;&gt;&amp;amp;raquo;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;lt;/span&amp;gt;&lt;/span&gt;
    {% endif %}
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
{% endif %}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There is a more sophisticated pagination package &lt;a href=&quot;https://github.com/sverrirs/jekyll-paginate-v2&quot;&gt;here&lt;/a&gt;, but it is not supported by GitHub Pages. I decided I didn’t need it.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;p&gt;I moved the full references tutorial to a &lt;a href=&quot;/tutorial/2021/08/06/bibtex-jekyll.html&quot;&gt;separate post&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;/h3&gt;
&lt;p&gt;Plop this in your markdown.&lt;/p&gt;

&lt;div class=&quot;language-markdown highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;-&lt;/span&gt; TOC
{:toc}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It will be replaced by an list of all your headers, with indentation.&lt;/p&gt;

&lt;p&gt;Here are some &lt;a href=&quot;https://kramdown.gettalong.org/converter/html.html#toc&quot;&gt;kramdown settings&lt;/a&gt; affecting table of contents, which you can set in _config.yml.&lt;/p&gt;

&lt;h3 id=&quot;tags&quot;&gt;Tags&lt;/h3&gt;

&lt;p&gt;Tags are supported in Jekyll. Just list tags separated by spaces in the front matter of the corresponding post. Unfortunately that’s pretty much it. There are packages to view a page of tags, or list posts by tag, but they are not supported by GitHub Pages. I’d have to implement that functionality myself.&lt;/p&gt;

&lt;p&gt;Someone did just that in &lt;a href=&quot;https://longqian.me/2017/02/09/github-jekyll-tag/&quot;&gt;this tutorial&lt;/a&gt;. With this approach, the markdown files in the tag folder must be generated for every new tag (even if automated with the given Python script).&lt;/p&gt;

&lt;p&gt;That felt like complete crap so I turned to &lt;a href=&quot;https://github.com/jekyll/jekyll-archives&quot;&gt;jekyll-archives&lt;/a&gt; (before this I was fully compatible with GitHub Pages, this was the breaking point for me haha). It also handles categories and archives. The only problem is the added burden of building the site on my own before pushing to GitHub, but if tag generation is not completely smooth it’s not worth sticking so fiercely to GitHub Pages.&lt;/p&gt;

&lt;p&gt;To have a dedicated page to display all your tags, make a tags.html with something like this in it:&lt;/p&gt;

&lt;div class=&quot;language-html highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---
layout: default
---
&lt;span class=&quot;nt&quot;&gt;&amp;lt;div&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;home other-pages&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;&amp;lt;h1&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;page-heading&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;Tags&lt;span class=&quot;nt&quot;&gt;&amp;lt;/h1&amp;gt;&lt;/span&gt;
  {% capture temp_tags %}
    {% for tag in site.tags %}
      {{ tag[1].size | plus: 1000 }}#{{ tag[0] }}#{{ tag[1].size }}
    {% endfor %}
  {% endcapture %}
  {% assign sorted_temp_tags = temp_tags | split:' ' | sort %}
  {% for temp_tag in sorted_temp_tags %}
    {% assign tag_items = temp_tag | split: '#' %}
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;span&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;post-meta&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;
      &lt;span class=&quot;nt&quot;&gt;&amp;lt;a&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;class=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;post-tag&quot;&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;href=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;/tag/{{ tag_items[1] }}&quot;&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;&amp;gt;&lt;/span&gt;{{ tag_items[1]}} ({{ tag_items[2] }})&lt;span class=&quot;nt&quot;&gt;&amp;lt;/a&amp;gt;&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;&amp;lt;/span&amp;gt;&amp;lt;br/&amp;gt;&lt;/span&gt;
  {% endfor %}
&lt;span class=&quot;nt&quot;&gt;&amp;lt;/div&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Similar things can be done for categories and archives.&lt;/p&gt;

&lt;h2 id=&quot;publishing-the-site&quot;&gt;Publishing the Site&lt;/h2&gt;

&lt;p&gt;Done? Time to publish! Remember we used custom plugins, so we can’t push to GitHub and let it build our site for us. Now at this stage people will say you need to build the site yourself, then move it to another branch called gh-pages, then push that to GitHub Pages, and since it is &lt;em&gt;such&lt;/em&gt; a burden, they will then promote their GitHub Actions and Travis CIs.&lt;/p&gt;

&lt;p&gt;Here’s what you do. Check you are on master or main branch (I haven’t checked if any arbitrary branch will work). Wrap up your changes and git commits. Now add &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;destination: docs&lt;/code&gt; to _config.yml to change the destination of the static site generated by Jekyll from _site to docs. Now run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bundler exec jekyll build&lt;/code&gt; to build the site. Also in terminal, run &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;touch .nojekyll&lt;/code&gt; . This is a file which tells GitHub not to run Jekyll on your files because you already did that. Now commit .nojekyll and docs, and push to GitHub. Finally, in GitHub, go to Settings &amp;gt; Pages &amp;gt; Source and change the root folder to docs. There may be some benefit to using separate branches instead of a docs folder in the same branch, but it’s just a personal site so I doubt it matters.&lt;/p&gt;

&lt;p&gt;From now on, just build before pushing.&lt;sup id=&quot;fnref:tried&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:tried&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;Assuming you are making a user site and you have not set a baseurl in _config.yml, you should be able to go to username.github.io and view your blog!&lt;sup id=&quot;fnref:otherwise&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:otherwise&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; Congratulations!&lt;/p&gt;

&lt;h2 id=&quot;misc-troubleshooting&quot;&gt;Misc. Troubleshooting&lt;/h2&gt;

&lt;p&gt;I tried to keep troubleshooting to each specific section above, but here’s some extras. In general, if things are misbehaving, be sure to check for any error messages in the terminal first. They’re pretty descriptive.&lt;/p&gt;

&lt;h3 id=&quot;changes-not-visible&quot;&gt;Changes not visible&lt;/h3&gt;
&lt;p&gt;This class of issues is fixed by having at least a double digit IQ.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If changes were made to _config.yaml, the site has to be relaunched.&lt;/li&gt;
  &lt;li&gt;If changes were made to styling, make sure it is being imported all the way through to assets/css/main.css.&lt;/li&gt;
  &lt;li&gt;Your styling code could just be wrong.&lt;/li&gt;
  &lt;li&gt;You think you’re editing/replacing one file but you’re changing another one. Or you think you’re refreshing the post you edited but you’re not. Or you think you’re refreshing the local testing site but you’re refreshing the production site.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;404-error&quot;&gt;404 Error&lt;/h3&gt;

&lt;p&gt;Annoying and frequent mistake: even if the code is automatically reloaded upon changes, if you just refresh your page you may reload a dead link so check the URL.&lt;/p&gt;

&lt;p&gt;Another possibility is that title has changed. If so go back to &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;localhost:4000&lt;/code&gt;, &lt;em&gt;reload&lt;/em&gt;, and navigate to the desired page. This happened to me on Safari.&lt;/p&gt;

&lt;h3 id=&quot;bundle-install-fails&quot;&gt;Bundle Install Fails&lt;/h3&gt;

&lt;p&gt;I remember having to delete the Gemfile.lock file to get my gems to install one time. I think it’s no big deal if you’re working solo but I’m not that knowledgeable about it so take it with a grain of salt.&lt;/p&gt;

&lt;h3 id=&quot;browser-problems&quot;&gt;Browser Problems&lt;/h3&gt;

&lt;p&gt;Firefox messed me up with the fonts at least, but browser compatibility could come into play with other things too. Try different browsers. This applies mostly to display oddities.&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:forthis&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;These are what I used so I can say with some confidence that you too can get a working solution with them. Alternatives may also work though. &lt;a href=&quot;#fnref:forthis&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:tried&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;I tested this with a main branch and a user site. I haven’t tried other branches, or project or organization sites. It seems the only acceptable folders are . (root) and ./docs. &lt;a href=&quot;#fnref:tried&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:otherwise&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Otherwise it’s not so hard to figure out your URL. Google about GitHub Pages if you need more help there. &lt;a href=&quot;#fnref:otherwise&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Michal Porubcin</name></author><category term="tutorial" /><category term="blog" /><category term="jekyll" /><summary type="html">Here are the notes and external tutorials I used and gathered while creating this blog. Includes everything from Gemfiles to plugins to LaTeX. No life story, no fluff!</summary></entry><entry><title type="html">Project Reupload #3: VI-sensor</title><link href="/project/2021/07/21/sensor.html" rel="alternate" type="text/html" title="Project Reupload #3: VI-sensor" /><published>2021-07-21T00:00:00-05:00</published><updated>2021-07-21T00:00:00-05:00</updated><id>/project/2021/07/21/sensor</id><content type="html" xml:base="/project/2021/07/21/sensor.html">&lt;blockquote&gt;
  &lt;p&gt;I’m writing posts on my old projects. Here I will introduce one of my most ambitious projects (at the time), a visual-inertial sensor for use with SLAM.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;tele-cube&quot;&gt;Tele-Cube&lt;/h2&gt;

&lt;p&gt;For this project, I tried to make a virtual cube move based on sensor movement.&lt;/p&gt;

&lt;blockquote class=&quot;callout&quot;&gt;
  &lt;div class=&quot;flexy&quot;&gt;
    
      &lt;div class=&quot;callout-icon&quot;&gt;🙄&lt;/div&gt;
    
    &lt;div class=&quot;callout-text&quot;&gt;That's trivial. There are cheap sensors with out-of-box demos for this.&lt;/div&gt;
  &lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/sparkfunimu.gif&quot; alt=&quot;sparkfunimu&quot; /&gt;
&lt;em&gt;Straight from the Sparkfun website&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I could have made that my project but I wanted to do more. A rotation demo shows three degrees of (rotational) freedom. That’s like knowing how an airplane is oriented without knowing where it actually is in the sky. I wanted an extra three degrees of freedom along positional axes.&lt;/p&gt;

&lt;blockquote class=&quot;callout&quot;&gt;
  &lt;div class=&quot;flexy&quot;&gt;
    
      &lt;div class=&quot;callout-icon&quot;&gt;🙄&lt;/div&gt;
    
    &lt;div class=&quot;callout-text&quot;&gt;Any IMU will have an accelerometer and a gyroscope, one for calculating position and one for rotation.&lt;/div&gt;
  &lt;/div&gt;
&lt;/blockquote&gt;

&lt;p&gt;The problem with IMUs is that the accelerometer directly measures acceleration. To get a position estimate, the IMU has to double integrate over the acceleration. The error from this operation accumulates pretty quickly and the result is a drifting position estimate, which becomes useless pretty quickly. The gyroscope has a similar problem.&lt;/p&gt;

&lt;p&gt;The problem of using sensor data to determining change of position and orientation, or pose, over time is called &lt;strong&gt;odometry&lt;/strong&gt;. IMUs are not capable of good odometry on their own,&lt;sup id=&quot;fnref:imubad&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:imubad&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; so one improvement is to combine multiple sensors with different properties together, like a visual-inertial (VI) sensor, and perform &lt;strong&gt;visual-inertial odometry&lt;/strong&gt;. Yet another improvement is to use a more sophisticated algorithm,&lt;sup id=&quot;fnref:sophisticated&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:sophisticated&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; called &lt;strong&gt;simultaneous localization and mapping (SLAM)&lt;/strong&gt;.&lt;/p&gt;

&lt;h2 id=&quot;the-hardware&quot;&gt;The Hardware&lt;/h2&gt;

&lt;p&gt;A good VI sensor, like the discontinued &lt;a href=&quot;/assets/images/visensor.png&quot;&gt;Skybotix VI-sensor&lt;/a&gt; below, uses one or more high quality cameras and IMUs, with hardware synchronization and god-tier calibration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/visensor.png&quot; alt=&quot;visensor&quot; /&gt;
&lt;em&gt;For just  €3900.00!&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Obviously my sensor was a bit lower spec. I decided to use an open-source SLAM algorithm called ROVIO, which requires hardware synchronization of the camera and IMU, and a global shutter camera. To satisfy those two points I got a Point Grey Firefly MV camera, the cheapest global shutter camera I could find with an external triggering capability. My IMU on the other hand was a cheapy from Sparkfun, which at least incorporated a magnetometer to improve measurements a bit. The IMU supports Arduino onboard, which I used to trigger the camera shutter at regular intervals. I 3D-printed a holder to keep both sensors locked in place. The sensors needed to be calibrated, both as individual sensors and together as a unit. The IMU came with a handy calibration tutorial, and &lt;a href=&quot;https://github.com/ethz-asl/kalibr&quot;&gt;Kalibr&lt;/a&gt; took care of the rest.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/visensor2.png&quot; alt=&quot;visensor2&quot; /&gt;
&lt;em&gt;My VI sensor ._.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The sensor ended up being the focus of the project, but remember it was just part of a whole pipeline. It needed to feed its data to a computer, which needed to run ROVIO, feed the pose data to a graphics engine, and rotate a cube.&lt;/p&gt;

&lt;p&gt;I wanted to keep a fairly mobile system so I could &lt;em&gt;in principle&lt;/em&gt; mount everything on a drone, which ruled out a desktop computer. I tried the very low power Raspberry Pi, but it was too weak to run SLAM, ruling out most single board computers. I chose Unity to handle the virtual cube, so I chose Ubuntu 16 for its compatibility with Unity and ROS (for ROVIO). The smallest system I could find that could run a full Ubuntu OS and had enough processing power to run ROVIO was an Intel NUC. Finally I used a battery pack to power the NUC.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/glow.jpg&quot; alt=&quot;glow&quot; /&gt;
&lt;em&gt;I thought this was a cool picture when I took it. Shows a NUC, my laptop, and an external drive.&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;note-on-money&quot;&gt;Note on Money&lt;/h2&gt;

&lt;p&gt;One flaw in this tidy little blog post is the omission of all the failed approaches I tried first, many which involved acquiring some special hardware.&lt;/p&gt;

&lt;p&gt;A small investment can go a long way. I would not have finished this project without some key purchases like the IMU or camera. As I realized, however, the purchases eventually hit diminishing returns. I ended up buying things and then returning them when I realized they didn’t quite fit my needs, or holding onto them to this day. Some of those items were somewhat cheap, like the Raspberry Pi, and some were not (like an eGPU…yeah…I later tried to make up for it by cryptomining).&lt;/p&gt;

&lt;p&gt;Some of the waste can be offset by spending even more time asking knowledgeable people, either online or in real life. Research is also hit by diminshing returns though, eventually resulting in a paralysis of ambivalence. You will eventually have to make a purchase and sometimes there’s no better way to know if it works than to buy it and try it.&lt;/p&gt;

&lt;p&gt;So while I did spend an uncomfortable amount of internship earnings on this project, I learned a few tips to squeeze the most efficiency out of my time and money. In order, I recommend:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Sticking to software instead of hardware if possible.&lt;/li&gt;
  &lt;li&gt;Reducing the scope of the project.&lt;/li&gt;
  &lt;li&gt;Taking advantage of a lab or makerspace.&lt;/li&gt;
  &lt;li&gt;Using Amazon Prime.&lt;sup id=&quot;fnref:sponsor&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:sponsor&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Notice choosing software projects comes &lt;em&gt;first!&lt;/em&gt; Nearly everything software-related outside of a production environment can be done for free. &lt;em&gt;Maybe&lt;/em&gt; you pay a few bucks for hosting or cloud services. For hardware projects, the other tips should help. To explain Amazon Prime a bit more, I found it useful both for quick deliveries and fully refunded returns, allowing me to iterate quickly. Students get a discount, and a free trial is available to anyone.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;After many painful months, I did manage to move the virtual cube!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/videmo.png&quot; alt=&quot;videmo&quot; /&gt;
&lt;em&gt;Please hold your applause&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;A video feed from the camera is displayed in the upper left corner, and the cube is rotated and translated to match the pose of the sensor in real time.&lt;/p&gt;

&lt;p&gt;Well, there was actually some noticeable lag in the movement, and I identified a few sources:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;At the time, I was using a package called ros-sharp to pass ROS video frames to Unity. I think either they were raw instead of compressed image frames, or they were being passed with JSON instead of BSON.&lt;/li&gt;
  &lt;li&gt;The method to display the video feed in Unity may have been inefficient. The demo ran more smoothly without video.&lt;/li&gt;
  &lt;li&gt;As my advisor kindly pointed out, SLAM is intensive enough that many mobile/embedded SLAM systems have a VPU dedicated to underlying computer vision procedures. My homemade version obviously lacked anything of the sort, but the NUC did have a decent general purpose CPU. I wasn’t able to test if SLAM alone was a bottleneck, but I did notice a performance difference with and without the screen recording running.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I wish I had more time to fix the display issues, but at least ROVIO runs smoothly, meaning my sensor worked too. I even managed to get the whole thing working off the battery, carrying it around in a backpack setup. I will add a picture if I can find it or recreate it.&lt;/p&gt;

&lt;p&gt;You may be wondering things like:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Why not just display the video straight from ROS?&lt;/li&gt;
  &lt;li&gt;Why not use a laptop instead of a NUC?&lt;/li&gt;
  &lt;li&gt;Why use Unity instead of say Gazebo?&lt;/li&gt;
  &lt;li&gt;Why use SLAM at all?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some choices can be excused by incomplete knowledge, some were completely asinine, and all were cut off from a satisfactory level of deliberation in my commitment to the MVP.&lt;/p&gt;

&lt;h2 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h2&gt;

&lt;p&gt;As usual, I ran into issues with project scope. Unlike my &lt;a href=&quot;/project/2021/07/19/wisp.html&quot;&gt;Wisp&lt;/a&gt; project, I &lt;em&gt;never&lt;/em&gt; added features, but instead the true scale of the project was completely hidden from me since I lacked experience with the field (even worse than the &lt;a href=&quot;/project/2021/07/20/poker-ai.html&quot;&gt;poker project&lt;/a&gt;). I thought okay the state-of-the-art is in SLAM so I’ll do that! When I realized how complicated everything was, I ended up using an off-the-shelf SLAM in order to reach the MVP. There’s nothing wrong with using a prebuilt package, but I could have chosen from a more lightweight family of algorithms like VIO, which would have lowered requirements on both the sensor components and computer. Additionally, I was unable to debug it when things went awry. I actually started with a completely different SLAM algorithm called VINS-MONO, but had to ditch it because I simply couldn’t get it to run.&lt;/p&gt;

&lt;p&gt;My number one priority after building the sensor was to dive more deeply into SLAM and related algorithms. Still, basic knowledge of the sensor and how it fits into the whole pose estimation pipeline is invaluable. I was exposed to tons of concepts about cameras, lenses, and basic photography that I find fascinating even now. Most of all I dove into the completely new fields of robotics and computer vision and came out with something to show for it. Without a doubt, one of my favorite projects.&lt;/p&gt;

&lt;p&gt;I will release specific tutorials I put together during the project.&lt;/p&gt;

&lt;p&gt;[Update 2021-08-08]: Tutorials &lt;a href=&quot;/tutorial/2021/08/07/sensor-tutorials.html&quot;&gt;uploaded&lt;/a&gt;!&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:imubad&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;In particular, I mean cheap MEMS IMUs. &lt;a href=&quot;#fnref:imubad&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:sophisticated&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;VIO is more complicated than one-sensor odometry by virtue of the additional sensor modality. SLAM is more sophisticated than VIO for reasons other than the particular sensor(s) used, namely mapping and loop-closure. &lt;a href=&quot;#fnref:sophisticated&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:sponsor&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Not sponsored I promise. &lt;a href=&quot;#fnref:sponsor&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Michal Porubcin</name></author><category term="project" /><category term="vio" /><category term="slam" /><category term="computer-vision" /><category term="hardware" /><category term="sensor" /><summary type="html">I’m writing posts on my old projects. Here I will introduce one of my most ambitious projects (at the time), a visual-inertial sensor for use with SLAM.</summary></entry><entry><title type="html">Project Reupload #2: Poker AI</title><link href="/project/2021/07/20/poker-ai.html" rel="alternate" type="text/html" title="Project Reupload #2: Poker AI" /><published>2021-07-20T00:00:00-05:00</published><updated>2021-07-20T00:00:00-05:00</updated><id>/project/2021/07/20/poker-ai</id><content type="html" xml:base="/project/2021/07/20/poker-ai.html">&lt;blockquote&gt;
  &lt;p&gt;I’m writing posts on my old projects. In this post, I’ll introduce my final project for one of my classes in junior year, a simplified implementation of a state-of-the-art poker AI.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;ai-poker&quot;&gt;AI Poker&lt;/h2&gt;
&lt;p&gt;Zoom in to my COMS4995 lecture in Spring of 2018, where I was discussing with two random classmates, Dan and Ethan, what we should do for our class project. We all recently learned about &lt;a href=&quot;https://www.nature.com/articles/nature16961&quot;&gt;AlphaGo’s&lt;/a&gt; victory against Lee Sedol in Go, and Ethan liked poker, so we thought, &lt;em&gt;maybe someone made an AlphaGo for poker&lt;/em&gt;. Turns out in January of that year, a duo from Carnegie Mellon presented Libratus, an algorithm that beat four top human professionals at &lt;strong&gt;heads-up no-limit (HUNL) Texas Hold’em poker&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Texas Hold’em: the most popular “core rules” of poker, including dealing and betting&lt;/li&gt;
  &lt;li&gt;No-limit: bets are not fixed, and are bounded by a predetermined minimum and an all-in maximum&lt;/li&gt;
  &lt;li&gt;Heads-up: two players&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The immediate question: did they more or less apply the method from AlphaGo to HUNL poker? The short answer is no.&lt;/p&gt;

&lt;p&gt;Even considering a two-player version, poker is different from Go in several aspects:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Drawn cards introduce randomness.&lt;/li&gt;
  &lt;li&gt;Each player’s hand is hidden from the other.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;While the first point complicates evaluation of the stronger player, the second point is more important. In Go, both players always know the exact state of the game, but in Poker, players have to deal with &lt;strong&gt;imperfect information&lt;/strong&gt;. This renders useless the core methods of AlphaGo and its successors.&lt;/p&gt;

&lt;p&gt;Like AlphaGo, no official open-source code was released, but unlike AlphaGo, nobody had taken a shot at an unofficial implementation. So Ethan, Dan, and I decided to create an open-source version of Libratus, with the added goal of aiding the research community. Enthusiasm and naivette: the perfect combo!&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/noam.png&quot; alt=&quot;noam&quot; /&gt;
&lt;em&gt;Objection: it needs 25 million core hours anyway so what’s the big deal??&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;process&quot;&gt;Process&lt;/h2&gt;
&lt;p&gt;Here’s a silly timeline of my experiences that spring:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;This notation is incomprehensible&lt;/li&gt;
  &lt;li&gt;There is no deep learning&lt;/li&gt;
  &lt;li&gt;This is actually a super complicated project but we’re past project milestone…&lt;/li&gt;
  &lt;li&gt;We cracked a core algorithm!&lt;/li&gt;
  &lt;li&gt;Wait we have to write a poker simulator&lt;/li&gt;
  &lt;li&gt;Wait we have to interface with ACPC in order to benchmark&lt;/li&gt;
  &lt;li&gt;Wait the class is over??&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I’ll walk through some of it.&lt;/p&gt;

&lt;h3 id=&quot;notation&quot;&gt;Notation&lt;/h3&gt;
&lt;p&gt;At the time I had zero experience with poker or game theory, crucial components of Libratus (and only brief exposure to reinforcement learning, which hindered understanding of related game AI literature). It would take almost a whole week to get through a paper. I remember griping over the Notation and Background section of the paper &lt;a href=&quot;https://arxiv.org/abs/1705.02955&quot;&gt;Safe And Nested Subsolving For Imperfect-Information Games&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;deep-learning&quot;&gt;Deep Learning&lt;/h3&gt;
&lt;p&gt;We were initially going to implement &lt;a href=&quot;https://science.sciencemag.org/content/356/6337/508&quot;&gt;DeepStack&lt;/a&gt;, a poker AI from 2017 that utilized deep learning. When we switched to Libratus, I think I just assumed there would be deep learning, because it seemed like the only way to tackle super challenging games like Go and Poker. I was shocked to find that Libratus did &lt;em&gt;not&lt;/em&gt; use deep learning and still performed better than DeepStack.&lt;/p&gt;

&lt;h3 id=&quot;project-milestone&quot;&gt;Project Milestone&lt;/h3&gt;
&lt;p&gt;Towards the middle of our project we realized just how complicated Libratus was. I thought we would be dealing with something like &lt;a href=&quot;https://www.nature.com/articles/nature24270&quot;&gt;AlphaGo Zero&lt;/a&gt; (the first successor to AlphaGo) which elegantly combined MCTS with a two-headed policy and value network, and removed “human data, guidance or domain knowledge beyond game rules.”&lt;/p&gt;

&lt;p&gt;I discovered, however, that AlphaGo Zero was an anomaly among game AIs, and for Libratus, domain knowledge was just as important as game theory. Even worse, we were well into the project already, and we faced the all-too-common dilemma of building a complete, faithful implementation versus having something presentable at the end of the semester. For example we may not have fully implemented card and action abstractions, which are basically groupings of cards and actions to reduce the number of possibilities per turn.&lt;sup id=&quot;fnref:forgot&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:forgot&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;It was at this point too that I became a bit jaded about benchmarking AIs with games. I might write another post about it, but in summary, the world-class poker bot transformed in my mind from a leap forward for AI into just another game someone was able to build an algorithm for.&lt;sup id=&quot;fnref:same&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:same&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt; Not a unique view at all, and not a slight against Libratus either; it’s still an unprecedented feat.&lt;/p&gt;

&lt;h3 id=&quot;core-algorithm-cracked&quot;&gt;Core Algorithm Cracked&lt;/h3&gt;
&lt;p&gt;We had one major win, successfully writing MCCFR-p, the algorithm which computes a so-called blueprint strategy. We started by reading up on the more basic CFR algorithm (this &lt;a href=&quot;http://modelai.gettysburg.edu/2013/cfr/cfr.pdf&quot;&gt;paper&lt;/a&gt; specifically), and writing a basic implementation for Khun poker, a toy version of poker. We were then able to translate this to the more sophisticated algorithm on a bigger poker game.&lt;/p&gt;

&lt;h3 id=&quot;simulator-and-acpc-server&quot;&gt;Simulator and ACPC server&lt;/h3&gt;
&lt;p&gt;The last stretch saw algorithmic challenges almost wholly replaced by engineering ones. We had to finalize the poker simulator, so a) we could demo the game and b) to simulate valid moves while pre-training the strategies. We wrote rules for Leduc poker which reduced the game size to something trainable on our puny machines (but still larger than Kuhn poker), but introduced its own difficulties because it’s a lesser known ruleset, and we had trouble validating it. Finally we needed to implement gameplay with other bots via an ACPC server. Luckily we found a Python wrapper and plugged it in.&lt;/p&gt;

&lt;h2 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h2&gt;
&lt;p&gt;I at least was completely burnt out by the end, and just when we were getting a hang of things we put a bow on it and never touched it again. I highly doubt it was helpful to researchers, as we had hoped. The message “This is a work in progress” sits on the readme like a dead open sign on a dilapidated storefront. It’s all kind of a regret of mine! Still I have to remember this was my first class with a final project, and I learned an incredible amount in a short period of time. It was also my first attempt at reading through the literature of a certain field and trying to implement something in it from scratch. The repo is &lt;a href=&quot;https://github.com/michalp21/coms4995-finalproj&quot;&gt;here&lt;/a&gt; for the curious.&lt;/p&gt;

&lt;p&gt;The research world doesn’t slow down. Noam Brown has continued putting out excellent work, such as &lt;a href=&quot;https://www.cs.cmu.edu/~noamb/papers/19-Science-Superhuman.pdf&quot;&gt;Pluribus&lt;/a&gt; for multiplayer poker, and &lt;a href=&quot;https://arxiv.org/abs/2007.13544&quot;&gt;ReBeL&lt;/a&gt;, which utilizes deep reinforcement learning. Someone seems to be working on an &lt;a href=&quot;https://github.com/fedden/poker_ai&quot;&gt;open-source&lt;/a&gt; implementation of Pluribus, and an official repo exists for ReBeL implemented on a different game called &lt;a href=&quot;https://github.com/facebookresearch/rebel&quot;&gt;Liar’s Dice&lt;/a&gt;. I’m excited to see what the future holds for poker AIs!&lt;/p&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:forgot&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Unfortunately I forgot a lot of the details, and looking at it now, our code isn’t a paragon of best practices. &lt;a href=&quot;#fnref:forgot&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:same&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Same applies to all versions of AlphaGo by the way. &lt;a href=&quot;#fnref:same&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Michal Porubcin</name></author><category term="project" /><category term="game" /><category term="poker" /><category term="reinforcement-learning" /><summary type="html">I’m writing posts on my old projects. In this post, I’ll introduce my final project for one of my classes in junior year, a simplified implementation of a state-of-the-art poker AI.</summary></entry><entry><title type="html">Project Reupload #1: Wisp</title><link href="/project/2021/07/19/wisp.html" rel="alternate" type="text/html" title="Project Reupload #1: Wisp" /><published>2021-07-19T00:00:00-05:00</published><updated>2021-07-19T00:00:00-05:00</updated><id>/project/2021/07/19/wisp</id><content type="html" xml:base="/project/2021/07/19/wisp.html">&lt;blockquote&gt;
  &lt;p&gt;I’m writing posts on my old projects. In this post, I talk about a game I made with Henry G. for Ludum Dare 40. I also talk about my old game dev experiences.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;!--more--&gt;

&lt;h2 id=&quot;before-the-actual-project-i-will-tell-you-about-my-previous-life-as-an-aspiring-game-dev-deal-with-it&quot;&gt;Before The Actual Project I Will Tell You About My Previous Life As an Aspiring Game Dev, Deal With It&lt;/h2&gt;

&lt;p&gt;Prior to discovering AI, I guess I could say game dev was my first choice of career. I made my first game back in high school with GameMaker. In college I ran through enough Unity tutorials to make an (unpublished) Android puzzle game. I vaguely recall some back-and-forth with Zynga about a summer internship, but withdrew my application.&lt;/p&gt;

&lt;p&gt;I decided to start a new project with a larger scope. Down the line I met Henry at gamedev club and even convinced him to help me out somehow. Unfortunately we weren’t making much meaningful progress because I kept adding features and underestimating the ballooning scope of the project. I had started with a basic multiplayer pirate-themed game with attack-defense gameplay like Clash of Clans – a big task already! Then I thought, why not add a two-phased battle? How about custom attacks for each troop with MOBA-style cooldowns? Maybe clans? (Why was I such a Supercell fanboy?)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pirates.png&quot; alt=&quot;pirates&quot; /&gt;
&lt;em&gt;Flat Boats at Dusk, 2017. Acrylic on canvas.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I also got stuck on details. I wanted deployed troops to avoid colliding, and I came across a paper about &lt;a href=&quot;https://gamma.cs.unc.edu/RVO/icra2008.pdf&quot;&gt;reciprocal velocity obstacles&lt;/a&gt; (the first research paper I ever read of my own volition, to my knowledge). I decided to code it in C# right in Unity, even though Henry later found a library which implemented it efficiently. We didn’t even need such strict collision avoidance; a proximity-based repelling force would have sufficed.&lt;/p&gt;

&lt;p&gt;It wasn’t getting anywhere, and in fact it never did get anywhere. In light of all this, Henry eventually suggested we try doing Ludum Dare, a competition to build a game from scratch over a single weekend. I can’t speak for him, but I at least saw the opportunity to walk away with a finished game.&lt;/p&gt;

&lt;h2 id=&quot;ludum-dare-40&quot;&gt;Ludum Dare 40&lt;/h2&gt;

&lt;p&gt;Every Ludum Dare has a theme, and for 2017 it was: &lt;strong&gt;The more you have, the worse it is&lt;/strong&gt;. After some brainstorming, we decided on a &lt;a href=&quot;https://en.wikipedia.org/wiki/Roguelike&quot;&gt;rogue-like&lt;/a&gt; game. Rogue-likes are a specific subgenre of role-playing games, and a key commonality is procedurally generated dungeons, yet another feature I secretly wanted to add to my pirate game.&lt;/p&gt;

&lt;p&gt;It was sort of an interesting problem. We had parameters for room count, room size, and hall size. We built the rooms randomly, then connected them with a minimum spanning tree, and added a few extra halls based on another connectivity parameter. We used Unity Tilemap to implement the actual grid.&lt;/p&gt;

&lt;p&gt;None of that was relevant to the theme of course. We stuffed the entire theme into a lighting buddy orbiting the player, called a wisp. The player could make the wisp brighter to see more of the dungeon, but the monsters would become more aggressive. It was pretty hard – I don’t even think we gave the player a weapon. We scattered healing potions around the map, but they gave some light too, making the monsters more aggressive in the vicinity.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/wisp1.png&quot; alt=&quot;wisp1&quot; /&gt;
&lt;em&gt;Some in-game screenshots&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;We didn’t win anything, but I think some guy on YouTube played a bunch of submissions and said some slightly positive things about ours. I wish I could find it!&lt;/p&gt;

&lt;h2 id=&quot;takeaways&quot;&gt;Takeaways&lt;/h2&gt;

&lt;p&gt;It struck me late in the weekend that I went in with the explicit purpose of &lt;em&gt;finishing&lt;/em&gt; a game, yet I still found a way to muddle around with a completely unnecessary feature. Ludum Dare taught me to value the MVP over whatever new idea or silly intellectual problem catches my fancy, something I hadn’t figured out in over a year of game dev on the side.&lt;/p&gt;

&lt;p&gt;I stopped working on any games after that. The semester of Ludum Dare 40, I took a Machine Learning class, which kickstarted my interest in AI. My game dev phase was far from a waste though. I didn’t use C# again, but experience with an extra language is never bad. I used Unity for several future projects, and the interface alone carries over to tons of different programs. And though it’s a bit difficult to articulate, there is a way of thinking like a game designer that I’ll always carry with me.&lt;/p&gt;</content><author><name>Michal Porubcin</name></author><category term="project" /><category term="game" /><category term="gamedev" /><category term="unity" /><summary type="html">I’m writing posts on my old projects. In this post, I talk about a game I made with Henry G. for Ludum Dare 40. I also talk about my old game dev experiences.</summary></entry></feed>