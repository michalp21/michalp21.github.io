---
---

@book{Sutton1998,
  added-at = {2019-07-13T10:11:53.000+0200},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  biburl = {https://www.bibsonomy.org/bibtex/2f46601cf8b13d39d1378af0d79438b12/lanteunis},
  edition = {Second},
  interhash = {ac6b144aaec1819919a2fba9f705c852},
  intrahash = {f46601cf8b13d39d1378af0d79438b12},
  publisher = {The MIT Press},
  timestamp = {2019-07-13T10:11:53.000+0200},
  title = {Reinforcement Learning: An Introduction},
  url = {http://incompleteideas.net/book/the-book-2nd.html},
  year = {2018},
  link = {http://incompleteideas.net/book/RLbook2020.pdf}
}

@Misc{silver2015,author = {David Silver},title = {Lectures on 
Reinforcement Learning},howpublished = {\textsc{url:}~\url
{https://www.davidsilver.uk/teaching/}},year = {2015},
link = {https://www.davidsilver.uk/teaching/}
}

@inproceedings{Pan9780999241127,
author = {Pan, Yangchen and Zaheer, Muhammad and White, Adam and Patterson, Andrew and White, Martha},
title = {Organizing Experience: A Deeper Look at Replay Mechanisms for Sample-Based Planning in Continuous State Domains},
year = {2018},
isbn = {9780999241127},
publisher = {AAAI Press},
abstract = {Model-based strategies for control are critical to obtain sample efficient learning.
Dyna is a planning paradigm that naturally interleaves learning and planning, by simulating
one-step experience to update the action-value function. This elegant planning strategy
has been mostly explored in the tabular setting. The aim of this paper is to revisit
sample-based planning, in stochastic and continuous domains with learned models. We
first highlight the flexibility afforded by a model over Experience Replay (ER). Replay-based
methods can be seen as stochastic planning methods that repeatedly sample from a buffer
of recent agent-environment interactions and perform updates to improve data efficiency.
We show that a model, as opposed to a replay buffer, is particularly useful for specifying
which states to sample from during planning, such as predecessor states that propagate
information in reverse from a state more quickly. We introduce a semi-parametric model
learning approach, called Reweighted Experience Models (REMs), that makes it simple
to sample next states or predecessors. We demonstrate that REM-Dyna exhibits similar
advantages over replay-based methods in learning in continuous state problems, and
that the performance gap grows when moving to stochastic domains, of increasing size.},
booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
pages = {4794â€“4800},
numpages = {7},
location = {Stockholm, Sweden},
series = {IJCAI'18},
link = {https://arxiv.org/pdf/1806.04624.pdf}
}

@inproceedings{NEURIPS2019_1b742ae2,
 author = {van Hasselt, Hado P and Hessel, Matteo and Aslanides, John},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 publisher = {Curran Associates, Inc.},
 title = {When to use parametric models in reinforcement learning?},
 url = {https://proceedings.neurips.cc/paper/2019/file/1b742ae215adf18b75449c6e272fd92d-Paper.pdf},
 volume = {32},
 year = {2019},
 link = {https://proceedings.neurips.cc/paper/2019/file/1b742ae215adf18b75449c6e272fd92d-Paper.pdf}
}

@misc{mnih2013playing,
  title={Playing Atari with Deep Reinforcement Learning}, 
  author={Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Alex Graves and Ioannis Antonoglou and Daan Wierstra and Martin Riedmiller},
  year={2013},
  eprint={1312.5602},
  archivePrefix={arXiv},
  primaryClass={cs.LG},
  link={https://arxiv.org/pdf/1312.5602.pdf}
}

